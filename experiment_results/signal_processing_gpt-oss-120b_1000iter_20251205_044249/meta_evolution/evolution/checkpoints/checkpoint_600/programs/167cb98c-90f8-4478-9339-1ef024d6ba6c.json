{"id": "167cb98c-90f8-4478-9339-1ef024d6ba6c", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import List, Dict, Any\n\ndef _to_arr(x):\n    return np.asarray(x, dtype=float)\n\ndef _validate(arr, w):\n    if w <= 0 or arr.size < w:\n        return np.empty(0, dtype=float)\n    return arr\n\n@lru_cache(maxsize=None)\ndef _exp_weights(w):\n    wgt = np.exp(np.linspace(-2, 0, w))\n    return wgt / wgt.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    if window_size == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    if window_size == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(window_size), mode=\"valid\")\n\ndef enhanced_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, window_size)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, window_size)\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best(x, w):\n    a = _to_arr(x)\n    cand = {\n        \"basic\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(a, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(input_signal, window_size)\n    return _select_best(input_signal, window_size)\n\ndef _density(item: Dict[str, Any]) -> float:\n    \"\"\"Value density of an item; safe against zero denominator.\"\"\"\n    denom = item.get(\"weight\", 0) + item.get(\"volume\", 0)\n    return item.get(\"value\", 0) / denom if denom else 0.0\n\ndef construct_packing(\n    items: List[Dict[str, Any]],\n    max_weight: float,\n    max_volume: float,\n) -> Dict[str, Any]:\n    \"\"\"\n    Greedy packing based on value density with a limited back\u2011tracking phase.\n\n    Returns a dictionary:\n        {\n            \"packed_items\": [...],\n            \"total_weight\": ...,\n            \"total_volume\": ...,\n            \"total_value\": ...,\n        }\n    \"\"\"\n    # Edge cases\n    if not items:\n        return {\n            \"packed_items\": [],\n            \"total_weight\": 0.0,\n            \"total_volume\": 0.0,\n            \"total_value\": 0.0,\n        }\n    if max_weight <= 0 or max_volume <= 0:\n        return {\n            \"packed_items\": [],\n            \"total_weight\": 0.0,\n            \"total_volume\": 0.0,\n            \"total_value\": 0.0,\n        }\n\n    # Sort items by descending density\n    sorted_items = sorted(items, key=_density, reverse=True)\n\n    packed: List[Dict[str, Any]] = []\n    remaining_weight = max_weight\n    remaining_volume = max_volume\n    total_value = 0.0\n\n    # Greedy selection\n    for it in sorted_items:\n        w = it.get(\"weight\", 0)\n        v = it.get(\"volume\", 0)\n        if w <= remaining_weight and v <= remaining_volume:\n            packed.append(it)\n            remaining_weight -= w\n            remaining_volume -= v\n            total_value += it.get(\"value\", 0)\n\n    # Limited back\u2011tracking: up to three beneficial swaps\n    skipped = [it for it in sorted_items if it not in packed]\n    swap_attempts = 0\n    while swap_attempts < 3 and skipped and packed:\n        # Identify the lowest\u2011density packed item\n        low_packed = min(packed, key=_density)\n        # Find the highest\u2011density skipped item that fits if we replace low_packed\n        improved = False\n        for cand in sorted(skipped, key=_density, reverse=True):\n            new_weight = remaining_weight + low_packed.get(\"weight\", 0) - cand.get(\"weight\", 0)\n            new_volume = remaining_volume + low_packed.get(\"volume\", 0) - cand.get(\"volume\", 0)\n            if new_weight >= 0 and new_volume >= 0:\n                # Perform swap\n                packed.remove(low_packed)\n                packed.append(cand)\n                remaining_weight = new_weight\n                remaining_volume = new_volume\n                total_value = total_value - low_packed.get(\"value\", 0) + cand.get(\"value\", 0)\n                skipped.remove(cand)\n                skipped.append(low_packed)\n                improved = True\n                break\n        if not improved:\n            break\n        swap_attempts += 1\n\n    return {\n        \"packed_items\": packed,\n        \"total_weight\": max_weight - remaining_weight,\n        \"total_volume\": max_volume - remaining_volume,\n        \"total_value\": total_value,\n    }\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "cfc72d07-76db-4ef7-9f1e-2015db59b19b", "generation": 7, "timestamp": 1764940229.6536021, "iteration_found": 475, "metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.3624766970140094, "slope_changes": 68.6, "lag_error": 0.4099887786960041, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.20076318890739916, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.709225502436145, "accuracy_score": 0.20076318890739916, "efficiency_score": 1.0, "execution_time": 0.0005306243896484375, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.3624766970140094, "slope_changes": 68.6, "lag_error": 0.4099887786960041, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.20076318890739916, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.709225502436145, "accuracy_score": 0.20076318890739916, "efficiency_score": 1.0, "execution_time": 0.0005090713500976562, "success_rate": 1.0}, "island": 0}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are an autonomous code\u2011evolution assistant. Your sole task is to improve the implementation of **`construct_packing()`** and any helper functions that reside inside the designated evolve block.\n\n### What you must do\n- **Output ONLY the Python code that belongs between the lines**  \n  `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.  \n- **Do NOT** include the marker comments themselves, any surrounding imports, the `run_packing()` driver, or any explanatory text.  \n- The surrounding file will be merged automatically; therefore **your response must be pure code**.\n\n### What you must NOT do\n- Do not write a completely new program or restructure the file outside the evolve block.  \n- Do not produce code that depends on undefined symbols or external libraries not already imported.  \n- Do not use overly aggressive \u201cinvent\u2011everything\u201d prompts that lead to syntax errors or missing variables.  \n- Do not repeat previous failures such as returning `None` everywhere, omitting required return values, or leaving placeholder `pass` statements.  \n\n### Guidance for improving `construct_packing()`\n\n1. **Preserve the original function signature**  \n   ```python\n   def construct_packing(items, max_weight, max_volume):\n       ...\n   ```  \n   It must accept the list of `items` (each item is a dict with keys `weight`, `volume`, `value`) and the container limits.\n\n2. **Implement a deterministic, greedy heuristic first**  \n   - Sort items by **value\u2011density** (`value / (weight + volume)`) descending.  \n   - Iterate through the sorted list, adding an item to the current packing only if both weight and volume constraints remain satisfied.  \n   - Return a dictionary: `{ \"packed_items\": [...], \"total_weight\": ..., \"total_volume\": ..., \"total_value\": ... }`.\n\n3. **Add a simple back\u2011tracking fallback** (optional but encouraged)  \n   - After the greedy pass, if there is remaining capacity, attempt to replace a low\u2011density packed item with a higher\u2011density item that was previously skipped.  \n   - Limit the back\u2011tracking depth to **3 swaps** to avoid exponential blow\u2011up and keep runtime predictable.\n\n4. **Edge\u2011case handling**  \n   - If `items` is empty, return an empty packing with all totals set to `0`.  \n   - If a single item exceeds either limit, it must be skipped \u2013 never cause the algorithm to raise an exception.  \n   - Ensure the returned list of packed items contains the original item objects (or copies) so that downstream code can access their fields.\n\n5. **Maintain readability and safety**  \n   - Use clear variable names (`remaining_weight`, `remaining_volume`, `candidate`, etc.).  \n   - Guard all arithmetic with explicit checks to avoid division\u2011by\u2011zero when computing density.  \n   - Include inline comments only if they are part of the code block; **do not** add explanatory prose outside the code.\n\n### Example structure (you must fill in the logic)\n\n```python\ndef construct_packing(items, max_weight, max_volume):\n    # Guard against empty input\n    if not items:\n        return {\n            \"packed_items\": [],\n            \"total_weight\": 0,\n            \"total_volume\": 0,\n            \"total_value\": 0,\n        }\n\n    # Compute density safely\n    def density(item):\n        denom = item[\"weight\"] + item[\"volume\"]\n        return item[\"value\"] / denom if denom else 0\n\n    # Greedy selection\n    sorted_items = sorted(items, key=density, reverse=True)\n    packed = []\n    remaining_weight = max_weight\n    remaining_volume = max_volume\n    total_value = 0\n\n    for item in sorted_items:\n        if item[\"weight\"] <= remaining_weight and item[\"volume\"] <= remaining_volume:\n            packed.append(item)\n            remaining_weight -= item[\"weight\"]\n            remaining_volume -= item[\"volume\"]\n            total_value += item[\"value\"]\n\n    # Optional limited back\u2011tracking to improve packing\n    # (implement up to three swap attempts here)\n\n    return {\n        \"packed_items\": packed,\n        \"total_weight\": max_weight - remaining_weight,\n        \"total_volume\": max_volume - remaining_volume,\n        \"total_value\": total_value,\n    }\n```\n\nReplace the placeholders with concrete, error\u2011free Python code that respects the constraints above. Remember: **output only the completed code block, nothing else.**", "user": "# Current Program Information\n- Fitness: 13.9292\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.0152 \u2192 13.9292. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0005, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0005, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"hybrid\": _hybrid_filter(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        # use hybrid for stronger noise reduction while preserving length\n        return _hybrid_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:m] - filt[:m]))\n\ndef _select_best_filter(x, window_size):\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": _hybrid_filter(x, window_size),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg == \"basic\" or alg == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return _hybrid_filter(input_signal, window_size)\n    # fallback: automatic selection based on residual variance\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0005), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _arr(x):\n    return np.asarray(x, float).ravel()\n\ndef _ok(a, w):\n    return w > 0 and a.size >= w\n\ndef _simple(x, w):\n    a = _arr(x)\n    return np.convolve(a, np.ones(w) / w, \"valid\") if _ok(a, w) else np.empty(0)\n\ndef _weighted(x, w):\n    a = _arr(x)\n    if not _ok(a, w):\n        return np.empty(0)\n    wts = np.exp(np.linspace(-2, 0, w))\n    wts /= wts.sum()\n    return np.convolve(a, wts, \"valid\")\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    k = str(alg).lower()\n    return _weighted(sig, w) if k == \"weighted\" else _simple(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0005), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _arr(x):\n    return np.asarray(x, float).ravel()\n\ndef _valid(a, w):\n    return w > 0 and a.size >= w\n\ndef adaptive(x, w=20):\n    a = _arr(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted(x, w=20):\n    a = _arr(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    wts = np.exp(np.linspace(-2, 0, w))\n    wts /= wts.sum()\n    return np.convolve(a, wts, mode=\"valid\")\n\ndef _auto(sig, w):\n    a = _arr(sig)\n    return weighted if np.var(a) > 1 else adaptive\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    key = str(alg).lower()\n    if key == \"auto\":\n        return _auto(sig, w)(sig, w)\n    if key in (\"adaptive\", \"enhanced\"):\n        return adaptive(sig, w)\n    if key == \"weighted\":\n        return weighted(sig, w)\n    raise ValueError(f\"Unsupported algorithm_type: {alg}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(a, w):\n    \"\"\"Return True if window size is positive and fits the array.\"\"\"\n    return w > 0 and a.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weights that sum to 1.\"\"\"\n    wts = np.exp(np.linspace(-2, 0, w))\n    return wts / wts.sum()\n\ndef _simple_filter(a, w):\n    \"\"\"Uniform moving\u2011average (simple) filter.\"\"\"\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    kernel = np.full(w, 1.0 / w, dtype=float)\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef _weighted_filter(a, w):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w)[::-1], mode=\"valid\")\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean squared error between original (truncated) and filtered signal.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return np.inf\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef process_signal(sig, w=20, alg=\"auto\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    w : int, optional\n        Window size (default 20). Must be > 0.\n    alg : str, optional\n        Algorithm selector (case\u2011insensitive):\n        - \"simple\"   : uniform moving average\n        - \"weighted\" : exponential weighted average\n        - \"auto\" or any other value : automatically pick the better of the two\n          based on residual variance with the original signal.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal (may be empty if input is invalid).\n    \"\"\"\n    a = _to_array(sig)\n    key = str(alg).lower()\n\n    if key == \"simple\":\n        return _simple_filter(a, w)\n    if key == \"weighted\":\n        return _weighted_filter(a, w)\n\n    # Auto\u2011selection: evaluate both candidates and choose the one with lower\n    # residual variance against the original (pre\u2011filter) signal.\n    simple_res = _simple_filter(a, w)\n    weighted_res = _weighted_filter(a, w)\n\n    # Discard empty results\n    candidates = {}\n    if simple_res.size:\n        candidates[\"simple\"] = simple_res\n    if weighted_res.size:\n        candidates[\"weighted\"] = weighted_res\n\n    if not candidates:\n        return np.empty(0, dtype=float)\n\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef _exp_weights(w: int) -> np.ndarray:\n    \"\"\"Right\u2011biased exponential weights that sum to 1.\"\"\"\n    w_arr = np.exp(np.linspace(-2, 0, w))\n    return w_arr / w_arr.sum()\n\ndef _to_array(x) -> np.ndarray:\n    \"\"\"Convert input to a 1\u2011D float array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr: np.ndarray, w: int) -> bool:\n    \"\"\"Check that window size is positive and fits the array.\"\"\"\n    return w > 0 and arr.size >= w\n\ndef adaptive(x, w=20) -> np.ndarray:\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted(x, w=20) -> np.ndarray:\n    \"\"\"Exponential\u2011weighted moving average.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema(x, w=20) -> np.ndarray:\n    \"\"\"Classic exponential moving average (EMA).\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, len(a)):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\ndef gaussian(x, w=20) -> np.ndarray:\n    \"\"\"Zero\u2011phase Gaussian weighted moving average.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    mid = (w - 1) / 2.0\n    sigma = max(w / 6.0, 1e-8)\n    idx = np.arange(w)\n    g = np.exp(-0.5 * ((idx - mid) / sigma) ** 2)\n    g /= g.sum()\n    return np.convolve(a, g[::-1], mode=\"valid\")\n\ndef hybrid(x, w=20) -> np.ndarray:\n    \"\"\"Adaptive filter followed by exponential\u2011weighted filter.\"\"\"\n    return weighted(adaptive(x, w), w)\n\ndef enhanced(x, w=20) -> np.ndarray:\n    \"\"\"Detrend with simple moving average, then apply weighted filter.\"\"\"\n    trend = adaptive(x, w)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    return weighted(x - trend_full, w)\n\ndef median(x, w=20) -> np.ndarray:\n    \"\"\"Median filter (requires odd window size).\"\"\"\n    a = _to_array(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0, dtype=float)\n    # enforce odd window for symmetric median\n    if w % 2 == 0:\n        w += 1\n        if a.size < w:\n            return np.empty(0, dtype=float)\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Variance of the residual (original \u2013 filtered) over overlapping region.\"\"\"\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return np.inf\n    return float(np.var(original[:n] - filtered[:n]))\n\ndef _select_best_filter(x: np.ndarray, w: int) -> np.ndarray:\n    \"\"\"Choose the filter yielding the smallest residual variance.\"\"\"\n    candidates = {\n        \"adaptive\": adaptive(x, w),\n        \"weighted\": weighted(x, w),\n        \"ema\": ema(x, w),\n        \"gaussian\": gaussian(x, w),\n        \"hybrid\": hybrid(x, w),\n        \"enhanced\": enhanced(x, w),\n        \"median\": median(x, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\n_algorithms = {\n    \"adaptive\": adaptive,\n    \"weighted\": weighted,\n    \"ema\": ema,\n    \"gaussian\": gaussian,\n    \"hybrid\": hybrid,\n    \"enhanced\": enhanced,\n    \"median\": median,\n    \"best\": _select_best_filter,\n    \"auto\": _select_best_filter,\n}\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Dispatch to the requested filtering algorithm.\n    Supported ``alg`` values (case\u2011insensitive):\n        - \"adaptive\"\n        - \"weighted\"\n        - \"ema\"\n        - \"gaussian\"\n        - \"hybrid\"\n        - \"enhanced\"\n        - \"median\"\n        - \"best\" / \"auto\"  \u2013 automatic selection of the best filter.\n    Unknown values fall back to the enhanced filter.\n    \"\"\"\n    key = str(alg).lower()\n    func = _algorithms.get(key, enhanced)\n    if func in (adaptive, weighted, ema, gaussian, hybrid, enhanced, median):\n        return func(sig, w)\n    # For selectors that expect only (x, w) and return an array\n    return func(_to_array(sig), w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(a, w):\n    return w > 0 and a.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    wts = np.exp(np.linspace(-2, 0, w))\n    return wts / wts.sum()\n\ndef adaptive(x, w=20):\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted(x, w=20):\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef enhanced(x, w=20):\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    trend = adaptive(a, w)\n    full_trend = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - full_trend\n    return weighted(detrended, w)\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return np.inf\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    key = str(alg).lower()\n    if key == \"adaptive\":\n        return adaptive(sig, w)\n    if key == \"weighted\":\n        return weighted(sig, w)\n    if key == \"enhanced\":\n        return enhanced(sig, w)\n    if key == \"auto\":\n        a = _to_array(sig)\n        candidates = {}\n        for name, func in ((\"adaptive\", adaptive), (\"weighted\", weighted), (\"enhanced\", enhanced)):\n            res = func(a, w)\n            if res.size:\n                candidates[name] = res\n        if not candidates:\n            return np.empty(0, dtype=float)\n        best = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n        return candidates[best]\n    raise ValueError(f\"Unsupported algorithm_type: {alg}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.9626, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _as_arr(x, w):\n    a = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if a.ndim != 1:\n        raise ValueError(\"1\u2011D array required\")\n    if a.size < w:\n        raise ValueError(\"signal shorter than window\")\n    return a\n\n@lru_cache(None)\ndef _exp_weights(w):\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef adaptive(x, w=20):\n    a = _as_arr(x, w)\n    return a.copy() if w == 1 else np.convolve(a, np.ones(w) / w, \"valid\")\n\ndef enhanced(x, w=20):\n    a = _as_arr(x, w)\n    return a.copy() if w == 1 else np.convolve(a, _exp_weights(w)[::-1], \"valid\")\n\ndef enhanced_blend(x, w=20):\n    \"\"\"Blend simple and exponential weighted averages.\"\"\"\n    a = _as_arr(x, w)\n    if w == 1:\n        return a.copy()\n    e = np.convolve(a, _exp_weights(w)[::-1], \"valid\")\n    m = np.convolve(a, np.ones(w) / w, \"valid\")\n    # both outputs have same length (len(a)-w+1)\n    return (e + m) / 2.0\n\ndef median(x, w=21):\n    if w % 2 == 0:\n        w += 1\n    a = _as_arr(x, w)\n    if w == 1:\n        return a.copy()\n    s = a.strides[0]\n    win = np.lib.stride_tricks.as_strided(a, (a.size - w + 1, w), (s, s))\n    return np.median(win, axis=1)\n\ndef hybrid(x, w=20):\n    \"\"\"Median filter followed by blended enhanced smoothing.\"\"\"\n    med = median(x, w if w % 2 else w + 1)\n    return enhanced_blend(med, w)\n\ndef _residual_variance(original, filtered):\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return np.inf\n    return float(np.var(original[:n] - filtered[:n]))\n\ndef process_signal(sig, w=20, alg=\"auto\"):\n    a = alg.lower()\n    if a == \"adaptive\":\n        return adaptive(sig, w)\n    if a == \"enhanced\":\n        return enhanced_blend(sig, w)\n    if a == \"median\":\n        return median(sig, w)\n    if a == \"hybrid\":\n        return hybrid(sig, w)\n\n    # auto\u2011selection: evaluate candidates and pick lowest residual variance\n    candidates = {\n        \"adaptive\": adaptive(sig, w),\n        \"enhanced\": enhanced_blend(sig, w),\n        \"median\": median(sig, w),\n        \"hybrid\": hybrid(sig, w),\n    }\n    # discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best = min(candidates, key=lambda k: _residual_variance(sig, candidates[k]))\n    return candidates[best]\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _to_arr(x):\n    return np.asarray(x, dtype=float)\n\ndef _validate(arr, w):\n    if w <= 0:\n        # Gracefully handle non\u2011positive window sizes\n        return np.empty(0, dtype=float)\n    if arr.size < w:\n        return np.empty(0, dtype=float)\n    return arr\n\n@lru_cache(maxsize=None)\ndef _exp_weights(w):\n    wgt = np.exp(np.linspace(-2, 0, w))\n    return wgt / wgt.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    if window_size == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    if window_size == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(window_size), mode=\"valid\")\n\ndef enhanced_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, window_size)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, window_size)\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best(x, w):\n    a = _to_arr(x)\n    cand = {\n        \"basic\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(a, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(input_signal, window_size)\n    return _select_best(input_signal, window_size)\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    First\u2011Fit Decreasing bin\u2011packing.\n\n    Args:\n        items: List of non\u2011negative item sizes.\n        capacity: Positive bin capacity.\n\n    Returns:\n        A list of bins, each bin is a list of original item indices that fit\n        within the given capacity.\n    \"\"\"\n    if capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not items:\n        return []\n\n    # Pair each item with its original index\n    indexed_items = [(i, size) for i, size in enumerate(items)]\n    # Sort by size descending\n    indexed_items.sort(key=lambda x: x[1], reverse=True)\n\n    bins: List[List[int]] = []          # stores indices per bin\n    remaining: List[int] = []           # remaining capacity per bin\n\n    for idx, size in indexed_items:\n        if size > capacity:\n            # Item too large to fit any bin \u2013 place it alone in its own bin\n            bins.append([idx])\n            remaining.append(0)\n            continue\n\n        placed = False\n        for b_idx, cap_left in enumerate(remaining):\n            if size <= cap_left:\n                bins[b_idx].append(idx)\n                remaining[b_idx] -= size\n                placed = True\n                break\n        if not placed:\n            # Open a new bin\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    # Optional: sort indices inside each bin for readability\n    for b in bins:\n        b.sort()\n    return bins\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\nfrom typing import List, Dict, Any\n\ndef _to_arr(x):\n    return np.asarray(x, dtype=float)\n\ndef _validate(arr, w):\n    if w <= 0 or arr.size < w:\n        return np.empty(0, dtype=float)\n    return arr\n\n@lru_cache(maxsize=None)\ndef _exp_weights(w):\n    wgt = np.exp(np.linspace(-2, 0, w))\n    return wgt / wgt.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    if window_size == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    if window_size == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(window_size), mode=\"valid\")\n\ndef enhanced_filter(x, window_size=20):\n    a = _validate(_to_arr(x), window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, window_size)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, window_size)\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best(x, w):\n    a = _to_arr(x)\n    cand = {\n        \"basic\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(a, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(input_signal, window_size)\n    return _select_best(input_signal, window_size)\n\ndef _density(item: Dict[str, Any]) -> float:\n    \"\"\"Value density of an item; safe against zero denominator.\"\"\"\n    denom = item.get(\"weight\", 0) + item.get(\"volume\", 0)\n    return item.get(\"value\", 0) / denom if denom else 0.0\n\ndef construct_packing(\n    items: List[Dict[str, Any]],\n    max_weight: float,\n    max_volume: float,\n) -> Dict[str, Any]:\n    \"\"\"\n    Greedy packing based on value density with a limited back\u2011tracking phase.\n\n    Returns a dictionary:\n        {\n            \"packed_items\": [...],\n            \"total_weight\": ...,\n            \"total_volume\": ...,\n            \"total_value\": ...,\n        }\n    \"\"\"\n    # Edge cases\n    if not items:\n        return {\n            \"packed_items\": [],\n            \"total_weight\": 0.0,\n            \"total_volume\": 0.0,\n            \"total_value\": 0.0,\n        }\n    if max_weight <= 0 or max_volume <= 0:\n        return {\n            \"packed_items\": [],\n            \"total_weight\": 0.0,\n            \"total_volume\": 0.0,\n            \"total_value\": 0.0,\n        }\n\n    # Sort items by descending density\n    sorted_items = sorted(items, key=_density, reverse=True)\n\n    packed: List[Dict[str, Any]] = []\n    remaining_weight = max_weight\n    remaining_volume = max_volume\n    total_value = 0.0\n\n    # Greedy selection\n    for it in sorted_items:\n        w = it.get(\"weight\", 0)\n        v = it.get(\"volume\", 0)\n        if w <= remaining_weight and v <= remaining_volume:\n            packed.append(it)\n            remaining_weight -= w\n            remaining_volume -= v\n            total_value += it.get(\"value\", 0)\n\n    # Limited back\u2011tracking: up to three beneficial swaps\n    skipped = [it for it in sorted_items if it not in packed]\n    swap_attempts = 0\n    while swap_attempts < 3 and skipped and packed:\n        # Identify the lowest\u2011density packed item\n        low_packed = min(packed, key=_density)\n        # Find the highest\u2011density skipped item that fits if we replace low_packed\n        improved = False\n        for cand in sorted(skipped, key=_density, reverse=True):\n            new_weight = remaining_weight + low_packed.get(\"weight\", 0) - cand.get(\"weight\", 0)\n            new_volume = remaining_volume + low_packed.get(\"volume\", 0) - cand.get(\"volume\", 0)\n            if new_weight >= 0 and new_volume >= 0:\n                # Perform swap\n                packed.remove(low_packed)\n                packed.append(cand)\n                remaining_weight = new_weight\n                remaining_volume = new_volume\n                total_value = total_value - low_packed.get(\"value\", 0) + cand.get(\"value\", 0)\n                skipped.remove(cand)\n                skipped.append(low_packed)\n                improved = True\n                break\n        if not improved:\n            break\n        swap_attempts += 1\n\n    return {\n        \"packed_items\": packed,\n        \"total_weight\": max_weight - remaining_weight,\n        \"total_volume\": max_volume - remaining_volume,\n        \"total_value\": total_value,\n    }"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}