{"id": "ce06efb1-3044-496b-b4c1-260030c2cc44", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n\ndef _to_numpy(arr):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(arr, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ewma(x, w):\n    \"\"\"Classic exponential moving average.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\n@lru_cache(maxsize=32)\ndef _exp_weights(w):\n    \"\"\"Gaussian\u2011like weights for smoothing.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\n\ndef _gaussian(x, w):\n    \"\"\"Weighted convolution using Gaussian\u2011shaped kernel.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, _exp_weights(w), mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter (valid).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    if w % 2 == 0:\n        w += 1  # ensure odd window for true median\n    stride = x.strides[0]\n    shape = (x.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _hybrid(x, w):\n    \"\"\"Average of Gaussian and simple moving average.\"\"\"\n    ma = _ma(x, w)\n    ga = _gaussian(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\n\ndef _combined(x, w):\n    \"\"\"\n    Weighted combination of all non\u2011empty filters.\n    Weights are inversely proportional to each filter's residual variance\n    relative to the original signal, yielding a deterministic result.\n    \"\"\"\n    orig = _to_numpy(x)\n    if w <= 0 or orig.size < w:\n        return np.array([], dtype=float)\n\n    # Gather filter outputs\n    filters = {\n        \"ma\": _ma(orig, w),\n        \"ema\": _ewma(orig, w),\n        \"gauss\": _gaussian(orig, w),\n        \"median\": _median(orig, w),\n        \"hybrid\": _hybrid(orig, w),\n    }\n    # Keep only non\u2011empty results\n    filters = {k: v for k, v in filters.items() if v.size}\n    if not filters:\n        return np.array([], dtype=float)\n\n    # Compute residual variances\n    residuals = {}\n    for name, f in filters.items():\n        n = min(len(orig), len(f))\n        residuals[name] = float(np.var(orig[:n] - f[:n]))\n\n    # If any filter matches perfectly, return it directly\n    perfect = [k for k, v in residuals.items() if v == 0.0]\n    if perfect:\n        return filters[perfect[0]]\n\n    # Inverse\u2011variance weighting (deterministic order via sorted keys)\n    inv_vars = np.array([1.0 / residuals[k] for k in sorted(filters.keys())])\n    weights = inv_vars / inv_vars.sum()\n\n    # Align all filters to the shortest length\n    min_len = min(len(f) for f in filters.values())\n    combined = np.zeros(min_len, dtype=float)\n\n    for (name, f), wgt in zip(sorted(filters.items()), weights):\n        combined += wgt * f[:min_len]\n\n    return combined\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best.\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ewma(sig, w)\n    if alg in (\"gauss\", \"weighted\"):\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg == \"hybrid\":\n        return _hybrid(sig, w)\n    if alg in (\"combined\", \"auto\", \"enhanced\"):\n        return _combined(sig, w)\n    # Fallback to combined filter for any unknown request\n    return _combined(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "626eb771-7293-4f1e-aed3-bc6a2f618244", "generation": 14, "timestamp": 1764941278.05313, "iteration_found": 596, "metrics": {"runs_successfully": 1.0, "composite_score": 0.4317240503340017, "output_length": 90.0, "overall_score": 0.3923286887526455, "slope_changes": 56.4, "lag_error": 1.1425128184388338, "avg_error": 1.2637612010954065, "false_reversals": 47.6, "correlation": 0.3364152383831823, "noise_reduction": 0.0, "smoothness_score": 0.2617801047120419, "responsiveness_score": 0.4667416649243953, "accuracy_score": 0.3364152383831823, "efficiency_score": 1.0, "execution_time": 0.0008565902709960938, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4205150578718804, "output_length": 91.0, "overall_score": 0.38035985150742135, "slope_changes": 68.0, "lag_error": 0.9780360456577568, "avg_error": 1.2635358917076887, "false_reversals": 57.8, "correlation": 0.3334964145206185, "noise_reduction": 0.0, "smoothness_score": 0.22727272727272727, "responsiveness_score": 0.5055519600844633, "accuracy_score": 0.3334964145206185, "efficiency_score": 1.0, "execution_time": 0.0011485099792480468, "success_rate": 1.0}, "island": 1}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are an autonomous code\u2011evolution assistant tasked with improving **only** the `construct_packing()` function and any helper functions that reside within the `# EVOLVE\u2011BLOCK-START` / `# EVOLVE\u2011BLOCK-END` region. The surrounding file (imports, `run_packing()`, etc.) is managed automatically and must **not** be altered or reproduced.\n\n### Output Requirements (must be obeyed exactly)\n- **Output ONLY the Python code that belongs between the `# EVOLVE\u2011BLOCK-START` and `# EVOLVE\u2011BLOCK-END` markers.**\n- **Do NOT include the marker lines themselves.**\n- **Do NOT output any code outside that block** (e.g., imports, main driver, comments that belong elsewhere).\n- The system will merge your output with the preserved surrounding code.\n\n### What to Do\n1. **Refactor `construct_packing()`** so that it:\n   - Handles empty or malformed input gracefully.\n   - Produces a packing list that respects the problem constraints (e.g., size limits, weight limits, no overlapping items).\n   - Returns a deterministic, reproducible result (avoid randomness unless a fixed seed is set inside the block).\n   - Uses clear variable names and inline comments *inside* the block to explain non\u2011obvious logic.\n\n2. **Improve or add helper functions** only if they are called by `construct_packing()` and reside inside the same block. Ensure:\n   - Each helper has a single responsibility.\n   - Edge cases (zero items, maximum capacity, duplicate identifiers) are explicitly checked.\n   - No external dependencies are introduced; rely only on the standard library modules already imported outside the block.\n\n3. **Avoid the failure patterns that caused previous invalid programs:**\n   - **Do not** write overly aggressive or speculative code that assumes unavailable variables or external state.\n   - **Do not** leave syntax errors, undefined names, or mismatched indentation.\n   - **Do not** produce code that would raise exceptions for typical valid inputs.\n   - **Do not** duplicate existing logic; keep the modifications minimal and focused.\n\n### Suggested Strategies\n- **Iterative Validation:** After constructing the packing list, run a lightweight validation loop (still inside the block) that verifies each item\u2019s constraints. If a violation is found, adjust or discard the offending item rather than raising an error.\n- **Greedy Packing with Back\u2011off:** Sort items by a heuristic (e.g., value\u2011to\u2011size ratio) and attempt to place them one by one. If placement fails, try the next best item instead of aborting.\n- **Pre\u2011compute Limits:** Compute total capacity, remaining space, and weight limits once at the start and update them incrementally; this avoids repeated calculations and reduces the chance of off\u2011by\u2011one bugs.\n- **Deterministic Tie\u2011Breaking:** When two items have equal priority, break ties by a stable attribute such as their original index or identifier to ensure reproducibility.\n- **Clear Return Structure:** Return a dictionary or list that matches the expected format of the surrounding code (e.g., `{'packed_items': [...], 'remaining_capacity': \u2026}`), and document that structure in a short comment.\n\n### Final Reminder\n- **Only the code inside the evolve block** may be output.\n- **No markers, no extra text, no explanations outside the code.**", "user": "# Current Program Information\n- Fitness: 14.0152\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.0152 \u2192 14.0152. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0012, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0012, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"hybrid\": _hybrid_filter(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        # use hybrid for stronger noise reduction while preserving length\n        return _hybrid_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n\ndef _to_arr(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _valid(arr, w):\n    \"\"\"Return (arr, w) if usable, else an empty array.\"\"\"\n    if w <= 0 or arr.size < w:\n        return np.empty(0, dtype=float), w\n    return arr, w\n\n\n@lru_cache(maxsize=64)\ndef _gauss_weights(w):\n    \"\"\"Gaussian kernel for weighted smoothing.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\n\ndef _ma(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\") if a.size else a\n\n\ndef _ema(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, a.size):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef _gauss(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    return np.convolve(a, _gauss_weights(w), mode=\"valid\") if a.size else a\n\n\ndef _median(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _hybrid(x, w):\n    ma = _ma(x, w)\n    ga = _gauss(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\n\ndef _resvar(orig, filt):\n    \"\"\"Residual variance on the overlapping segment.\"\"\"\n    if not filt.size:\n        return float(\"inf\")\n    n = min(orig.size, filt.size)\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gauss(a, w),\n        \"median\": _median(a, w),\n        \"hybrid\": _hybrid(a, w),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _resvar(a, candidates[k]))\n    return candidates[best_key]\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm or automatically select the best one.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\", \"ma\"      : simple moving average\n        - \"ewma\", \"ema\"      : exponential moving average\n        - \"weighted\", \"gauss\", \"gaussian\"\n                              : Gaussian\u2011weighted moving average\n        - \"median\"           : median filter\n        - \"hybrid\"           : average of MA and Gaussian\n        - \"combined\", \"auto\", \"enhanced\"\n                              : automatic selection of the best filter\n        - any other value   : falls back to automatic selection\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg in (\"weighted\", \"gauss\", \"gaussian\"):\n        return _gauss(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg == \"hybrid\":\n        return _hybrid(sig, w)\n    # default / auto\u2011selection\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0012), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_np(x):\n    return np.asarray(x, dtype=float).ravel()\n\ndef _ma(x, w):\n    a = _to_np(x)\n    if w <= 0 or a.size < w:\n        return np.array([], float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef _ema(x, w):\n    a = _to_np(x)\n    if w <= 0 or a.size < w:\n        return np.array([], float)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n@lru_cache(maxsize=32)\ndef _gauss_weights(w):\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\ndef _gauss(x, w):\n    a = _to_np(x)\n    if w <= 0 or a.size < w:\n        return np.array([], float)\n    return np.convolve(a, _gauss_weights(w), mode=\"valid\")\n\ndef _median(x, w):\n    a = _to_np(x)\n    if w <= 0 or a.size < w:\n        return np.array([], float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\ndef _resvar(orig, filt):\n    if filt.size == 0:\n        return np.inf\n    n = min(len(orig), len(filt))\n    return np.var(orig[:n] - filt[:n])\n\ndef _best(x, w):\n    a = _to_np(x)\n    candidates = [_ma(a, w), _ema(a, w), _gauss(a, w), _median(a, w)]\n    candidates = [c for c in candidates if c.size]\n    if not candidates:\n        return np.array([], float)\n    return min(candidates, key=lambda f: _resvar(a, f))\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg == \"weighted\":\n        return _gauss(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0012), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_numpy(arr):\n    \"\"\"Convert input to 1\u2011D float ndarray.\"\"\"\n    return np.asarray(arr, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ewma(x, w):\n    \"\"\"Classic exponential moving average.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\n@lru_cache(maxsize=32)\ndef _exp_weights(w):\n    \"\"\"Gaussian\u2011like weights for smoothing.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\n\ndef _gaussian(x, w):\n    \"\"\"Weighted convolution using Gaussian\u2011shaped kernel.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, _exp_weights(w), mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter (valid).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    if w % 2 == 0:\n        w += 1  # ensure odd window for true median\n    stride = x.strides[0]\n    shape = (x.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _hybrid(x, w):\n    \"\"\"Average of Gaussian and simple moving average.\"\"\"\n    ma = _ma(x, w)\n    ga = _gaussian(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\n\ndef _best(x, w):\n    \"\"\"Select the filter with the smallest residual variance.\"\"\"\n    x = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(x, w),\n        \"ema\": _ewma(x, w),\n        \"gauss\": _gaussian(x, w),\n        \"median\": _median(x, w),\n        \"hybrid\": _hybrid(x, w),\n    }\n    # discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.array([], dtype=float)\n\n    def _residual(orig, filt):\n        n = min(len(orig), len(filt))\n        return float(np.var(orig[:n] - filt[:n]))\n\n    best_key = min(candidates, key=lambda k: _residual(x, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best.\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ewma(sig, w)\n    if alg in (\"gauss\", \"weighted\"):\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg == \"hybrid\":\n        return _hybrid(sig, w)\n    # default / auto\u2011selection\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import Callable, Dict, Tuple\n\n\ndef _to_arr(x) -> np.ndarray:\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _valid(arr: np.ndarray, w: int) -> Tuple[np.ndarray, int]:\n    \"\"\"Return (arr, w) if usable, else an empty array.\"\"\"\n    if w <= 0 or arr.size < w:\n        return np.empty(0, dtype=float), w\n    return arr, w\n\n\n@lru_cache(maxsize=64)\ndef _gauss_weights(w: int) -> np.ndarray:\n    \"\"\"Gaussian kernel for weighted smoothing.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\n\ndef _ma(x, w: int) -> np.ndarray:\n    a, _ = _valid(_to_arr(x), w)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\") if a.size else a\n\n\ndef _ema(x, w: int) -> np.ndarray:\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, a.size):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef _gauss(x, w: int) -> np.ndarray:\n    a, _ = _valid(_to_arr(x), w)\n    return np.convolve(a, _gauss_weights(w), mode=\"valid\") if a.size else a\n\n\ndef _median(x, w: int) -> np.ndarray:\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _hybrid(x, w: int) -> np.ndarray:\n    ma = _ma(x, w)\n    ga = _gauss(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\n\ndef _resvar(orig: np.ndarray, filt: np.ndarray) -> float:\n    \"\"\"Residual variance on the overlapping segment.\"\"\"\n    if not filt.size:\n        return float(\"inf\")\n    n = min(orig.size, filt.size)\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best(x, w: int) -> np.ndarray:\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    candidates: Dict[str, np.ndarray] = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gauss(a, w),\n        \"median\": _median(a, w),\n        \"hybrid\": _hybrid(a, w),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _resvar(a, candidates[k]))\n    return candidates[best_key]\n\n\ndef adaptive_filter(x, w: int = 20) -> np.ndarray:\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w: int = 20, algorithm_type: str = \"combined\") -> np.ndarray:\n    \"\"\"Dispatch to a filter or auto\u2011select the best one.\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg in (\"weighted\", \"gauss\", \"gaussian\"):\n        return _gauss(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg == \"hybrid\":\n        return _hybrid(sig, w)\n    if alg in (\"enhanced\", \"combined\"):\n        return _best(sig, w)\n    # Fallback to automatic selection\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_input(x, window):\n    ws = int(window)\n    if ws <= 0:\n        raise ValueError(\"window must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, float), ws\n    return arr, ws\n\n@lru_cache(maxsize=32)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window=20):\n    arr, ws = _prepare_input(x, window)\n    if arr.size == 0:\n        return arr\n    c = np.cumsum(np.insert(arr, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef enhanced_filter(x, window=20):\n    arr, ws = _prepare_input(x, window)\n    if arr.size == 0:\n        return arr\n    return np.convolve(arr, _exp_weights(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window=20):\n    arr, ws = _prepare_input(x, window)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _savgol(signal, win, poly=3):\n    if signal.size == 0:\n        return signal\n    try:\n        from scipy.signal import savgol_filter\n    except Exception:\n        return np.convolve(signal, np.ones(win) / win, mode=\"valid\")\n    w = int(win)\n    if w % 2 == 0:\n        w += 1\n    min_len = poly + 2\n    if w < min_len:\n        w = min_len + (min_len % 2 == 0)\n    return savgol_filter(signal, w, poly, mode=\"interp\")\n\ndef process_signal(sig, window=20, alg=\"enhanced\"):\n    a = alg.lower()\n    if a == \"basic\":\n        return adaptive_filter(sig, window)\n    if a == \"enhanced\":\n        return enhanced_filter(sig, window)\n    if a == \"exp\":\n        return exponential_smoothing(sig, window)\n    if a == \"savgol\":\n        arr, ws = _prepare_input(sig, window)\n        return _savgol(arr, ws, poly=3) if arr.size else arr\n    raise ValueError(f\"Unsupported algorithm_type '{alg}'\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_input(x, window):\n    ws = int(window)\n    if ws <= 0:\n        raise ValueError(\"window must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, float), ws\n    return arr, ws\n\n@lru_cache(maxsize=32)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window=20):\n    \"\"\"Simple moving\u2011average (box) filter.\"\"\"\n    arr, ws = _prepare_input(x, window)\n    if arr.size == 0:\n        return arr\n    c = np.cumsum(np.insert(arr, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef enhanced_filter(x, window=20):\n    \"\"\"Exponential\u2011weighted moving average.\"\"\"\n    arr, ws = _prepare_input(x, window)\n    if arr.size == 0:\n        return arr\n    return np.convolve(arr, _exp_weights(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window=20):\n    \"\"\"Classic EMA (exponential moving average).\"\"\"\n    arr, ws = _prepare_input(x, window)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _savgol(signal, win, poly=3):\n    \"\"\"Savitzky\u2011Golay filter with safe fallback to simple MA.\"\"\"\n    if signal.size == 0:\n        return signal\n    try:\n        from scipy.signal import savgol_filter\n    except Exception:\n        return np.convolve(signal, np.ones(win) / win, mode=\"valid\")\n    w = int(win)\n    # enforce odd window size\n    if w % 2 == 0:\n        w += 1\n    min_len = poly + 2\n    if w < min_len:\n        w = min_len + (min_len % 2 == 0)\n    w = min(w, signal.size if signal.size % 2 else signal.size - 1)\n    return savgol_filter(signal, w, poly, mode=\"interp\")\n\ndef _short_moving_average(signal, length=3):\n    \"\"\"Final tiny smoothing pass to kill residual high\u2011frequency noise.\"\"\"\n    if signal.size < length or length <= 0:\n        return signal\n    kernel = np.ones(length) / length\n    return np.convolve(signal, kernel, mode=\"valid\")\n\ndef _hybrid_filter(x, window=20):\n    \"\"\"\n    Multi\u2011stage filter:\n        1. Exponential smoothing (EMA)\n        2. Savitzky\u2011Golay adaptive smoothing\n        3. Very short moving\u2011average (3\u2011point) to tidy up\n    \"\"\"\n    # Stage 1 \u2013 EMA\n    ema = exponential_smoothing(x, window)\n    if ema.size == 0:\n        return ema\n\n    # Stage 2 \u2013 Savitzky\u2011Golay; adapt window to signal length but keep it odd\n    sg_win = max(5, min(window, ema.size // 2))\n    if sg_win % 2 == 0:\n        sg_win += 1\n    sg = _savgol(ema, sg_win, poly=2)\n\n    # Stage 3 \u2013 final short MA\n    return _short_moving_average(sg, length=3)\n\ndef process_signal(sig, window=20, alg=\"enhanced\"):\n    \"\"\"\n    Dispatch to the requested filtering algorithm.\n    Supported ``alg`` values (case\u2011insensitive):\n        basic      \u2013 simple moving average\n        enhanced   \u2013 exponential weighted average\n        exp        \u2013 classic EMA\n        savgol     \u2013 Savitzky\u2011Golay (fallback to simple MA)\n        hybrid     \u2013 EMA \u2192 Savitzky\u2011Golay \u2192 short MA (default for \u201cenhanced\u201d)\n    Unknown values raise a ValueError.\n    \"\"\"\n    a = (alg or \"\").lower()\n    if a == \"basic\":\n        return adaptive_filter(sig, window)\n    if a == \"enhanced\":\n        return enhanced_filter(sig, window)\n    if a == \"exp\":\n        return exponential_smoothing(sig, window)\n    if a == \"savgol\":\n        arr, ws = _prepare_input(sig, window)\n        return _savgol(arr, ws, poly=3) if arr.size else arr\n    if a == \"hybrid\":\n        return _hybrid_filter(sig, window)\n    # default fallback \u2013 use hybrid as the strongest performer\n    return _hybrid_filter(sig, window)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_input(x, window_size):\n    \"\"\"Validate ``window_size`` and convert ``x`` to a 1\u2011D float array.\n\n    Returns (arr, ws). If ``arr`` is shorter than ``ws`` an empty array is\n    returned so callers can handle the degenerate case without raising.\n    \"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    ws = int(window_size)\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    \"\"\"Cached exponential weighting vector (right\u2011biased).\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef _moving_average(arr, ws):\n    \"\"\"Fast simple moving average using cumulative sum.\"\"\"\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[ws:] - csum[:-ws]) / ws\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    return _moving_average(arr, ws)\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (enhanced).\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    w = _exp_weights(ws)[::-1]  # reverse for np.convolve semantics\n    return np.convolve(arr, w, mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    \"\"\"Recursive exponential smoothing; output length matches other filters.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"smooth\"   : recursive exponential smoothing\n    Any unknown value falls back to the enhanced filter.\n    \"\"\"\n    alg = algorithm_type.lower()\n    if alg == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    if alg == \"smooth\":\n        return exponential_smoothing(sig, window_size)\n    # Fallback for unsupported types\n    return enhanced_filter(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_numpy(arr):\n    \"\"\"Convert input to 1\u2011D float ndarray.\"\"\"\n    return np.asarray(arr, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ewma(x, w):\n    \"\"\"Classic exponential moving average.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\n@lru_cache(maxsize=32)\ndef _exp_weights(w):\n    \"\"\"Gaussian\u2011like weights for smoothing.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\n\ndef _gaussian(x, w):\n    \"\"\"Weighted convolution using Gaussian\u2011shaped kernel.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, _exp_weights(w), mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter (valid).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    if w % 2 == 0:\n        w += 1  # ensure odd window for true median\n    stride = x.strides[0]\n    shape = (x.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _hybrid(x, w):\n    \"\"\"Average of Gaussian and simple moving average.\"\"\"\n    ma = _ma(x, w)\n    ga = _gaussian(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\n\ndef _best(x, w):\n    \"\"\"Select the filter with the smallest residual variance.\"\"\"\n    x = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(x, w),\n        \"ema\": _ewma(x, w),\n        \"gauss\": _gaussian(x, w),\n        \"median\": _median(x, w),\n        \"hybrid\": _hybrid(x, w),\n    }\n    # discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.array([], dtype=float)\n\n    def _residual(orig, filt):\n        n = min(len(orig), len(filt))\n        return float(np.var(orig[:n] - filt[:n]))\n\n    best_key = min(candidates, key=lambda k: _residual(x, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best.\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ewma(sig, w)\n    if alg in (\"gauss\", \"weighted\"):\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg == \"hybrid\":\n        return _hybrid(sig, w)\n    # default / auto\u2011selection\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\n\n\ndef _to_numpy(arr):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(arr, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ewma(x, w):\n    \"\"\"Classic exponential moving average.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\n@lru_cache(maxsize=32)\ndef _exp_weights(w):\n    \"\"\"Gaussian\u2011like weights for smoothing.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\n\ndef _gaussian(x, w):\n    \"\"\"Weighted convolution using Gaussian\u2011shaped kernel.\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, _exp_weights(w), mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter (valid).\"\"\"\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    if w % 2 == 0:\n        w += 1  # ensure odd window for true median\n    stride = x.strides[0]\n    shape = (x.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _hybrid(x, w):\n    \"\"\"Average of Gaussian and simple moving average.\"\"\"\n    ma = _ma(x, w)\n    ga = _gaussian(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\n\ndef _combined(x, w):\n    \"\"\"\n    Weighted combination of all non\u2011empty filters.\n    Weights are inversely proportional to each filter's residual variance\n    relative to the original signal, yielding a deterministic result.\n    \"\"\"\n    orig = _to_numpy(x)\n    if w <= 0 or orig.size < w:\n        return np.array([], dtype=float)\n\n    # Gather filter outputs\n    filters = {\n        \"ma\": _ma(orig, w),\n        \"ema\": _ewma(orig, w),\n        \"gauss\": _gaussian(orig, w),\n        \"median\": _median(orig, w),\n        \"hybrid\": _hybrid(orig, w),\n    }\n    # Keep only non\u2011empty results\n    filters = {k: v for k, v in filters.items() if v.size}\n    if not filters:\n        return np.array([], dtype=float)\n\n    # Compute residual variances\n    residuals = {}\n    for name, f in filters.items():\n        n = min(len(orig), len(f))\n        residuals[name] = float(np.var(orig[:n] - f[:n]))\n\n    # If any filter matches perfectly, return it directly\n    perfect = [k for k, v in residuals.items() if v == 0.0]\n    if perfect:\n        return filters[perfect[0]]\n\n    # Inverse\u2011variance weighting (deterministic order via sorted keys)\n    inv_vars = np.array([1.0 / residuals[k] for k in sorted(filters.keys())])\n    weights = inv_vars / inv_vars.sum()\n\n    # Align all filters to the shortest length\n    min_len = min(len(f) for f in filters.values())\n    combined = np.zeros(min_len, dtype=float)\n\n    for (name, f), wgt in zip(sorted(filters.items()), weights):\n        combined += wgt * f[:min_len]\n\n    return combined\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best.\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ewma(sig, w)\n    if alg in (\"gauss\", \"weighted\"):\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg == \"hybrid\":\n        return _hybrid(sig, w)\n    if alg in (\"combined\", \"auto\", \"enhanced\"):\n        return _combined(sig, w)\n    # Fallback to combined filter for any unknown request\n    return _combined(sig, w)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}