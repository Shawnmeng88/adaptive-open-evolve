{"id": "9bf99f64-b013-4986-99dd-a7a30f7d1f97", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(window_size: int) -> np.ndarray:\n    \"\"\"Return right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, window_size))\n    return w / w.sum()\n\n\ndef _prepare_signal(x, window_size: int) -> np.ndarray:\n    \"\"\"Validate input and return a 1\u2011D float ndarray.\"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.ndim != 1:\n        raise ValueError(\"input must be one\u2011dimensional\")\n    if arr.size < window_size:\n        raise ValueError(f\"input length ({arr.size}) < window_size ({window_size})\")\n    return arr\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Uniform moving\u2011average (simple) filter.\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if window_size == 1:\n        return arr.copy()\n    kernel = np.ones(window_size, dtype=float) / window_size\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Trend\u2011preserving exponential\u2011weighted moving average.\n\n    The algorithm:\n    1. Estimate the local trend with a simple moving average (SMA).\n    2. Detrend the signal.\n    3. Apply exponential\u2011weighted moving average (EWMA) to the detrended part.\n    4. Add the trend back.\n\n    This yields smoother results while keeping slow variations.\n    \"\"\"\n    arr = _prepare_signal(x, window_size)\n\n    # Edge case: single\u2011sample window \u2192 identity\n    if window_size == 1:\n        return arr.copy()\n\n    # 1. Simple moving average as trend estimate (valid convolution)\n    sma_kernel = np.ones(window_size, dtype=float) / window_size\n    trend = np.convolve(arr, sma_kernel, mode=\"valid\")  # length N - w + 1\n\n    # Align the original signal with the trend (last element of each window)\n    aligned_signal = arr[window_size - 1 :]  # same length as trend\n\n    # 2. Detrend\n    detrended = aligned_signal - trend\n\n    # 3. EWMA on detrended signal\n    ewma_weights = _exp_weights(window_size)[::-1]  # reverse for causal alignment\n    smoothed = np.convolve(detrended, ewma_weights, mode=\"valid\")  # length N - 2w + 2\n\n    # 4. Add back the trend (trim the trend to match smoothed length)\n    trend_trimmed = trend[window_size - 1 :]  # drop first w-1 entries\n    result = smoothed + trend_trimmed\n\n    return result\n\n\ndef median_filter(x, window_size: int = 21) -> np.ndarray:\n    \"\"\"Median filter; forces an odd window size.\"\"\"\n    if window_size % 2 == 0:\n        window_size += 1\n    arr = _prepare_signal(x, window_size)\n    if window_size == 1:\n        return arr.copy()\n    shape = (arr.size - window_size + 1, window_size)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef process_signal(\n    input_signal, window_size: int = 20, algorithm_type: str = \"enhanced\"\n) -> np.ndarray:\n    \"\"\"Dispatch to the requested filter (case\u2011insensitive).\"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "10e59f92-8bf3-4dc4-aecd-d3d77fbbc428", "generation": 3, "timestamp": 1764937789.637457, "iteration_found": 163, "metrics": {"runs_successfully": 1.0, "composite_score": 0.40447870981586415, "output_length": 82.0, "overall_score": 0.3592327679213706, "slope_changes": 66.4, "lag_error": 0.928461380989026, "avg_error": 1.7006765066326346, "false_reversals": 54.2, "correlation": 0.25572493849364303, "noise_reduction": 0.0, "smoothness_score": 0.23148148148148145, "responsiveness_score": 0.5185481077599503, "accuracy_score": 0.25572493849364303, "efficiency_score": 1.0, "execution_time": 0.00033388137817382815, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.00031414031982421873, "success_rate": 1.0}, "island": 0}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are a code\u2011evolution assistant tasked with improving the **`construct_packing()`** function and any helper functions that reside inside the **`# EVOLVE-BLOCK-START` \u2026 `# EVOLVE-BLOCK-END`** region.\n\n### Core Requirements\n- **Output ONLY the Python code that belongs between the two markers.**  \n  Do **not** include the markers themselves, any import statements, or any surrounding code (e.g., `run_packing()`). The system will automatically splice your output into the preserved outer file.\n- The code you produce must be **syntactically correct** and **runnable** when inserted.\n- **Do NOT** introduce infinite loops, recursion without a base case, or any construct that can raise an unhandled exception for typical inputs.\n- **Do NOT** repeat the failing patterns from previous attempts (e.g., overly aggressive heuristics that cause the program to crash or produce invalid output for all test cases).\n\n### What You Must Achieve\n`construct_packing()` should:\n1. Accept the problem data (items, bins, capacities, etc.) exactly as the existing signature expects.\n2. Build a feasible packing solution that respects all constraints:\n   - No bin exceeds its capacity.\n   - Every item is assigned to at most one bin (or left unpacked if allowed by the original spec).\n3. Return the packing in the format required by the surrounding code (usually a list of bin\u2011to\u2011item mappings or a similar structure).\n4. Prefer deterministic, easy\u2011to\u2011reason\u2011about logic over complex stochastic or overly greedy heuristics that have previously caused failures.\n\n### Suggested Concrete Strategies\n- **Simple First\u2011Fit Decreasing (FFD):**  \n  * Sort items by descending size.  \n  * Iterate over bins in a fixed order, placing each item into the first bin that can accommodate it.  \n  * This deterministic algorithm is fast, respects capacities, and rarely fails on typical test data.\n\n- **Backtracking Guard:**  \n  * If an item cannot be placed by FFD, attempt a limited depth\u2011first backtrack (e.g., try moving a previously placed smaller item to another bin).  \n  * Limit the recursion depth to **2** to avoid exponential blow\u2011up.\n\n- **Capacity\u2011Check Helper:**  \n  * Write a small helper `can_place(item, bin_state)` that returns `True` only if `item.size + bin_state.current_load <= bin_state.capacity`.  \n  * Use this helper everywhere to centralize the capacity logic and avoid off\u2011by\u2011one errors.\n\n- **Clear Data Structures:**  \n  * Represent each bin as a dictionary `{ \"capacity\": int, \"load\": int, \"items\": [] }`.  \n  * Update `load` atomically when adding or removing an item to keep the state consistent.\n\n- **Edge\u2011Case Handling:**  \n  * If an item is larger than **any** bin capacity, skip it (or record it as \u201cunpacked\u201d if the outer code expects that).  \n  * Ensure the function returns an empty packing list when the input item list is empty.\n\n- **Deterministic Ordering:**  \n  * Do **not** use random shuffling or nondeterministic tie\u2011breakers; keep the order of bins and items predictable to aid debugging and testing.\n\n### Formatting Rules for Your Output\n- **Only** include the Python statements that belong inside the evolve block.\n- Do **not** wrap the code in triple\u2011backticks or any other markup.\n- Do **not** add comments that reference the markers themselves.\n- The code must be ready to be inserted directly between `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.\n\n### Example Skeleton (do not copy verbatim; adapt to your solution)\n```python\ndef construct_packing(items, bins):\n    # Prepare bin state\n    bin_state = [\n        {\"capacity\": b.capacity, \"load\": 0, \"items\": []}\n        for b in bins\n    ]\n\n    # Sort items descending by size\n    sorted_items = sorted(items, key=lambda i: i.size, reverse=True)\n\n    for item in sorted_items:\n        placed = False\n        for b in bin_state:\n            if b[\"load\"] + item.size <= b[\"capacity\"]:\n                b[\"items\"].append(item.id)\n                b[\"load\"] += item.size\n                placed = True\n                break\n        if not placed:\n            # Optional limited backtrack or mark as unpacked\n            continue\n\n    # Convert to required output format\n    packing = [{ \"bin_id\": idx, \"items\": b[\"items\"] } for idx, b in enumerate(bin_state) if b[\"items\"]]\n    return packing\n```\n\nUse the above ideas to craft a robust, deterministic implementation that satisfies all constraints and avoids the repeated invalid\u2011program failures observed in earlier iterations.", "user": "# Current Program Information\n- Fitness: 13.7277\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 13.7277 \u2192 13.7277. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4200, output_length: 91.0000, overall_score: 0.3899, slope_changes: 66.0000, lag_error: 0.8697, avg_error: 1.2418, false_reversals: 55.2000, correlation: 0.3768, noise_reduction: 0.0000, smoothness_score: 0.2326, responsiveness_score: 0.5349, accuracy_score: 0.3768, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4200, output_length: 91.0000, overall_score: 0.3899, slope_changes: 66.0000, lag_error: 0.8697, avg_error: 1.2418, false_reversals: 55.2000, correlation: 0.3768, noise_reduction: 0.0000, smoothness_score: 0.2326, responsiveness_score: 0.5349, accuracy_score: 0.3768, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4200, output_length: 91.0000, overall_score: 0.3899, slope_changes: 66.0000, lag_error: 0.8697, avg_error: 1.2418, false_reversals: 55.2000, correlation: 0.3768, noise_reduction: 0.0000, smoothness_score: 0.2326, responsiveness_score: 0.5349, accuracy_score: 0.3768, efficiency_score: 1.0000, execution_time: 0.0018, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 13.7278)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nReal-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n\nThis algorithm implements a sliding window approach to filter volatile, non-stationary\ntime series data while minimizing noise and preserving signal dynamics.\n\"\"\"\nimport numpy as np\nfrom scipy import signal\nfrom collections import deque\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Adaptive signal processing algorithm using sliding window approach.\n\n    Args:\n        x: Input signal (1D array of real-valued samples)\n        window_size: Size of the sliding window (W samples)\n\n    Returns:\n        y: Filtered output signal with length = len(x) - window_size + 1\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n\n    # Initialize output array\n    output_length = len(x) - window_size + 1\n    y = np.zeros(output_length)\n\n    # Simple moving average as baseline\n    for i in range(output_length):\n        window = x[i : i + window_size]\n\n        # Basic moving average filter\n        y[i] = np.mean(window)\n\n    return y\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Enhanced version with trend preservation using weighted moving average.\n\n    Args:\n        x: Input signal (1D array of real-valued samples)\n        window_size: Size of the sliding window\n\n    Returns:\n        y: Filtered output signal\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n\n    output_length = len(x) - window_size + 1\n    y = np.zeros(output_length)\n\n    # Create weights that emphasize recent samples\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights = weights / np.sum(weights)\n\n    for i in range(output_length):\n        window = x[i : i + window_size]\n\n        # Weighted moving average with exponential weights\n        y[i] = np.sum(window * weights)\n\n    return y\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Main signal processing function that applies the selected algorithm.\n\n    Args:\n        input_signal: Input time series data\n        window_size: Window size for processing\n        algorithm_type: Type of algorithm to use (\"basic\" or \"enhanced\")\n\n    Returns:\n        Filtered signal\n    \"\"\"\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    else:\n        return adaptive_filter(input_signal, window_size)\n\n\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4200), Performs well on output_length (91.0000), Performs well on overall_score (0.3899), Performs well on slope_changes (66.0000), Performs well on lag_error (0.8697), Performs well on avg_error (1.2418), Performs well on false_reversals (55.2000), Performs well on correlation (0.3768), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2326), Performs well on responsiveness_score (0.5349), Performs well on accuracy_score (0.3768), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0018), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_input(x, window_size):\n    \"\"\"\n    Validate and normalise the input signal and window size.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        1\u2011D sequence of numeric values.\n    window_size : int or castable to int\n        Desired length of the sliding window; must be positive.\n\n    Returns\n    -------\n    np.ndarray\n        A 1\u2011D float array of the input data.\n\n    Raises\n    ------\n    ValueError\n        If ``x`` is not one\u2011dimensional, if ``window_size`` is not a positive\n        integer, or if the signal is shorter than the window.\n    \"\"\"\n    # coerce window size to int and check positivity\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        raise ValueError(f\"len({a.size}) < window_size ({w})\")\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"\n    Cached exponential weighting vector for a given window length.\n    The vector is right\u2011biased (larger weight for newer samples) and sums to 1.\n    \"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving\u2011average filter (uniform weights).\n\n    Returns a ``valid`` convolution, i.e. length ``len(x) - window_size + 1``.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasises recent samples.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias in convolution\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter using a stride\u2011trick view for efficiency.\n    An odd window size is enforced (incremented if even).\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w % 2 == 0:\n        w += 1  # enforce odd size\n    if w == 1:\n        return a.copy()\n    # Create a sliding window view without copies\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"\n    Hybrid filter: first a simple moving average, then the exponential\u2011weighted filter.\n    \"\"\"\n    first_pass = adaptive_filter(x, window_size)\n    # The second stage works on the already\u2011filtered signal; reuse the same window size\n    return enhanced_filter_with_trend_preservation(first_pass, window_size)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"median\"   : median filter\n        - \"hybrid\"   : adaptive followed by enhanced\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    window_size : int, optional\n        Length of the sliding window (default 20).\n    algorithm_type : str, optional\n        Identifier of the algorithm to use.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported ``algorithm_type`` is supplied.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4200), Performs well on output_length (91.0000), Performs well on overall_score (0.3899), Performs well on slope_changes (66.0000), Performs well on lag_error (0.8697), Performs well on avg_error (1.2418), Performs well on false_reversals (55.2000), Performs well on correlation (0.3768), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2326), Performs well on responsiveness_score (0.5349), Performs well on accuracy_score (0.3768), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n\n    Args:\n        x: 1\u2011D array\u2011like input signal.\n        window_size: Size of the sliding window.\n\n    Returns:\n        Filtered signal (valid convolution, length = len(x) - window_size + 1).\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input length ({len(x)}) < window_size ({window_size})\")\n    kernel = np.full(window_size, 1.0 / window_size, dtype=float)\n    return np.convolve(x, kernel, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasizes recent samples.\n\n    The weight vector is reversed before convolution because `np.convolve`\n    flips the second argument.\n\n    Args:\n        x: 1\u2011D array\u2011like input signal.\n        window_size: Size of the sliding window.\n\n    Returns:\n        Filtered signal (valid convolution).\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input length ({len(x)}) < window_size ({window_size})\")\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(x, w[::-1], mode=\"valid\")\n\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter that reduces impulsive noise while preserving edges.\n\n    Args:\n        x: 1\u2011D array\u2011like input signal.\n        window_size: Size of the sliding window (must be odd for a true median).\n\n    Returns:\n        Filtered signal (valid window, length = len(x) - window_size + 1).\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input length ({len(x)}) < window_size ({window_size})\")\n    # Ensure integer window size\n    w = int(window_size)\n    # Compute median for each sliding window\n    med = [np.median(x[i : i + w]) for i in range(len(x) - w + 1)]\n    return np.asarray(med, dtype=float)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filter.\n\n    Supported algorithm_type values:\n        - \"enhanced\": exponential\u2011weighted moving average (default)\n        - \"adaptive\": simple moving average\n        - \"median\"  : median filter\n\n    Args:\n        input_signal: 1\u2011D array\u2011like signal to be filtered.\n        window_size: Size of the sliding window.\n        algorithm_type: Identifier of the filtering algorithm.\n\n    Returns:\n        Filtered signal as a NumPy array.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4200), Performs well on output_length (91.0000), Performs well on overall_score (0.3899), Performs well on slope_changes (66.0000), Performs well on lag_error (0.8697), Performs well on avg_error (1.2418), Performs well on false_reversals (55.2000), Performs well on correlation (0.3768), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2326), Performs well on responsiveness_score (0.5349), Performs well on accuracy_score (0.3768), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Filter `sig` using one of three algorithms:\n    - \"enhanced\": exponential\u2011weighted moving average\n    - \"adaptive\": simple moving average\n    - \"median\": median filter (odd window)\n    \"\"\"\n    a = np.asarray(sig, float)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        raise ValueError(\"signal shorter than window size\")\n    alg = alg.lower()\n    if alg.startswith(\"a\"):  # adaptive\n        return np.convolve(a, np.ones(w) / w, \"valid\")\n    if alg.startswith(\"m\"):  # median\n        if w % 2 == 0:\n            w += 1\n        s = a.strides[0]\n        shape = (a.size - w + 1, w)\n        win = np.lib.stride_tricks.as_strided(a, shape, (s, s))\n        return np.median(win, axis=1)\n    # enhanced (default)\n    wv = np.exp(np.linspace(-2, 0, w))\n    wv /= wv.sum()\n    return np.convolve(a, wv[::-1], \"valid\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nCompact adaptive signal processing.\n\"\"\"\nimport numpy as np\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Filter `sig` with either an exponential\u2011weighted (enhanced) or simple\n    moving\u2011average (adaptive) filter.\n\n    Returns a NumPy array of length len(sig) - window_size + 1.\n    \"\"\"\n    a = np.asarray(sig, dtype=float).ravel()\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if a.size < window_size:\n        raise ValueError(f\"Input length ({a.size}) < window_size ({window_size})\")\n    if algorithm_type.lower() == \"enhanced\":\n        w = np.exp(np.linspace(-2, 0, window_size))\n        w /= w.sum()\n        return np.convolve(a, w[::-1], mode=\"valid\")\n    # adaptive (simple moving average)\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving average using convolution.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if x.size < window_size:\n        raise ValueError(f\"Input length ({x.size}) < window_size ({window_size})\")\n    return np.convolve(x, np.ones(window_size) / window_size, \"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Weighted moving average with exponential weights.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if x.size < window_size:\n        raise ValueError(f\"Input length ({x.size}) < window_size ({window_size})\")\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(x, w, \"valid\")\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Select and apply the requested filtering algorithm.\"\"\"\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    return adaptive_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_input(x, window_size):\n    \"\"\"\n    Validate and normalise the input signal and window size.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        1\u2011D sequence of numeric values.\n    window_size : int or castable to int\n        Desired length of the sliding window; must be positive.\n\n    Returns\n    -------\n    np.ndarray\n        A 1\u2011D float array of the input data.\n\n    Raises\n    ------\n    ValueError\n        If ``x`` is not one\u2011dimensional, if ``window_size`` is not a positive\n        integer, or if the signal is shorter than the window.\n    \"\"\"\n    # coerce window size to int and check positivity\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        raise ValueError(f\"len({a.size}) < window_size ({w})\")\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"\n    Cached exponential weighting vector for a given window length.\n    The vector is right\u2011biased (larger weight for newer samples) and sums to 1.\n    \"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving\u2011average filter (uniform weights).\n\n    Returns a ``valid`` convolution, i.e. length ``len(x) - window_size + 1``.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasises recent samples.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias in convolution\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter using a stride\u2011trick view for efficiency.\n    An odd window size is enforced (incremented if even).\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w % 2 == 0:\n        w += 1  # enforce odd size\n    if w == 1:\n        return a.copy()\n    # Create a sliding window view without copies\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"\n    Hybrid filter: first a simple moving average, then the exponential\u2011weighted filter.\n    \"\"\"\n    first_pass = adaptive_filter(x, window_size)\n    # The second stage works on the already\u2011filtered signal; reuse the same window size\n    return enhanced_filter_with_trend_preservation(first_pass, window_size)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"median\"   : median filter\n        - \"hybrid\"   : adaptive followed by enhanced\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    window_size : int, optional\n        Length of the sliding window (default 20).\n    algorithm_type : str, optional\n        Identifier of the algorithm to use.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported ``algorithm_type`` is supplied.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _check_window(x, w):\n    \"\"\"Validate input length and window size, return float array.\"\"\"\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    a = np.asarray(x, dtype=float)\n    if a.size < w:\n        raise ValueError(f\"Input length ({a.size}) < window_size ({w})\")\n    return a\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _check_window(x, window_size)\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average, recent samples weighted more.\"\"\"\n    a = _check_window(x, window_size)\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(a, w[::-1], mode=\"valid\")  # reverse for right\u2011bias\n\ndef median_filter(x, window_size=21):\n    \"\"\"Median filter (odd window).\"\"\"\n    if window_size % 2 == 0:\n        window_size += 1\n    a = _check_window(x, window_size)\n    shape = (a.size - window_size + 1, window_size)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Adaptive filter followed by the enhanced filter.\"\"\"\n    return enhanced_filter_with_trend_preservation(adaptive_filter(x, window_size), window_size)\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the selected filtering algorithm.\"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(input_signal, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nAdaptive signal processing with simple, exponential\u2011weighted, and median filters.\n\"\"\"\n\nimport numpy as np\nfrom functools import lru_cache\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(window_size: int) -> np.ndarray:\n    \"\"\"Right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, window_size))\n    return w / w.sum()\n\n\ndef _prepare_signal(x, window_size: int) -> np.ndarray:\n    \"\"\"Validate and convert input to a 1\u2011D float ndarray.\"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.ndim != 1:\n        raise ValueError(\"input must be one\u2011dimensional\")\n    if arr.size < window_size:\n        raise ValueError(f\"input length ({arr.size}) < window_size ({window_size})\")\n    return arr\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Uniform moving\u2011average filter.\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if window_size == 1:\n        return arr.copy()\n    kernel = np.ones(window_size, dtype=float) / window_size\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Exponential\u2011weighted moving average emphasizing recent samples.\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if window_size == 1:\n        return arr.copy()\n    w = _exp_weights(window_size)[::-1]  # reverse for causal alignment\n    return np.convolve(arr, w, mode=\"valid\")\n\n\ndef median_filter(x, window_size: int = 21) -> np.ndarray:\n    \"\"\"Median filter; forces an odd window size.\"\"\"\n    if window_size % 2 == 0:\n        window_size += 1\n    arr = _prepare_signal(x, window_size)\n    if window_size == 1:\n        return arr.copy()\n    shape = (arr.size - window_size + 1, window_size)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef process_signal(\n    input_signal, window_size: int = 20, algorithm_type: str = \"enhanced\"\n) -> np.ndarray:\n    \"\"\"Dispatch to the requested filter (case\u2011insensitive).\"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(window_size: int) -> np.ndarray:\n    \"\"\"Return right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, window_size))\n    return w / w.sum()\n\n\ndef _prepare_signal(x, window_size: int) -> np.ndarray:\n    \"\"\"Validate input and return a 1\u2011D float ndarray.\"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.ndim != 1:\n        raise ValueError(\"input must be one\u2011dimensional\")\n    if arr.size < window_size:\n        raise ValueError(f\"input length ({arr.size}) < window_size ({window_size})\")\n    return arr\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Uniform moving\u2011average (simple) filter.\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if window_size == 1:\n        return arr.copy()\n    kernel = np.ones(window_size, dtype=float) / window_size\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Trend\u2011preserving exponential\u2011weighted moving average.\n\n    The algorithm:\n    1. Estimate the local trend with a simple moving average (SMA).\n    2. Detrend the signal.\n    3. Apply exponential\u2011weighted moving average (EWMA) to the detrended part.\n    4. Add the trend back.\n\n    This yields smoother results while keeping slow variations.\n    \"\"\"\n    arr = _prepare_signal(x, window_size)\n\n    # Edge case: single\u2011sample window \u2192 identity\n    if window_size == 1:\n        return arr.copy()\n\n    # 1. Simple moving average as trend estimate (valid convolution)\n    sma_kernel = np.ones(window_size, dtype=float) / window_size\n    trend = np.convolve(arr, sma_kernel, mode=\"valid\")  # length N - w + 1\n\n    # Align the original signal with the trend (last element of each window)\n    aligned_signal = arr[window_size - 1 :]  # same length as trend\n\n    # 2. Detrend\n    detrended = aligned_signal - trend\n\n    # 3. EWMA on detrended signal\n    ewma_weights = _exp_weights(window_size)[::-1]  # reverse for causal alignment\n    smoothed = np.convolve(detrended, ewma_weights, mode=\"valid\")  # length N - 2w + 2\n\n    # 4. Add back the trend (trim the trend to match smoothed length)\n    trend_trimmed = trend[window_size - 1 :]  # drop first w-1 entries\n    result = smoothed + trend_trimmed\n\n    return result\n\n\ndef median_filter(x, window_size: int = 21) -> np.ndarray:\n    \"\"\"Median filter; forces an odd window size.\"\"\"\n    if window_size % 2 == 0:\n        window_size += 1\n    arr = _prepare_signal(x, window_size)\n    if window_size == 1:\n        return arr.copy()\n    shape = (arr.size - window_size + 1, window_size)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef process_signal(\n    input_signal, window_size: int = 20, algorithm_type: str = \"enhanced\"\n) -> np.ndarray:\n    \"\"\"Dispatch to the requested filter (case\u2011insensitive).\"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}