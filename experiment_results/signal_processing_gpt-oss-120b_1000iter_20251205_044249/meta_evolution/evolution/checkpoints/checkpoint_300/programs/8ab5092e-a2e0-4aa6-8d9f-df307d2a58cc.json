{"id": "8ab5092e-a2e0-4aa6-8d9f-df307d2a58cc", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_signal(x, window_size):\n    \"\"\"Validate input and convert to 1\u2011D float array.\n\n    Returns a tuple ``(arr, w)`` where ``arr`` may be empty if the input\n    length is smaller than the window size. This avoids raising exceptions\n    in downstream code and keeps the API robust.\n    \"\"\"\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Fast simple moving\u2011average using cumulative sums.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (right\u2011biased).\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"Median filter with stride\u2011trick view; enforces odd window size.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w % 2 == 0:\n        w += 1\n    if w == 1:\n        return a.copy()\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Adaptive filter followed by the exponential\u2011weighted filter.\"\"\"\n    first = adaptive_filter(x, window_size)\n    return enhanced_filter(first, window_size)\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original \u2013 filtered) over the overlapping region.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef _select_best(x, window_size):\n    \"\"\"Choose the filter with the lowest residual variance.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n        \"median\": median_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    best_name = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_name]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best one.\"\"\"\n    algo = (algorithm_type or \"auto\").lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    # Fallback: automatically pick the filter with the lowest residual variance\n    return _select_best(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "944ce8c2-d048-4c81-935d-f38a96c46d9c", "generation": 5, "timestamp": 1764938463.051001, "iteration_found": 250, "metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.00036602020263671876, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.0005815505981445312, "success_rate": 1.0}, "island": 1}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\n**System Prompt \u2013 Code Evolution for `construct_packing()`**\n\nYou are an expert code\u2011generation assistant tasked with improving the `construct_packing()` function (and any helper functions defined inside the evolve block). Follow *exactly* the rules below; any deviation will cause the program to be rejected.\n\n---\n\n### \u2757 Mandatory Output Format\n- **Only** output the Python code that belongs *between* the markers `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.\n- **Do NOT** include the markers themselves, any surrounding imports, the `run_packing()` wrapper, or any other code outside the evolve block.\n- The system will automatically merge your output with the preserved surrounding code.\n\n### \ud83d\udeab What You Must NOT Do\n- Do **not** write overly aggressive or vague prompts that ignore the constraints (this caused the \u201chigh invalidity rate\u201d in previous iterations).\n- Do **not** produce code that relies on undefined variables, external files, or unavailable libraries.\n- Do **not** repeat the same faulty logic that led to consecutive invalid programs.\n- Do **not** add comments or print statements that are outside the evolve block.\n\n### \ud83c\udfaf Goal\nRewrite `construct_packing()` so that it reliably builds a packing plan for a given list of item sizes and a bin capacity. The function should return a list of bins, each bin being a list of item indices or sizes that fit within the capacity.\n\n### \ud83d\udccb Specific Strategies to Try (choose **one** and implement it cleanly)\n\n1. **First\u2011Fit Decreasing (FFD) Greedy Algorithm**  \n   - Sort items by descending size.  \n   - Iterate through items, placing each into the first bin that has enough remaining capacity; if none, open a new bin.  \n   - Guarantees a solution within 11/9\u202f\u00d7\u202fOPT\u202f+\u202f1 bins and is simple to implement.\n\n2. **Best\u2011Fit Decreasing (BFD) Greedy Algorithm**  \n   - Similar to FFD, but place each item into the bin that will have the *least* leftover space after insertion (still \u2265\u202f0).  \n   - Often yields tighter packings than plain First\u2011Fit.\n\n3. **Recursive Depth\u2011First Search with Pruning**  \n   - Use a recursive helper that tries to assign the next item to any bin that can accommodate it, backtracking on failure.  \n   - Prune branches when the current number of bins already exceeds the best solution found so far.  \n   - Include a global `best_solution` cache to stop early when an optimal packing is discovered.\n\n4. **Dynamic Programming (DP) Subset\u2011Sum for Single Bin, then Repeat**  \n   - Compute the maximal subset of remaining items that fits into one bin via DP (knapsack).  \n   - Remove those items, repeat until all items are packed.  \n   - Works well for moderate item counts (\u2264\u202f30\u201140) and ensures each bin is as full as possible.\n\n5. **Hybrid Approach**  \n   - Start with a fast greedy pass (FFD or BFD) to obtain an upper\u2011bound number of bins.  \n   - Then invoke the recursive DFS with the bound as a pruning limit to try to improve the solution.\n\n### \ud83d\udee0 Implementation Guidelines\n- **Function Signature** (preserve existing signature):\n  ```python\n  def construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n  ```\n- Return a list where each inner list contains the *indices* (or original values) of items placed in that bin. Choose the representation that the surrounding code expects (use indices if the rest of the program references items by position).\n- Keep the code **self\u2011contained**: only use built\u2011in types (`list`, `int`, etc.) and the `typing` module if already imported.\n- Ensure all variables are locally scoped; do not rely on globals.\n- Include concise inline comments *inside* the evolve block to explain non\u2011obvious steps, but keep them minimal to stay within token limits.\n\n### \ud83d\udcc8 Expected Outcome\nA correct, deterministic implementation of `construct_packing()` that:\n- Produces a valid packing for any input list of non\u2011negative integers and a positive capacity.\n- Runs in reasonable time for typical test sizes (up to a few hundred items).\n- Passes the hidden validation suite without triggering the \u201cinvalid program\u201d guards.\n\n--- \n\nWhen you are ready, output **only** the revised code for the evolve block, adhering strictly to the mandatory output format. Good luck!", "user": "# Current Program Information\n- Fitness: 13.7277\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 13.9292 \u2192 13.7277. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0004, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0003, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _validate_signal(x, window_size):\n    \"\"\"Validate and convert input to a 1\u2011D float array.\"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size == 0:\n        return arr\n    if arr.size < window_size:\n        return np.empty(0, dtype=float)\n    return arr\n\ndef _moving_average(arr, w):\n    \"\"\"Fast moving\u2011average using cumulative sum (valid convolution).\"\"\"\n    if w <= 1:\n        return arr.copy()\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[w:] - csum[:-w]) / w\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    arr = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return np.array([], dtype=float)\n    return _moving_average(arr, window_size)\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the selected algorithm (all map to simple moving average).\"\"\"\n    return adaptive_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0003), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n    Returns the filtered signal with length len(x) - window_size + 1.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        # Not enough data \u2013 return empty array for consistency\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"\n    Exponential weighted moving average.\n    Emphasises recent samples while preserving overall trend.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"\n    Classic exponential moving average (EMA).\n    Output length matches other filters (len(x) - window_size + 1).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Detrends the signal using a simple moving average, then applies\n    a weighted moving average to the detrended series.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    # Estimate local trend\n    trend = adaptive_filter(x, window_size)\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = x - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"\n    Compute variance of the residual (original - filtered).\n    Signals are aligned to the shortest length.\n    \"\"\"\n    min_len = min(len(original), len(filtered))\n    if min_len == 0:\n        return np.inf\n    residual = original[:min_len] - filtered[:min_len]\n    return float(np.var(residual))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"\n    Evaluate all available filters and return the one that yields the lowest\n    residual variance (i.e., highest estimated noise reduction).\n    \"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    # Choose filter with minimal residual variance relative to the raw signal\n    best_name = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_name]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatches to the requested filtering algorithm.\n    Supported types:\n        - \"basic\": simple moving average (adaptive_filter)\n        - \"weighted\": exponential weighted moving average (weighted_filter)\n        - \"ema\": classic exponential moving average (ema_filter)\n        - \"enhanced\": detrended weighted filter (enhanced_filter_with_trend_preservation)\n        - any other value: automatic selection of the best filter based on residual variance.\n    \"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if algorithm_type == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n\n    # Fallback: automatic selection\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0004), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 13.7278)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nReal-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n\nThis algorithm implements a sliding window approach to filter volatile, non-stationary\ntime series data while minimizing noise and preserving signal dynamics.\n\"\"\"\nimport numpy as np\nfrom scipy import signal\nfrom collections import deque\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Adaptive signal processing algorithm using sliding window approach.\n\n    Args:\n        x: Input signal (1D array of real-valued samples)\n        window_size: Size of the sliding window (W samples)\n\n    Returns:\n        y: Filtered output signal with length = len(x) - window_size + 1\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n\n    # Initialize output array\n    output_length = len(x) - window_size + 1\n    y = np.zeros(output_length)\n\n    # Simple moving average as baseline\n    for i in range(output_length):\n        window = x[i : i + window_size]\n\n        # Basic moving average filter\n        y[i] = np.mean(window)\n\n    return y\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Enhanced version with trend preservation using weighted moving average.\n\n    Args:\n        x: Input signal (1D array of real-valued samples)\n        window_size: Size of the sliding window\n\n    Returns:\n        y: Filtered output signal\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n\n    output_length = len(x) - window_size + 1\n    y = np.zeros(output_length)\n\n    # Create weights that emphasize recent samples\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights = weights / np.sum(weights)\n\n    for i in range(output_length):\n        window = x[i : i + window_size]\n\n        # Weighted moving average with exponential weights\n        y[i] = np.sum(window * weights)\n\n    return y\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Main signal processing function that applies the selected algorithm.\n\n    Args:\n        input_signal: Input time series data\n        window_size: Window size for processing\n        algorithm_type: Type of algorithm to use (\"basic\" or \"enhanced\")\n\n    Returns:\n        Filtered signal\n    \"\"\"\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    else:\n        return adaptive_filter(input_signal, window_size)\n\n\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_input(x, window_size):\n    \"\"\"\n    Validate and normalise the input signal and window size.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        1\u2011D sequence of numeric values.\n    window_size : int or castable to int\n        Desired length of the sliding window; must be positive.\n\n    Returns\n    -------\n    np.ndarray\n        A 1\u2011D float array of the input data.\n\n    Raises\n    ------\n    ValueError\n        If ``x`` is not one\u2011dimensional, if ``window_size`` is not a positive\n        integer, or if the signal is shorter than the window.\n    \"\"\"\n    # coerce window size to int and check positivity\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        raise ValueError(f\"len({a.size}) < window_size ({w})\")\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"\n    Cached exponential weighting vector for a given window length.\n    The vector is right\u2011biased (larger weight for newer samples) and sums to 1.\n    \"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving\u2011average filter (uniform weights).\n\n    Returns a ``valid`` convolution, i.e. length ``len(x) - window_size + 1``.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasises recent samples.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias in convolution\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter using a stride\u2011trick view for efficiency.\n    An odd window size is enforced (incremented if even).\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w % 2 == 0:\n        w += 1  # enforce odd size\n    if w == 1:\n        return a.copy()\n    # Create a sliding window view without copies\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"\n    Hybrid filter: first a simple moving average, then the exponential\u2011weighted filter.\n    \"\"\"\n    first_pass = adaptive_filter(x, window_size)\n    # The second stage works on the already\u2011filtered signal; reuse the same window size\n    return enhanced_filter_with_trend_preservation(first_pass, window_size)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"median\"   : median filter\n        - \"hybrid\"   : adaptive followed by enhanced\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    window_size : int, optional\n        Length of the sliding window (default 20).\n    algorithm_type : str, optional\n        Identifier of the algorithm to use.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported ``algorithm_type`` is supplied.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\ndef _as_array(x, window_size):\n    \"\"\"Validate input and convert to a 1\u2011D NumPy float array.\n\n    Returns\n    -------\n    tuple (arr, ws)\n        arr \u2013 NumPy array of the data\n        ws  \u2013 validated integer window size\n    \"\"\"\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"Input signal must be one\u2011dimensional\")\n    ws = int(window_size)\n    if ws <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n    if arr.size < ws:\n        raise ValueError(f\"Input length ({arr.size}) < window_size ({ws})\")\n    return arr, ws\n\n\nfrom functools import lru_cache\n\n@lru_cache(maxsize=32)\ndef _exp_weights(ws):\n    \"\"\"Pre\u2011compute exponential weights for a given window size.\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Fast simple moving\u2011average using cumulative sum.\"\"\"\n    arr, ws = _as_array(x, window_size)\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[ws:] - csum[:-ws]) / ws\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter (valid convolution).\"\"\"\n    arr, ws = _as_array(x, window_size)\n    kernel = _exp_weights(ws)\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\ndef exponential_smoothing(x, window_size=20):\n    \"\"\"Recursive exponential smoothing; output length matches other filters.\"\"\"\n    arr, ws = _as_array(x, window_size)\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1:]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the selected filtering algorithm (case\u2011insensitive).\"\"\"\n    alg = algorithm_type.lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if alg in (\"exp\", \"exponential\"):\n        return exponential_smoothing(input_signal, window_size)\n    # Fallback to enhanced for unknown types to keep processing robust\n    return enhanced_filter_with_trend_preservation(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Alternative runs_successfully approach, NumPy-based implementation\n\n### Inspiration 2 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\ndef _prepare_signal(x, window_size):\n    \"\"\"Validate input and convert to 1\u2011D NumPy array.\n\n    Returns an empty array when the signal length is shorter than *window_size*,\n    allowing callers to handle the case gracefully.\n    \"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"Input signal must be one\u2011dimensional\")\n    if arr.size < window_size:\n        return np.array([], dtype=float)\n    return arr\n\n\nfrom functools import lru_cache\n\n@lru_cache(maxsize=64)\ndef _gaussian_weights(window_size):\n    \"\"\"Return a normalized Gaussian kernel for the given window size.\"\"\"\n    sigma = max(window_size / 6.0, 0.1)  # \u00b13\u03c3 \u2248 window size\n    centre = (window_size - 1) / 2.0\n    idx = np.arange(window_size)\n    w = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return w / w.sum()\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (valid convolution).\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    return np.convolve(arr, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Gaussian\u2011weighted moving average (valid convolution).\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    w = _gaussian_weights(window_size)\n    return np.convolve(arr, w, mode=\"valid\")\n\n\ndef _savgol_filter(x, window_size=20, polyorder=3):\n    \"\"\"Savitzky\u2011Golay filter with safe fallback to Gaussian\u2011weighted MA.\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    try:\n        from scipy import signal\n\n        w = int(window_size)\n        if w % 2 == 0:\n            w += 1\n        w = max(w, polyorder + 2)\n        if w % 2 == 0:\n            w += 1\n        return signal.savgol_filter(arr, window_length=w, polyorder=polyorder, mode=\"interp\")\n    except Exception:\n        # Fallback to Gaussian\u2011weighted MA if SciPy is unavailable or parameters invalid\n        return enhanced_filter_with_trend_preservation(arr, window_size)\n\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Enhanced \u2192 Savitzky\u2011Golay \u2192 simple moving average pipeline.\"\"\"\n    # First pass: Gaussian\u2011weighted smoothing\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return first\n\n    # Second pass: Savitzky\u2011Golay (may fall back to Gaussian\u2011weighted)\n    second = _savgol_filter(first, window_size)\n\n    # Third pass: final simple moving average to further reduce high\u2011frequency noise\n    if second.size == 0:\n        return second\n    return np.convolve(second, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm (case\u2011insensitive).\"\"\"\n    alg = algorithm_type.lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if alg == \"savgol\":\n        return _savgol_filter(input_signal, window_size)\n    if alg == \"hybrid\":\n        return hybrid_filter(input_signal, window_size)\n    # Unknown algorithm \u2013 default to the most balanced hybrid approach\n    return hybrid_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Alternative runs_successfully approach, NumPy-based implementation\n\n### Inspiration 3 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _as_array(x, w):\n    a = np.asarray(x, dtype=float)\n    w = int(w)\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if a.size < w:\n        raise ValueError(f\"Input length ({a.size}) < window_size ({w})\")\n    return a, w\n\ndef adaptive_filter(x, w=20):\n    a, w = _as_array(x, w)\n    return np.convolve(a, np.ones(w) / w, \"valid\")\n\ndef enhanced_filter(x, w=20):\n    a, w = _as_array(x, w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    weights /= weights.sum()\n    return np.convolve(a, weights, \"valid\")\n\ndef exponential_smoothing(x, w=20):\n    a, w = _as_array(x, w)\n    alpha = 2.0 / (w + 1.0)\n    y = np.empty_like(a)\n    y[0] = a[0]\n    for i in range(1, a.size):\n        y[i] = alpha * a[i] + (1.0 - alpha) * y[i - 1]\n    return y[w - 1 :]\n\ndef process_signal(sig, w=20, typ=\"enhanced\"):\n    typ = typ.lower()\n    if typ == \"basic\":\n        return adaptive_filter(sig, w)\n    if typ == \"exp\":\n        return exponential_smoothing(sig, w)\n    return enhanced_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_input(x, window_size):\n    \"\"\"\n    Validate and normalise the input signal and window size.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        1\u2011D sequence of numeric values.\n    window_size : int or castable to int\n        Desired length of the sliding window; must be positive.\n\n    Returns\n    -------\n    np.ndarray\n        A 1\u2011D float array of the input data.\n\n    Raises\n    ------\n    ValueError\n        If ``x`` is not one\u2011dimensional, if ``window_size`` is not a positive\n        integer, or if the signal is shorter than the window.\n    \"\"\"\n    # coerce window size to int and check positivity\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        raise ValueError(f\"len({a.size}) < window_size ({w})\")\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"\n    Cached exponential weighting vector for a given window length.\n    The vector is right\u2011biased (larger weight for newer samples) and sums to 1.\n    \"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving\u2011average filter (uniform weights).\n\n    Returns a ``valid`` convolution, i.e. length ``len(x) - window_size + 1``.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasises recent samples.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias in convolution\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter using a stride\u2011trick view for efficiency.\n    An odd window size is enforced (incremented if even).\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w % 2 == 0:\n        w += 1  # enforce odd size\n    if w == 1:\n        return a.copy()\n    # Create a sliding window view without copies\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"\n    Hybrid filter: first a simple moving average, then the exponential\u2011weighted filter.\n    \"\"\"\n    first_pass = adaptive_filter(x, window_size)\n    # The second stage works on the already\u2011filtered signal; reuse the same window size\n    return enhanced_filter_with_trend_preservation(first_pass, window_size)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"median\"   : median filter\n        - \"hybrid\"   : adaptive followed by enhanced\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    window_size : int, optional\n        Length of the sliding window (default 20).\n    algorithm_type : str, optional\n        Identifier of the algorithm to use.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported ``algorithm_type`` is supplied.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\n\ndef _prepare_signal(x, window_size):\n    \"\"\"Validate input and convert to 1\u2011D float array.\n\n    Returns a tuple ``(arr, w)`` where ``arr`` may be empty if the input\n    length is smaller than the window size. This avoids raising exceptions\n    in downstream code and keeps the API robust.\n    \"\"\"\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Fast simple moving\u2011average using cumulative sums.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (right\u2011biased).\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"Median filter with stride\u2011trick view; enforces odd window size.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w % 2 == 0:\n        w += 1\n    if w == 1:\n        return a.copy()\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Adaptive filter followed by the exponential\u2011weighted filter.\"\"\"\n    first = adaptive_filter(x, window_size)\n    return enhanced_filter(first, window_size)\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original \u2013 filtered) over the overlapping region.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef _select_best(x, window_size):\n    \"\"\"Choose the filter with the lowest residual variance.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n        \"median\": median_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    best_name = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_name]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best one.\"\"\"\n    algo = (algorithm_type or \"auto\").lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    # Fallback: automatically pick the filter with the lowest residual variance\n    return _select_best(sig, window_size)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}