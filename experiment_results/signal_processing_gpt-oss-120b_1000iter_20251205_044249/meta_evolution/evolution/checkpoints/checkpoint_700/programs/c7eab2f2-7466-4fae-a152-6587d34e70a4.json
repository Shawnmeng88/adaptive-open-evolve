{"id": "c7eab2f2-7466-4fae-a152-6587d34e70a4", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef median_filter(x, window_size=20):\n    \"\"\"Median filter (robust to outliers).\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    # Create rolling windows without copying data\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"median\": median_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"auto\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"median\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the auto\u2011selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"median\":\n        return median_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback to auto\u2011selection for any unknown algorithm_type\n    return _auto_select(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "f8f7be24-9a69-493b-a67c-8354c262cdea", "generation": 8, "timestamp": 1764941355.92135, "iteration_found": 604, "metrics": {"runs_successfully": 1.0, "composite_score": 0.41069177655857453, "output_length": 91.0, "overall_score": 0.36931604257091033, "slope_changes": 70.2, "lag_error": 1.0017800350739763, "avg_error": 1.30369374594483, "false_reversals": 59.0, "correlation": 0.30346716971522975, "noise_reduction": 0.0, "smoothness_score": 0.22172949002217296, "responsiveness_score": 0.49955538694492213, "accuracy_score": 0.30346716971522975, "efficiency_score": 1.0, "execution_time": 0.0003409862518310547, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.41069177655857453, "output_length": 91.0, "overall_score": 0.36931604257091033, "slope_changes": 70.2, "lag_error": 1.0017800350739763, "avg_error": 1.30369374594483, "false_reversals": 59.0, "correlation": 0.30346716971522975, "noise_reduction": 0.0, "smoothness_score": 0.22172949002217296, "responsiveness_score": 0.49955538694492213, "accuracy_score": 0.30346716971522975, "efficiency_score": 1.0, "execution_time": 0.00040121078491210936, "success_rate": 1.0}, "island": 0}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are tasked with evolving the `construct_packing()` function (and any helper functions inside the evolve block) to correctly build a packing plan for the given items and constraints.\n\n**IMPORTANT OUTPUT RULES**  \n- **Only** output the code that belongs **between** the markers `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.  \n- **Do not** include the markers themselves, any imports, the surrounding `run_packing()` wrapper, or any other code outside the evolve block.  \n- The system will automatically splice your output into the preserved file, so any extra text will cause a failure.\n\n**WHAT NOT TO DO (avoid the patterns that caused previous failures)**  \n- Do not write a completely new program that replaces the whole file; only modify the target function(s).  \n- Do not produce code that references undefined variables, missing imports, or external libraries not already present.  \n- Do not leave placeholder `pass` statements or `TODO` comments in the returned code.  \n- Do not generate syntactically invalid Python (e.g., mismatched parentheses, indentation errors).  \n- Do not repeat the same logic that previously produced invalid programs (e.g., returning `None` unconditionally, or using undefined helper names).  \n\n**STRATEGIES TO TRY (domain\u2011specific guidance)**  \n\n1. **Analyze the existing signature**  \n   - `construct_packing(items, max_weight, max_volume)` receives a list of item dicts (`{'id':\u2026, 'weight':\u2026, 'volume':\u2026, 'value':\u2026}`) and two numeric limits.  \n   - Your goal is to return a list of item IDs that satisfy both weight and volume constraints while maximizing total value (a classic 0\u20111 knapsack with two resources).\n\n2. **Implement a deterministic DP or branch\u2011and\u2011bound**  \n   - Use a 2\u2011dimensional dynamic programming table `dp[w][v]` storing the best total value achievable with weight \u2264\u202fw and volume \u2264\u202fv.  \n   - Iterate over items, updating the table backwards to avoid reuse.  \n   - After filling the table, reconstruct the selected item IDs by back\u2011tracking from the optimal cell.\n\n3. **Fallback greedy heuristic (if DP is too heavy for the test size)**  \n   - Compute a \u201cvalue density\u201d = `value / (weight + volume)` for each item.  \n   - Sort items by density descending, then iteratively add an item if it does not exceed either limit.  \n   - Ensure the function still returns a list of IDs and never exceeds the constraints.\n\n4. **Edge\u2011case handling**  \n   - If no items can be added (all exceed a limit), return an empty list `[]`.  \n   - Guard against empty `items` input.  \n   - Validate that `max_weight` and `max_volume` are non\u2011negative; if not, treat them as zero.\n\n5. **Performance considerations**  \n   - For typical test sizes (\u2264\u202f200 items, weight/volume \u2264\u202f10\u2074), a DP with O(N\u00b7W\u00b7V) may be too large; you can limit the DP dimensions by scaling down (e.g., integer division by a small factor) **only if** you clearly comment the scaling and reverse\u2011scale during reconstruction.  \n   - Otherwise, prefer the greedy heuristic with a deterministic tie\u2011breaker (e.g., higher value then lower ID).\n\n6. **Return format**  \n   - The function must return a plain Python `list` of the selected item IDs in any order.  \n   - Do not wrap the list in another structure or return additional metadata.\n\n**Final reminder**: Output **only** the revised code for `construct_packing()` (and any new helper functions you create) that sits between the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers. No extra text, comments outside the function, or the markers themselves.", "user": "# Current Program Information\n- Fitness: 14.2259\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.2259 \u2192 14.2259. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0005, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0005, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"hybrid\": _hybrid_filter(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        # use hybrid for stronger noise reduction while preserving length\n        return _hybrid_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(a, w):\n    \"\"\"Return True if window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and a.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weights that sum to 1.\"\"\"\n    w = int(w)\n    wts = np.exp(np.linspace(-2, 0, w))\n    return wts / wts.sum()\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Uniform moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    kernel = np.full(w, 1.0 / w, dtype=float)\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef weighted_filter(x, w=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, w=20):\n    \"\"\"Exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, w=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a_f = adaptive_filter(x, w)\n    w_f = weighted_filter(x, w)\n    if a_f.size == 0 and w_f.size == 0:\n        return np.empty(0, dtype=float)\n    if a_f.size == 0:\n        return w_f\n    if w_f.size == 0:\n        return a_f\n    return (a_f + w_f) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean squared error between original (truncated) and filtered signal.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best(x, w):\n    \"\"\"Choose the filter with smallest residual variance against the original.\"\"\"\n    a = _to_array(x)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    w : int, optional\n        Window size (>0). Default 20.\n    alg : str, optional\n        Algorithm selector (case\u2011insensitive):\n        - \"adaptive\"\n        - \"weighted\"\n        - \"ema\"\n        - \"hybrid\" or \"enhanced\"\n        - \"auto\" (or any unknown value) \u2013 automatically pick the best filter.\n    \"\"\"\n    a = _to_array(sig)\n    key = str(alg).lower()\n    if key == \"adaptive\":\n        return adaptive_filter(a, w)\n    if key == \"weighted\":\n        return weighted_filter(a, w)\n    if key == \"ema\":\n        return ema_filter(a, w)\n    if key in (\"hybrid\", \"enhanced\"):\n        return hybrid_filter(a, w)\n    # Auto\u2011selection for \"auto\" or any other value\n    return _select_best(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0005), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:m] - filt[:m]))\n\ndef _select_best_filter(x, window_size):\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": _hybrid_filter(x, window_size),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg == \"basic\" or alg == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return _hybrid_filter(input_signal, window_size)\n    # fallback: automatic selection based on residual variance\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0005), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is positive and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = int(w)\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    s = adaptive_filter(x, window_size)\n    w = weighted_filter(x, window_size)\n    if s.size == 0:\n        return w\n    if w.size == 0:\n        return s\n    return (s + w) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual (orig \u2013 filt) over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"   \u2013 automatic selection based on residual variance\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback to enhanced for unknown types\n    return hybrid_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the hybrid filter.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback\n    return hybrid_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _check(x, w):\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if x.size < w:\n        raise ValueError(f\"input length ({x.size}) < window_size ({w})\")\n    return x\n\ndef adaptive_filter(x, window_size=20):\n    x = _check(x, window_size)\n    if window_size == 1:\n        return x.copy()\n    return np.convolve(x, np.ones(window_size) / window_size, \"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    x = _check(x, window_size)\n    if window_size == 1:\n        return x.copy()\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(x, w[::-1], \"valid\")\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    return (\n        enhanced_filter_with_trend_preservation(sig, window_size)\n        if algorithm_type.lower() == \"enhanced\"\n        else adaptive_filter(sig, window_size)\n    )\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 11.7820, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\n_kcache = {}\n\ndef _as_arr(x, w):\n    a = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if a.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if a.size < w:\n        raise ValueError(f\"input length ({a.size}) < window_size ({w})\")\n    return a\n\ndef _kernel(w):\n    k = _kcache.get(w)\n    if k is None:\n        idx = np.arange(w)\n        exp = np.exp(-0.6 * (w - 1 - idx))\n        sigma = max(w / 6.0, 1e-8)\n        mid = (w - 1) / 2.0\n        gauss = np.exp(-0.5 * ((idx - mid) / sigma) ** 2)\n        k = exp * gauss\n        k /= k.sum()\n        _kcache[w] = k\n    return k\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    a = _as_arr(sig, w)\n    if w == 1:\n        return a.copy()\n    if alg.lower() == \"adaptive\":\n        return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n    return np.convolve(a, _kernel(w)[::-1], mode=\"valid\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 14.2259, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prep(x,w):\n    a=np.asarray(x,float).ravel()\n    w=int(w)\n    if w<=0: raise ValueError\n    if a.size<w: return np.empty(0),w\n    return a,w\n\n@lru_cache(None)\ndef _weights(w):\n    k=np.exp(np.linspace(-2,0,w))\n    return k/k.sum()\n\ndef _adapt(a,w):\n    return a.copy() if w==1 else np.convolve(a,np.ones(w)/w,'valid')\n\ndef _weight(a,w):\n    return a.copy() if w==1 else np.convolve(a,_weights(w),'valid')\n\ndef _ema(a,w):\n    if w==1: return a.copy()\n    alpha=2/(w+1)\n    out=np.empty_like(a)\n    out[0]=a[0]\n    for i in range(1,len(a)):\n        out[i]=alpha*a[i]+(1-alpha)*out[i-1]\n    return out[w-1:]\n\ndef _hybrid(a,w):\n    return (_adapt(a,w)+_weight(a,w))/2\n\ndef _var(o,f):\n    m=min(len(o),len(f))\n    return float('inf') if m==0 else float(np.var(o[:m]-f[:m]))\n\ndef _best(x,w):\n    a,_= _prep(x,w)\n    cand={'adaptive':_adapt(a,w),'weighted':_weight(a,w),'ema':_ema(a,w),'hybrid':_hybrid(a,w)}\n    cand={k:v for k,v in cand.items() if v.size}\n    if not cand: return np.empty(0)\n    return cand[min(cand,key=lambda k:_var(a,cand[k]))]\n\ndef process_signal(sig,w=20,alg='enhanced'):\n    a,_= _prep(sig,w)\n    alg=alg.lower()\n    if alg in ('adaptive','basic'): return _adapt(a,w)\n    if alg=='weighted': return _weight(a,w)\n    if alg=='ema': return _ema(a,w)\n    if alg in ('enhanced','hybrid'): return _hybrid(a,w)\n    return _best(sig,w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the hybrid filter.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback\n    return hybrid_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef median_filter(x, window_size=20):\n    \"\"\"Median filter (robust to outliers).\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    # Create rolling windows without copying data\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"median\": median_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"auto\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"median\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the auto\u2011selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"median\":\n        return median_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback to auto\u2011selection for any unknown algorithm_type\n    return _auto_select(input_signal, window_size)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}