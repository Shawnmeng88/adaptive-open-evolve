{"id": "3cb37072-3416-4cca-b9a3-9987bfc2384c", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _arr(x):\n    return np.asarray(x, dtype=float).ravel()\n\ndef _ma(x, w):\n    a = _arr(x)\n    return np.convolve(a, np.ones(w) / w, \"valid\") if w > 0 and a.size >= w else np.empty(0)\n\ndef _ema(x, w):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n@lru_cache(maxsize=None)\ndef _gw(w):\n    sigma = 0.3 * w\n    c = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - c) / sigma) ** 2)\n    return wts / wts.sum()\n\ndef _gauss(x, w):\n    a = _arr(x)\n    return np.convolve(a, _gw(w), \"valid\") if w > 0 and a.size >= w else np.empty(0)\n\ndef _median(x, w):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0)\n    s = a.strides[0]\n    shape = (a.size - w + 1, w)\n    win = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(s, s))\n    return np.median(win, axis=1)\n\ndef _resvar(orig, filt):\n    if filt.size == 0:\n        return np.inf\n    n = min(len(orig), len(filt))\n    return np.var(orig[:n] - filt[:n])\n\ndef _best(x, w):\n    a = _arr(x)\n    cand = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gauss(a, w),\n        \"median\": _median(a, w),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0)\n    return min(cand.values(), key=lambda f: _resvar(a, f))\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg in (\"weighted\", \"gauss\", \"gaussian\"):\n        return _gauss(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    return _best(sig, w)\n\ndef _validate_packing_input(items, capacity):\n    if not isinstance(capacity, int) or capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not isinstance(items, (list, tuple)):\n        raise ValueError(\"items must be a list or tuple\")\n    for i, sz in enumerate(items):\n        if not isinstance(sz, (int, float)):\n            raise ValueError(f\"Item at index {i} is not a number\")\n        if sz < 0:\n            raise ValueError(f\"Item size must be non\u2011negative, got {sz}\")\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    First\u2011Fit Decreasing bin\u2011packing.\n\n    Returns a list of bins, each containing the original indices of items\n    that fit within the given capacity.\n    \"\"\"\n    _validate_packing_input(items, capacity)\n    if not items:\n        return []\n\n    # Pair each item with its original index and sort by size descending\n    indexed_items = [(idx, size) for idx, size in enumerate(items)]\n    indexed_items.sort(key=lambda x: x[1], reverse=True)\n\n    bins: List[List[int]] = []\n    remaining: List[int] = []  # remaining capacity for each bin\n\n    for idx, size in indexed_items:\n        if size > capacity:\n            # Item too large for any bin; place alone\n            bins.append([idx])\n            remaining.append(0)\n            continue\n\n        placed = False\n        for b_idx, space in enumerate(remaining):\n            if size <= space:\n                bins[b_idx].append(idx)\n                remaining[b_idx] -= size\n                placed = True\n                break\n\n        if not placed:\n            # Open a new bin\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    # Sort indices inside each bin for deterministic output\n    for b in bins:\n        b.sort()\n    return bins\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "a0709d2a-2a60-40ec-9c33-1a995115b37a", "generation": 14, "timestamp": 1764941886.886512, "iteration_found": 668, "metrics": {"runs_successfully": 1.0, "composite_score": 0.4205150578718804, "output_length": 91.0, "overall_score": 0.38035985150742135, "slope_changes": 68.0, "lag_error": 0.9780360456577568, "avg_error": 1.2635358917076887, "false_reversals": 57.8, "correlation": 0.3334964145206185, "noise_reduction": 0.0, "smoothness_score": 0.22727272727272727, "responsiveness_score": 0.5055519600844633, "accuracy_score": 0.3334964145206185, "efficiency_score": 1.0, "execution_time": 0.000911712646484375, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4205150578718804, "output_length": 91.0, "overall_score": 0.38035985150742135, "slope_changes": 68.0, "lag_error": 0.9780360456577568, "avg_error": 1.2635358917076887, "false_reversals": 57.8, "correlation": 0.3334964145206185, "noise_reduction": 0.0, "smoothness_score": 0.22727272727272727, "responsiveness_score": 0.5055519600844633, "accuracy_score": 0.3334964145206185, "efficiency_score": 1.0, "execution_time": 0.000892019271850586, "success_rate": 1.0}, "island": 1}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are to generate only the Python code that belongs inside the `# EVOLVE-BLOCK-START` \u2026 `# EVOLVE-BLOCK-END` section. **Do not** output the markers themselves, any imports, or any surrounding code (e.g., `run_packing`). The system will automatically splice your output into the existing file.\n\n### Core Task\nImprove the implementation of `construct_packing()` and any helper functions it uses. The goal is to produce a correct, efficient packing algorithm that passes all hidden tests.\n\n### Mandatory Output Rules (must be obeyed exactly)\n- **Output ONLY** the lines of code that go **between** the two markers.\n- **No extra text** before, after, or between the code lines.\n- **Do not** include the markers (`# EVOLVE-BLOCK-START` / `# EVOLVE-BLOCK-END`) themselves.\n- **Do not** add imports, global variables, or calls to `run_packing()`.\n\n### What to Avoid (strictly prohibited)\n- Writing an overly\u2011aggressive prompt that leaves the function undefined or returns placeholder values.\n- Leaving the function incomplete, returning `None`, or using `pass`.\n- Introducing syntax errors, undefined names, or mismatched indentation.\n- Repeating the same invalid pattern that caused previous failures (e.g., returning a constant list, ignoring input parameters, or using non\u2011existent helper functions).\n\n### Suggested Concrete Strategies\n1. **Validate Input Early**  \n   - Check that `items` is a list of tuples `(weight, value)` and that `capacity` is a non\u2011negative number. Raise `ValueError` with a clear message if not.\n\n2. **Dynamic Programming (0/1 Knapsack) Implementation**  \n   - Build a DP table `dp[i][w]` where `i` iterates over items and `w` over weight capacities up to `capacity`.  \n   - Use integer indexing; avoid floating\u2011point weights unless the problem explicitly permits them (round them to integers if needed).\n\n3. **Reconstruct the Selected Items**  \n   - After filling the table, backtrack from `dp[n][capacity]` to determine which items are included.  \n   - Return a list of the selected item indices (or the items themselves) in the original order.\n\n4. **Space\u2011Optimized Variant (optional)**  \n   - If memory is a concern, implement a one\u2011dimensional DP array and keep a parallel \u201cchoice\u201d structure to allow reconstruction.\n\n5. **Helper Functions**  \n   - If you need helpers (e.g., `_validate_input`, `_build_dp_table`, `_reconstruct_solution`), define them **inside** the evolve block so they are available to `construct_packing`. Keep them short, well\u2011named, and fully self\u2011contained.\n\n6. **Edge Cases**  \n   - Empty `items` list \u2192 return empty list.  \n   - `capacity` = 0 \u2192 return empty list.  \n   - Items with weight > `capacity` should be automatically excluded during DP filling.\n\n7. **Performance Considerations**  \n   - Use `range` loops, avoid unnecessary list copies, and pre\u2011allocate tables with list comprehensions for speed.\n\n### Example Return Format (you must follow this pattern)\n```python\ndef construct_packing(items, capacity):\n    # ... implementation ...\n    return selected_items  # a list of the original item tuples or their indices\n```\n\nRemember: **Only the code** between the markers will be kept. Follow the rules precisely to avoid invalid submissions. Good luck!", "user": "# Current Program Information\n- Fitness: 14.0152\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.2259 \u2192 14.0152. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0005, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4001, output_length: 91.0000, overall_score: 0.3625, slope_changes: 80.4000, lag_error: 0.9635, avg_error: 1.2731, false_reversals: 65.8000, correlation: 0.3131, noise_reduction: 0.0000, smoothness_score: 0.1992, responsiveness_score: 0.5093, accuracy_score: 0.3131, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 15.2834)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prep(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _expw(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef weighted_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _expw(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(a)\n    y[0] = a[0]\n    for i in range(1, a.size):\n        y[i] = alpha * a[i] + (1 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _resvar(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    d = orig[:m] - filt[:m]\n    return float(np.mean(d * d))\n\ndef _combined(x, ws):\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n    # generate candidate filters\n    cand = {\n        \"basic\": adaptive_filter(a, ws),\n        \"weighted\": weighted_filter(a, ws),\n        \"exp\": exponential_smoothing(a, ws),\n    }\n    # compute residual variances, keep only non\u2011empty results\n    vars = {k: _resvar(a, v) for k, v in cand.items() if v.size}\n    if not vars:\n        return np.empty(0, dtype=float)\n    # perfect match shortcut\n    for k, v in vars.items():\n        if v == 0.0:\n            return cand[k][:len(a) - ws + 1]\n    # inverse\u2011variance weighting (deterministic order)\n    keys = sorted(vars)\n    inv = np.array([1.0 / vars[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    combined = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return combined\n\ndef _best(x, ws):\n    return _combined(x, ws)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    alg = str(algorithm_type).lower()\n    if alg == \"basic\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return _combined(sig, window_size)\n    if alg == \"exp\":\n        return exponential_smoothing(sig, window_size)\n    return _best(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4001), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (80.4000), Performs well on lag_error (0.9635), Performs well on avg_error (1.2731), Performs well on false_reversals (65.8000), Performs well on correlation (0.3131), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.1992), Performs well on responsiveness_score (0.5093), Performs well on accuracy_score (0.3131), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"hybrid\": _hybrid_filter(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        # use hybrid for stronger noise reduction while preserving length\n        return _hybrid_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(a, w):\n    \"\"\"Return True if window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and a.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weights that sum to 1.\"\"\"\n    w = int(w)\n    wts = np.exp(np.linspace(-2, 0, w))\n    return wts / wts.sum()\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Uniform moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    kernel = np.full(w, 1.0 / w, dtype=float)\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef weighted_filter(x, w=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, w=20):\n    \"\"\"Exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, w=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a_f = adaptive_filter(x, w)\n    w_f = weighted_filter(x, w)\n    if a_f.size == 0 and w_f.size == 0:\n        return np.empty(0, dtype=float)\n    if a_f.size == 0:\n        return w_f\n    if w_f.size == 0:\n        return a_f\n    return (a_f + w_f) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean squared error between original (truncated) and filtered signal.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best(x, w):\n    \"\"\"Choose the filter with smallest residual variance against the original.\"\"\"\n    a = _to_array(x)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    w : int, optional\n        Window size (>0). Default 20.\n    alg : str, optional\n        Algorithm selector (case\u2011insensitive):\n        - \"adaptive\"\n        - \"weighted\"\n        - \"ema\"\n        - \"hybrid\" or \"enhanced\"\n        - \"auto\" (or any unknown value) \u2013 automatically pick the best filter.\n    \"\"\"\n    a = _to_array(sig)\n    key = str(alg).lower()\n    if key == \"adaptive\":\n        return adaptive_filter(a, w)\n    if key == \"weighted\":\n        return weighted_filter(a, w)\n    if key == \"ema\":\n        return ema_filter(a, w)\n    if key in (\"hybrid\", \"enhanced\"):\n        return hybrid_filter(a, w)\n    # Auto\u2011selection for \"auto\" or any other value\n    return _select_best(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0005), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:m] - filt[:m]))\n\ndef _select_best_filter(x, window_size):\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": _hybrid_filter(x, window_size),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg == \"basic\" or alg == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return _hybrid_filter(input_signal, window_size)\n    # fallback: automatic selection based on residual variance\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the hybrid filter.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback\n    return hybrid_filter(input_signal, window_size)\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    First\u2011Fit Decreasing bin\u2011packing.\n\n    Args:\n        items: List of non\u2011negative item sizes.\n        capacity: Positive bin capacity.\n\n    Returns:\n        A list of bins, each bin is a list of original item indices that fit\n        within the given capacity.\n    \"\"\"\n    if capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not items:\n        return []\n\n    # Pair each item with its original index\n    indexed_items = [(i, size) for i, size in enumerate(items)]\n    # Sort items by size descending\n    indexed_items.sort(key=lambda x: x[1], reverse=True)\n\n    bins: List[List[int]] = []\n    remaining: List[int] = []\n\n    for idx, size in indexed_items:\n        if size > capacity:\n            # Item too large: allocate its own bin (cannot be packed)\n            bins.append([idx])\n            remaining.append(0)\n            continue\n\n        placed = False\n        for b_idx, cap_left in enumerate(remaining):\n            if size <= cap_left:\n                bins[b_idx].append(idx)\n                remaining[b_idx] -= size\n                placed = True\n                break\n        if not placed:\n            # Open a new bin\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    # Optional: sort indices inside each bin for readability\n    for b in bins:\n        b.sort()\n    return bins\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 14.0152, Type: Migrant)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    \"\"\"Ensure window size is a positive integer.\"\"\"\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef _linear_detrend(arr):\n    \"\"\"Remove linear trend using least\u2011squares regression.\"\"\"\n    n = arr.size\n    if n == 0:\n        return arr\n    t = np.arange(n, dtype=float)\n    A = np.vstack([t, np.ones_like(t)]).T\n    slope, intercept = np.linalg.lstsq(A, arr, rcond=None)[0]\n    return arr - (slope * t + intercept)\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011weighted moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    weights /= weights.sum()\n    return np.convolve(arr, weights, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned to the length of other filters.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _median(x, w):\n    \"\"\"Median filter over a sliding window (valid length).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(\n        arr, shape=shape, strides=(stride, stride)\n    )\n    return np.median(windows, axis=1)\n\n\ndef _combined(x, w):\n    \"\"\"\n    Detrend \u2192 Gaussian weighted moving average \u2192 median smoothing.\n    Returns an empty array if any stage fails.\n    \"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n\n    detrended = _linear_detrend(arr)\n    filtered = _gaussian(detrended, w)\n    if filtered.size == 0:\n        return np.array([], dtype=float)\n\n    return _median(filtered, w)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of (original \u2013 filtered) over the overlapping region.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"\n    Choose the filter with the lowest residual variance.\n    Deterministic tie\u2011break order: combined, gaussian, ema, median, ma.\n    \"\"\"\n    o = _to_numpy(x)\n    if o.size < w:\n        return np.array([], dtype=float)\n\n    candidates = {\n        \"combined\": _combined(o, w),\n        \"gaussian\": _gaussian(o, w),\n        \"ema\": _ema(o, w),\n        \"median\": _median(o, w),\n        \"ma\": _ma(o, w),\n    }\n    variances = {k: _residual_variance(o, v) for k, v in candidates.items()}\n    best_val = min(variances.values())\n    order = (\"combined\", \"gaussian\", \"ema\", \"median\", \"ma\")\n    for k in order:\n        if candidates[k].size and variances[k] == best_val:\n            return candidates[k]\n    return np.array([], dtype=float)\n\n\ndef _chunked_process(sig, w, chunk_size=2000):\n    \"\"\"\n    Process large signals by splitting them into manageable chunks.\n    Each chunk is filtered independently with the best filter and\n    the results are concatenated. Edge effects are ignored for simplicity.\n    \"\"\"\n    sig_arr = _to_numpy(sig)\n    n = sig_arr.size\n    if n <= chunk_size:\n        return _best_filter(sig_arr, w)\n\n    chunks = [sig_arr[i:i + chunk_size] for i in range(0, n, chunk_size)]\n    processed = [_best_filter(chunk, w) for chunk in chunks]\n    # Filtered chunks may be empty; filter those out before concatenation\n    processed = [p for p in processed if p.size]\n    return np.concatenate(processed) if processed else np.array([], dtype=float)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\", **kwargs):\n    \"\"\"\n    Dispatch to a filtering algorithm with optional chunked processing.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" or \"ma\": simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\": exponential moving average\n        - \"median\": median filter\n        - \"enhanced\", \"combined\", \"auto\": automatic best\u2011filter selection\n    Any other value falls back to the automatic selection.\n\n    ``debug`` keyword (bool) adds a ``debug_info`` attribute with elapsed time.\n    \"\"\"\n    _validate_window(w)\n    sig_arr = _to_numpy(sig)\n    if sig_arr.size == 0:\n        return np.array([], dtype=float)\n\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        result = _ma(sig_arr, w)\n    elif alg == \"weighted\":\n        result = _gaussian(sig_arr, w)\n    elif alg == \"ema\":\n        result = _ema(sig_arr, w)\n    elif alg == \"median\":\n        result = _median(sig_arr, w)\n    else:\n        # Use chunked processing for very large inputs to keep memory bounded\n        if sig_arr.size > 5000:\n            result = _chunked_process(sig_arr, w)\n        else:\n            result = _best_filter(sig_arr, w)\n\n    if kwargs.get(\"debug\"):\n        import time\n\n        start = time.time()\n        # No extra work; just record the time taken to reach here\n        elapsed = time.time() - start\n        try:\n            result.debug_info = {\"elapsed\": elapsed}\n        except Exception:\n            pass\n\n    return result\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    \"\"\"Validate input and convert to 1\u2011D float array.\"\"\"\n    arr = np.asarray(x, dtype=float).ravel()\n    w = int(ws)\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if arr.size < w:\n        # Graceful handling: return empty array so callers can proceed\n        return np.empty(0, dtype=float), w\n    return arr, w\n\n@lru_cache(maxsize=32)\ndef _exp_weights(w):\n    \"\"\"Exponential weights for weighted moving average.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\n@lru_cache(maxsize=32)\ndef _gauss_weights(w):\n    \"\"\"Gaussian\u2011shaped kernel.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\ndef _ma(x, w):\n    a, _ = _prepare(x, w)\n    if a.size == 0:\n        return a\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef _weighted(x, w):\n    a, _ = _prepare(x, w)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef _ema(x, w):\n    a, _ = _prepare(x, w)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, a.size):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\ndef _gauss(x, w):\n    a, _ = _prepare(x, w)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _gauss_weights(w), mode=\"valid\")\n\ndef _median(x, w):\n    a, _ = _prepare(x, w)\n    if a.size == 0:\n        return a\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\ndef _hybrid(x, w):\n    ma = _ma(x, w)\n    ga = _gauss(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual on the overlapping segment.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _best(x, w):\n    \"\"\"Select filter with smallest residual variance (deterministic).\"\"\"\n    a, _ = _prepare(x, w)\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"weighted\": _weighted(a, w),\n        \"gauss\": _gauss(a, w),\n        \"median\": _median(a, w),\n        \"hybrid\": _hybrid(a, w),\n    }\n    # discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    # deterministic tie\u2011breaker via sorted keys\n    best_key = min(sorted(candidates), key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Alias for simple moving average.\"\"\"\n    return _ma(x, window_size)\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to requested filter or auto\u2011select the best one.\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\", \"adaptive\"):\n        return _ma(input_signal, window_size)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(input_signal, window_size)\n    if alg in (\"weighted\", \"exp\", \"exponential\"):\n        return _weighted(input_signal, window_size)\n    if alg in (\"gauss\", \"gaussian\"):\n        return _gauss(input_signal, window_size)\n    if alg == \"median\":\n        return _median(input_signal, window_size)\n    if alg == \"hybrid\":\n        return _hybrid(input_signal, window_size)\n    # default / auto\u2011selection\n    return _best(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _a(x):return np.asarray(x,float).ravel()\ndef _v(a,w):return w>0 and a.size>=w\n@lru_cache(maxsize=64)\ndef _e(w):w=int(w);e=np.exp(np.linspace(-2,0,w));return e/e.sum()\n\ndef adaptive(x,w=20):\n    a=_a(x);w=int(w)\n    if not _v(a,w):return np.empty(0,float)\n    return np.convolve(a,np.ones(w)/w,mode=\"valid\")\ndef weighted(x,w=20):\n    a=_a(x);w=int(w)\n    if not _v(a,w):return np.empty(0,float)\n    return np.convolve(a,_e(w),mode=\"valid\")\ndef ema(x,w=20):\n    a=_a(x);w=int(w)\n    if not _v(a,w):return np.empty(0,float)\n    alpha=2.0/(w+1);out=np.empty_like(a);out[0]=a[0]\n    for i in range(1,len(a)):out[i]=alpha*a[i]+(1-alpha)*out[i-1]\n    return out[w-1:]\ndef hybrid(x,w=20):\n    a=adaptive(x,w);b=weighted(x,w)\n    return (a+b)/2.0 if a.size and b.size else (a if a.size else b)\n\ndef _var(o,f):\n    n=min(len(o),len(f))\n    return float(np.var(o[:n]-f[:n])) if n else float(\"inf\")\ndef _best(x,w):\n    a=_a(x);c={'a':adaptive(a,w),'w':weighted(a,w),'e':ema(a,w),'h':hybrid(a,w)}\n    c={k:v for k,v in c.items() if v.size}\n    return min(c.values(),key=lambda v:_var(a,v)) if c else np.empty(0,float)\n\ndef process_signal(sig,w=20,algorithm_type=\"enhanced\"):\n    alg=str(algorithm_type).lower()\n    if alg==\"adaptive\":return adaptive(sig,w)\n    if alg==\"weighted\":return weighted(sig,w)\n    if alg==\"ema\":return ema(sig,w)\n    if alg==\"hybrid\":return hybrid(sig,w)\n    return _best(sig,w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _arr(x):\n    return np.asarray(x, dtype=float).ravel()\n\ndef _ma(x, w):\n    a = _arr(x)\n    return np.convolve(a, np.ones(w) / w, \"valid\") if w > 0 and a.size >= w else np.empty(0)\n\ndef _ema(x, w):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n@lru_cache(maxsize=None)\ndef _gw(w):\n    sigma = 0.3 * w\n    c = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - c) / sigma) ** 2)\n    return wts / wts.sum()\n\ndef _gauss(x, w):\n    a = _arr(x)\n    return np.convolve(a, _gw(w), \"valid\") if w > 0 and a.size >= w else np.empty(0)\n\ndef _median(x, w):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0)\n    s = a.strides[0]\n    shape = (a.size - w + 1, w)\n    win = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(s, s))\n    return np.median(win, axis=1)\n\ndef _resvar(orig, filt):\n    if filt.size == 0:\n        return np.inf\n    n = min(len(orig), len(filt))\n    return np.var(orig[:n] - filt[:n])\n\ndef _best(x, w):\n    a = _arr(x)\n    cand = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gauss(a, w),\n        \"median\": _median(a, w),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0)\n    return min(cand.values(), key=lambda f: _resvar(a, f))\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg in (\"weighted\", \"gauss\", \"gaussian\"):\n        return _gauss(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _arr(x):\n    return np.asarray(x, dtype=float).ravel()\n\ndef _ma(x, w):\n    a = _arr(x)\n    return np.convolve(a, np.ones(w) / w, \"valid\") if w > 0 and a.size >= w else np.empty(0)\n\ndef _ema(x, w):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n@lru_cache(maxsize=None)\ndef _gw(w):\n    sigma = 0.3 * w\n    c = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - c) / sigma) ** 2)\n    return wts / wts.sum()\n\ndef _gauss(x, w):\n    a = _arr(x)\n    return np.convolve(a, _gw(w), \"valid\") if w > 0 and a.size >= w else np.empty(0)\n\ndef _median(x, w):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0)\n    s = a.strides[0]\n    shape = (a.size - w + 1, w)\n    win = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(s, s))\n    return np.median(win, axis=1)\n\ndef _resvar(orig, filt):\n    if filt.size == 0:\n        return np.inf\n    n = min(len(orig), len(filt))\n    return np.var(orig[:n] - filt[:n])\n\ndef _best(x, w):\n    a = _arr(x)\n    cand = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gauss(a, w),\n        \"median\": _median(a, w),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0)\n    return min(cand.values(), key=lambda f: _resvar(a, f))\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg in (\"weighted\", \"gauss\", \"gaussian\"):\n        return _gauss(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    return _best(sig, w)\n\ndef _validate_packing_input(items, capacity):\n    if not isinstance(capacity, int) or capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not isinstance(items, (list, tuple)):\n        raise ValueError(\"items must be a list or tuple\")\n    for i, sz in enumerate(items):\n        if not isinstance(sz, (int, float)):\n            raise ValueError(f\"Item at index {i} is not a number\")\n        if sz < 0:\n            raise ValueError(f\"Item size must be non\u2011negative, got {sz}\")\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    First\u2011Fit Decreasing bin\u2011packing.\n\n    Returns a list of bins, each containing the original indices of items\n    that fit within the given capacity.\n    \"\"\"\n    _validate_packing_input(items, capacity)\n    if not items:\n        return []\n\n    # Pair each item with its original index and sort by size descending\n    indexed_items = [(idx, size) for idx, size in enumerate(items)]\n    indexed_items.sort(key=lambda x: x[1], reverse=True)\n\n    bins: List[List[int]] = []\n    remaining: List[int] = []  # remaining capacity for each bin\n\n    for idx, size in indexed_items:\n        if size > capacity:\n            # Item too large for any bin; place alone\n            bins.append([idx])\n            remaining.append(0)\n            continue\n\n        placed = False\n        for b_idx, space in enumerate(remaining):\n            if size <= space:\n                bins[b_idx].append(idx)\n                remaining[b_idx] -= size\n                placed = True\n                break\n\n        if not placed:\n            # Open a new bin\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    # Sort indices inside each bin for deterministic output\n    for b in bins:\n        b.sort()\n    return bins"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}