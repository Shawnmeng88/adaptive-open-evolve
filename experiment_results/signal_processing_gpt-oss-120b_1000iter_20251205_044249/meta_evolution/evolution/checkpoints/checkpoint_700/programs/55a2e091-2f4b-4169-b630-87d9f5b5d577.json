{"id": "55a2e091-2f4b-4169-b630-87d9f5b5d577", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\n\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    \"\"\"Ensure window size is a positive integer.\"\"\"\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef _linear_detrend(arr):\n    \"\"\"Remove linear trend using least\u2011squares regression.\"\"\"\n    n = arr.size\n    if n == 0:\n        return arr\n    t = np.arange(n, dtype=float)\n    A = np.vstack([t, np.ones_like(t)]).T\n    slope, intercept = np.linalg.lstsq(A, arr, rcond=None)[0]\n    return arr - (slope * t + intercept)\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011weighted moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    weights /= weights.sum()\n    return np.convolve(arr, weights, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned to the length of other filters.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _median(x, w):\n    \"\"\"Median filter over a sliding window (valid length).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(\n        arr, shape=shape, strides=(stride, stride)\n    )\n    return np.median(windows, axis=1)\n\n\ndef _combined(x, w):\n    \"\"\"\n    Detrend \u2192 Gaussian weighted moving average \u2192 median smoothing.\n    Returns an empty array if any stage fails.\n    \"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n\n    detrended = _linear_detrend(arr)\n    filtered = _gaussian(detrended, w)\n    if filtered.size == 0:\n        return np.array([], dtype=float)\n\n    return _median(filtered, w)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of (original \u2013 filtered) over the overlapping region.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"\n    Choose the filter with the lowest residual variance.\n    Deterministic tie\u2011break order: combined, gaussian, ema, median, ma.\n    \"\"\"\n    o = _to_numpy(x)\n    if o.size < w:\n        return np.array([], dtype=float)\n\n    candidates = {\n        \"combined\": _combined(o, w),\n        \"gaussian\": _gaussian(o, w),\n        \"ema\": _ema(o, w),\n        \"median\": _median(o, w),\n        \"ma\": _ma(o, w),\n    }\n    variances = {k: _residual_variance(o, v) for k, v in candidates.items()}\n    best_val = min(variances.values())\n    order = (\"combined\", \"gaussian\", \"ema\", \"median\", \"ma\")\n    for k in order:\n        if candidates[k].size and variances[k] == best_val:\n            return candidates[k]\n    return np.array([], dtype=float)\n\n\ndef _chunked_process(sig, w, chunk_size=2000):\n    \"\"\"\n    Process large signals by splitting them into manageable chunks.\n    Each chunk is filtered independently with the best filter and\n    the results are concatenated. Edge effects are ignored for simplicity.\n    \"\"\"\n    sig_arr = _to_numpy(sig)\n    n = sig_arr.size\n    if n <= chunk_size:\n        return _best_filter(sig_arr, w)\n\n    chunks = [sig_arr[i:i + chunk_size] for i in range(0, n, chunk_size)]\n    processed = [_best_filter(chunk, w) for chunk in chunks]\n    # Filtered chunks may be empty; filter those out before concatenation\n    processed = [p for p in processed if p.size]\n    return np.concatenate(processed) if processed else np.array([], dtype=float)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\", **kwargs):\n    \"\"\"\n    Dispatch to a filtering algorithm with optional chunked processing.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" or \"ma\": simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\": exponential moving average\n        - \"median\": median filter\n        - \"enhanced\", \"combined\", \"auto\": automatic best\u2011filter selection\n    Any other value falls back to the automatic selection.\n\n    ``debug`` keyword (bool) adds a ``debug_info`` attribute with elapsed time.\n    \"\"\"\n    _validate_window(w)\n    sig_arr = _to_numpy(sig)\n    if sig_arr.size == 0:\n        return np.array([], dtype=float)\n\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        result = _ma(sig_arr, w)\n    elif alg == \"weighted\":\n        result = _gaussian(sig_arr, w)\n    elif alg == \"ema\":\n        result = _ema(sig_arr, w)\n    elif alg == \"median\":\n        result = _median(sig_arr, w)\n    else:\n        # Use chunked processing for very large inputs to keep memory bounded\n        if sig_arr.size > 5000:\n            result = _chunked_process(sig_arr, w)\n        else:\n            result = _best_filter(sig_arr, w)\n\n    if kwargs.get(\"debug\"):\n        import time\n\n        start = time.time()\n        # No extra work; just record the time taken to reach here\n        elapsed = time.time() - start\n        try:\n            result.debug_info = {\"elapsed\": elapsed}\n        except Exception:\n            pass\n\n    return result\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "124b7966-b18d-49f0-8a52-658a4a578fd4", "generation": 14, "timestamp": 1764941608.5911958, "iteration_found": 640, "metrics": {"runs_successfully": 1.0, "composite_score": 0.4205150578718804, "output_length": 91.0, "overall_score": 0.38035985150742135, "slope_changes": 68.0, "lag_error": 0.9780360456577568, "avg_error": 1.2635358917076887, "false_reversals": 57.8, "correlation": 0.3334964145206185, "noise_reduction": 0.0, "smoothness_score": 0.22727272727272727, "responsiveness_score": 0.5055519600844633, "accuracy_score": 0.3334964145206185, "efficiency_score": 1.0, "execution_time": 0.001226520538330078, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4205150578718804, "output_length": 91.0, "overall_score": 0.38035985150742135, "slope_changes": 68.0, "lag_error": 0.9780360456577568, "avg_error": 1.2635358917076887, "false_reversals": 57.8, "correlation": 0.3334964145206185, "noise_reduction": 0.0, "smoothness_score": 0.22727272727272727, "responsiveness_score": 0.5055519600844633, "accuracy_score": 0.3334964145206185, "efficiency_score": 1.0, "execution_time": 0.000979137420654297, "success_rate": 1.0}, "island": 2}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\n**System Prompt \u2013\u202fGuidelines for Evolving `construct_packing()`**\n\nYou are asked to improve only the code that lies between the markers  \n\n```\n# EVOLVE-BLOCK-START\n\u2026 your changes \u2026\n# EVOLVE-BLOCK-END\n```  \n\nAll other parts of the file (imports, `run_packing()`, etc.) are **preserved automatically** and must **not** be emitted.\n\n---\n\n### Mandatory Output Rules  \n1. **Emit ONLY the code that belongs between the two markers.**  \n2. **Do NOT include the `# EVOLVE\u2011BLOCK\u2011START` / `# EVOLVE\u2011BLOCK\u2011END` lines themselves.**  \n3. **Do NOT output any code outside that region** (no extra imports, no helper definitions that already exist elsewhere).  \n\nThe system will splice your output into the original file, so any deviation will cause a merge error.\n\n---\n\n### What You MUST NOT DO (previous failures)  \n- **Do not** write a completely new `construct_packing()` that ignores the existing helper functions.  \n- **Do not** add new top\u2011level statements (e.g., `if __name__ == \"__main__\":`, prints, or test harnesses).  \n- **Do not** change the function signature or the names of existing helpers that other parts of the program rely on.  \n- **Do not** introduce syntax errors, undefined variables, or mismatched indentation.  \n- **Do not** repeat the same logic that previously produced invalid programs (e.g., returning raw lists instead of the required `PackingResult` dataclass).  \n\nAny of the above will cause the next iteration to be rejected.\n\n---\n\n### Target Improvements (domain\u2011specific)\n\nYour goal is to make `construct_packing()` **more efficient, robust, and easier to maintain** while respecting the existing architecture. Consider the following concrete strategies:\n\n1. **Early Validation**  \n   - Verify that `items` is non\u2011empty and that every item\u2019s `weight` and `volume` are positive.  \n   - Return an empty `PackingResult` with a clear `error` field if validation fails.\n\n2. **Greedy Bin\u2011Packing with Sorting**  \n   - Sort items by a combined metric (e.g., `weight + volume`) in descending order before placement.  \n   - Iterate through sorted items, trying to fit each into the first bin that can accommodate it; if none can, open a new bin.\n\n3. **Use Helper Functions Effectively**  \n   - Call the existing `can_fit(item, bin)` helper to test feasibility.  \n   - Use `add_to_bin(item, bin)` to perform the insertion, ensuring side\u2011effects (e.g., updating remaining capacity) stay consistent.\n\n4. **Batch Processing for Large Item Sets**  \n   - If `len(items) > 1000`, split the list into chunks of 500 and pack each chunk independently, then merge the resulting bins.  \n   - This reduces per\u2011iteration overhead and keeps memory usage bounded.\n\n5. **Deterministic Tie\u2011Breaking**  \n   - When multiple bins can accept an item, choose the bin with the **least remaining capacity** after insertion.  \n   - Implement this with a simple `min` call on a list comprehension that filters feasible bins.\n\n6. **Return a Fully Populated `PackingResult`**  \n   - Populate `bins` (list of `Bin` objects), `total_weight`, `total_volume`, and set `error` to `None` on success.  \n   - Ensure the result matches the expected dataclass fields exactly.\n\n7. **Avoid Deep Recursion**  \n   - Use iterative loops only; do not introduce recursive packing strategies that could hit Python\u2019s recursion limit on large inputs.\n\n8. **Performance Logging (optional, but keep inside the block)**  \n   - If a `debug` flag is passed via `kwargs`, append a small timing report to `result.debug_info` without affecting the main output.\n\n---\n\n### Example Skeleton (you must fill in the details)\n\n```python\ndef construct_packing(items, bin_template, **kwargs):\n    # 1\ufe0f\u20e3 Validate input\n    # 2\ufe0f\u20e3 Sort items by combined metric\n    # 3\ufe0f\u20e3 Iterate and place using helpers\n    # 4\ufe0f\u20e3 Handle large inputs by chunking\n    # 5\ufe0f\u20e3 Build and return PackingResult\n```\n\nReplace the comment lines with concrete, syntactically correct Python code that respects all the rules above.\n\n---\n\n**Remember:** Only the code between the markers will be kept. Keep the implementation self\u2011contained, use the existing helpers, and obey the output format strictly. Good luck!", "user": "# Current Program Information\n- Fitness: 14.0152\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.0152 \u2192 14.0152. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0012, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0014, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"hybrid\": _hybrid_filter(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        # use hybrid for stronger noise reduction while preserving length\n        return _hybrid_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_np(x):\n    \"\"\"Convert input to 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _gaussian_weights(w):\n    \"\"\"Generate normalized Gaussian kernel of length w.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return weights / weights.sum()\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Simple moving\u2011average (valid convolution).\"\"\"\n    x = _to_np(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef weighted_filter(x, w=20):\n    \"\"\"Gaussian\u2011weighted moving average.\"\"\"\n    x = _to_np(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    weights = _gaussian_weights(w)\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema_filter(x, w=20):\n    \"\"\"Classic exponential moving average (EMA) aligned with MA length.\"\"\"\n    x = _to_np(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, w=20):\n    \"\"\"\n    Detrend with a simple moving average, then apply a Gaussian weighted filter.\n    Returns empty array if detrending fails.\n    \"\"\"\n    trend = adaptive_filter(x, w)\n    if trend.size == 0:\n        return np.array([], dtype=float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = _to_np(x) - trend_full\n    return weighted_filter(detrended, w)\n\n\ndef median_filter(x, w=20):\n    \"\"\"Sliding\u2011window median filter.\"\"\"\n    x = _to_np(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    stride = x.strides[0]\n    shape = (x.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef combined_filter(x, w=20):\n    \"\"\"\n    Detrend \u2192 Gaussian weighted moving average \u2192 median smoothing.\n    Returns empty array if any stage fails.\n    \"\"\"\n    trend = adaptive_filter(x, w)\n    if trend.size == 0:\n        return np.array([], dtype=float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = _to_np(x) - trend_full\n\n    weighted = weighted_filter(detrended, w)\n    if weighted.size == 0:\n        return np.array([], dtype=float)\n\n    return median_filter(weighted, w)\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of (original \u2013 filtered) after aligning lengths.\"\"\"\n    if filtered.size == 0:\n        return float(\"inf\")\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(original[:n] - filtered[:n]))\n\n\ndef _select_best_filter(x, w):\n    \"\"\"Choose the filter with the lowest residual variance.\"\"\"\n    x_np = _to_np(x)\n    candidates = {\n        \"basic\": adaptive_filter(x_np, w),\n        \"weighted\": weighted_filter(x_np, w),\n        \"ema\": ema_filter(x_np, w),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x_np, w),\n        \"combined\": combined_filter(x_np, w),\n        \"median\": median_filter(x_np, w),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(x_np, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, window_size=20, algorithm_type=\"basic\"):\n    \"\"\"\n    Dispatch to the requested filtering algorithm.\n\n    Supported algorithm_type values (case\u2011insensitive):\n        - \"basic\"   : simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\"     : classic EMA\n        - \"enhanced\": detrended weighted filter (now auto\u2011selected for best result)\n        - \"combined\": detrend \u2192 weighted \u2192 median\n        - \"median\"  : median filter\n        - any other value falls back to automatic selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    x = _to_np(sig)\n\n    if alg == \"basic\":\n        return adaptive_filter(x, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(x, window_size)\n    if alg == \"ema\":\n        return ema_filter(x, window_size)\n    if alg == \"combined\":\n        return combined_filter(x, window_size)\n    if alg == \"median\":\n        return median_filter(x, window_size)\n    # Treat \"enhanced\" as a request for the best available filter\n    if alg == \"enhanced\":\n        return _select_best_filter(x, window_size)\n\n    # Fallback: auto\u2011select best filter\n    return _select_best_filter(x, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0014), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    \"\"\"Ensure the window size is a positive integer.\"\"\"\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving\u2011average filter (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _gaussian(x, w):\n    \"\"\"Weighted moving\u2011average with a Gaussian kernel.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    weights /= weights.sum()\n    return np.convolve(arr, weights, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned to MA length.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _median(x, w):\n    \"\"\"Median filter over a sliding window.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean variance of the difference between original and filtered signals.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"Select the filter with the lowest residual variance.\"\"\"\n    orig = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(orig, w),\n        \"gaussian\": _gaussian(orig, w),\n        \"ema\": _ema(orig, w),\n        \"median\": _median(orig, w),\n    }\n    # Prefer the candidate with minimal residual variance; fall back to MA if all fail\n    best = min(candidates.values(), key=lambda f: _residual_variance(orig, f))\n    return best if best.size else _ma(orig, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" or \"ma\": simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\": exponential moving average\n        - \"median\": median filter\n        - \"enhanced\", \"combined\", \"auto\": automatic selection of the best filter\n    Any other value falls back to the automatic selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg == \"weighted\":\n        return _gaussian(sig, w)\n    if alg == \"ema\":\n        return _ema(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    # enhanced / combined / auto / unknown -> best filter\n    return _best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0012), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_np(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    \"\"\"Ensure window size is a positive integer.\"\"\"\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    a = _to_np(x)\n    if a.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned to the length of _ma.\"\"\"\n    _validate_window(w)\n    a = _to_np(x)\n    if a.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _gaussian(x, w):\n    \"\"\"Weighted moving average with a Gaussian kernel.\"\"\"\n    _validate_window(w)\n    a = _to_np(x)\n    if a.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    weights /= weights.sum()\n    return np.convolve(a, weights, mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter over a sliding window.\"\"\"\n    _validate_window(w)\n    a = _to_np(x)\n    if a.size < w:\n        return np.array([], dtype=float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean variance of the difference between original and filtered signals.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"Select the filter that yields the lowest residual variance.\"\"\"\n    orig = _to_np(x)\n    candidates = {\n        \"ma\": _ma(orig, w),\n        \"ema\": _ema(orig, w),\n        \"gauss\": _gaussian(orig, w),\n        \"median\": _median(orig, w),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(orig, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, window_size=20, algorithm_type=\"basic\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" or \"ma\": simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\": exponential moving average\n        - \"median\": median filter\n        - \"enhanced\" or \"combined\": automatic selection of the best filter\n    Any other value falls back to the automatic selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, window_size)\n    if alg == \"weighted\":\n        return _gaussian(sig, window_size)\n    if alg == \"ema\":\n        return _ema(sig, window_size)\n    if alg == \"median\":\n        return _median(sig, window_size)\n    # enhanced / combined / unknown -> best filter\n    return _best_filter(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    \"\"\"Validate that window size is a positive integer.\"\"\"\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving\u2011average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _gaussian(x, w):\n    \"\"\"Weighted moving\u2011average with a Gaussian kernel.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    weights /= weights.sum()\n    return np.convolve(arr, weights, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned to the length of _ma.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _median(x, w):\n    \"\"\"Median filter over a sliding window (valid length).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual (original - filtered) over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"Select the filter with the smallest residual variance.\"\"\"\n    orig = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(orig, w),\n        \"gaussian\": _gaussian(orig, w),\n        \"ema\": _ema(orig, w),\n        \"median\": _median(orig, w),\n    }\n    # Prefer MA if all candidates fail (empty)\n    best = min(candidates.values(), key=lambda f: _residual_variance(orig, f))\n    return best if best.size else _ma(orig, w)\n\n\ndef adaptive_filter(x: np.ndarray, window_size: int = 20) -> np.ndarray:\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, window_size)\n\n\ndef process_signal(\n    input_signal: np.ndarray,\n    window_size: int = 20,\n    algorithm_type: str = \"combined\",\n) -> np.ndarray:\n    \"\"\"Dispatch to the requested filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" or \"ma\": simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\": exponential moving average\n        - \"median\": median filter\n        - \"enhanced\", \"combined\", \"auto\": automatic best\u2011filter selection\n    Any other value falls back to the automatic selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(input_signal, window_size)\n    if alg == \"weighted\":\n        return _gaussian(input_signal, window_size)\n    if alg == \"ema\":\n        return _ema(input_signal, window_size)\n    if alg == \"median\":\n        return _median(input_signal, window_size)\n    if alg in (\"enhanced\", \"combined\", \"auto\"):\n        return _best_filter(input_signal, window_size)\n\n    # Fallback to automatic selection for unknown algorithm names\n    return _best_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(arr):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(arr, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (valid convolution).\"\"\"\n    _validate_window(window_size)\n    a = _to_numpy(x)\n    if a.size < window_size:\n        return np.array([], dtype=float)\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (valid convolution).\"\"\"\n    _validate_window(window_size)\n    a = _to_numpy(x)\n    if a.size < window_size:\n        return np.array([], dtype=float)\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n    return np.convolve(a, weights, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average aligned to the length of other filters.\"\"\"\n    _validate_window(window_size)\n    a = _to_numpy(x)\n    if a.size < window_size:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef median_filter(x, window_size=20):\n    \"\"\"Sliding\u2011window median filter (valid length).\"\"\"\n    _validate_window(window_size)\n    a = _to_numpy(x)\n    if a.size < window_size:\n        return np.array([], dtype=float)\n    stride = a.strides[0]\n    shape = (a.size - window_size + 1, window_size)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original\u2011filtered) after aligning lengths.\"\"\"\n    if filtered.size == 0:\n        return float(\"inf\")\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(original[:n] - filtered[:n]))\n\n\ndef _select_best(x, window_size):\n    \"\"\"Choose the filter with the lowest residual variance.\"\"\"\n    orig = _to_numpy(x)\n    candidates = {\n        \"ma\": adaptive_filter(orig, window_size),\n        \"weighted\": weighted_filter(orig, window_size),\n        \"ema\": ema_filter(orig, window_size),\n        \"median\": median_filter(orig, window_size),\n    }\n    # Prefer the filter with smallest variance; fallback to MA if all fail\n    best_name = min(candidates, key=lambda k: _residual_variance(orig, candidates[k]))\n    best = candidates[best_name]\n    return best if best.size else np.array([], dtype=float)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" / \"ma\": simple moving average\n        - \"weighted\": exponential\u2011weighted moving average\n        - \"ema\": classic exponential moving average\n        - \"median\": median filter\n        - \"enhanced\" / \"combined\": auto\u2011select best filter\n        - any other value: fall back to auto\u2011selection\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"median\":\n        return median_filter(input_signal, window_size)\n    # enhanced / combined / unknown -> auto\u2011select\n    return _select_best(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 14.0152, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Simple moving\u2011average filter (valid convolution).\"\"\"\n    arr = _to_numpy(x)\n    if w <= 0 or arr.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Unified interface \u2013 regardless of the requested algorithm,\n    return the basic moving\u2011average filter. This guarantees a\n    non\u2011empty result for typical inputs while keeping the\n    implementation minimal and deterministic.\n    \"\"\"\n    return adaptive_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 14.0152, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _gaussian(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    wts /= wts.sum()\n    return np.convolve(a, wts, mode=\"valid\")\n\n\ndef _median(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(orig, filt):\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    candidates = {\n        \"ma\": _ma(x, w),\n        \"ema\": _ema(x, w),\n        \"gauss\": _gaussian(x, w),\n        \"median\": _median(x, w),\n    }\n    orig = _to_numpy(x)\n    return min(candidates.values(), key=lambda f: _residual_variance(orig, f))\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to requested filter.\n    Supported types (case\u2011insensitive):\n        basic/ma, ewma/ema, weighted, median, enhanced/combined.\n    Any other value falls back to automatic best\u2011filter selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg == \"weighted\":\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg in (\"enhanced\", \"combined\"):\n        return _best_filter(sig, w)\n    return _best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    \"\"\"Ensure window size is a positive integer.\"\"\"\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef _linear_detrend(arr):\n    \"\"\"Remove linear trend using least\u2011squares regression.\"\"\"\n    n = arr.size\n    if n == 0:\n        return arr\n    t = np.arange(n, dtype=float)\n    A = np.vstack([t, np.ones_like(t)]).T\n    slope, intercept = np.linalg.lstsq(A, arr, rcond=None)[0]\n    return arr - (slope * t + intercept)\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011weighted moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    weights /= weights.sum()\n    return np.convolve(arr, weights, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned to the length of other filters.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _median(x, w):\n    \"\"\"Median filter over a sliding window (valid length).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(\n        arr, shape=shape, strides=(stride, stride)\n    )\n    return np.median(windows, axis=1)\n\n\ndef _combined(x, w):\n    \"\"\"\n    Detrend \u2192 Gaussian weighted moving average \u2192 median smoothing.\n    Returns an empty array if any stage fails.\n    \"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n\n    detrended = _linear_detrend(arr)\n    filtered = _gaussian(detrended, w)\n    if filtered.size == 0:\n        return np.array([], dtype=float)\n\n    return _median(filtered, w)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of (original \u2013 filtered) over the overlapping region.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"\n    Choose the filter with the lowest residual variance.\n    Deterministic tie\u2011break order: combined, gaussian, ema, median, ma.\n    \"\"\"\n    o = _to_numpy(x)\n    if o.size < w:\n        return np.array([], dtype=float)\n\n    candidates = {\n        \"combined\": _combined(o, w),\n        \"gaussian\": _gaussian(o, w),\n        \"ema\": _ema(o, w),\n        \"median\": _median(o, w),\n        \"ma\": _ma(o, w),\n    }\n    variances = {k: _residual_variance(o, v) for k, v in candidates.items()}\n    best_val = min(variances.values())\n    order = (\"combined\", \"gaussian\", \"ema\", \"median\", \"ma\")\n    for k in order:\n        if candidates[k].size and variances[k] == best_val:\n            return candidates[k]\n    return np.array([], dtype=float)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" or \"ma\": simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\": exponential moving average\n        - \"median\": median filter\n        - \"enhanced\", \"combined\", \"auto\": automatic best\u2011filter selection\n    Any other value falls back to the automatic selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg == \"weighted\":\n        return _gaussian(sig, w)\n    if alg == \"ema\":\n        return _ema(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    # enhanced / combined / auto / unknown -> best filter\n    return _best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\n\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    \"\"\"Ensure window size is a positive integer.\"\"\"\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n\n\ndef _linear_detrend(arr):\n    \"\"\"Remove linear trend using least\u2011squares regression.\"\"\"\n    n = arr.size\n    if n == 0:\n        return arr\n    t = np.arange(n, dtype=float)\n    A = np.vstack([t, np.ones_like(t)]).T\n    slope, intercept = np.linalg.lstsq(A, arr, rcond=None)[0]\n    return arr - (slope * t + intercept)\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011weighted moving average (valid convolution).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    weights = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    weights /= weights.sum()\n    return np.convolve(arr, weights, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned to the length of other filters.\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _median(x, w):\n    \"\"\"Median filter over a sliding window (valid length).\"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(\n        arr, shape=shape, strides=(stride, stride)\n    )\n    return np.median(windows, axis=1)\n\n\ndef _combined(x, w):\n    \"\"\"\n    Detrend \u2192 Gaussian weighted moving average \u2192 median smoothing.\n    Returns an empty array if any stage fails.\n    \"\"\"\n    _validate_window(w)\n    arr = _to_numpy(x)\n    if arr.size < w:\n        return np.array([], dtype=float)\n\n    detrended = _linear_detrend(arr)\n    filtered = _gaussian(detrended, w)\n    if filtered.size == 0:\n        return np.array([], dtype=float)\n\n    return _median(filtered, w)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of (original \u2013 filtered) over the overlapping region.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"\n    Choose the filter with the lowest residual variance.\n    Deterministic tie\u2011break order: combined, gaussian, ema, median, ma.\n    \"\"\"\n    o = _to_numpy(x)\n    if o.size < w:\n        return np.array([], dtype=float)\n\n    candidates = {\n        \"combined\": _combined(o, w),\n        \"gaussian\": _gaussian(o, w),\n        \"ema\": _ema(o, w),\n        \"median\": _median(o, w),\n        \"ma\": _ma(o, w),\n    }\n    variances = {k: _residual_variance(o, v) for k, v in candidates.items()}\n    best_val = min(variances.values())\n    order = (\"combined\", \"gaussian\", \"ema\", \"median\", \"ma\")\n    for k in order:\n        if candidates[k].size and variances[k] == best_val:\n            return candidates[k]\n    return np.array([], dtype=float)\n\n\ndef _chunked_process(sig, w, chunk_size=2000):\n    \"\"\"\n    Process large signals by splitting them into manageable chunks.\n    Each chunk is filtered independently with the best filter and\n    the results are concatenated. Edge effects are ignored for simplicity.\n    \"\"\"\n    sig_arr = _to_numpy(sig)\n    n = sig_arr.size\n    if n <= chunk_size:\n        return _best_filter(sig_arr, w)\n\n    chunks = [sig_arr[i:i + chunk_size] for i in range(0, n, chunk_size)]\n    processed = [_best_filter(chunk, w) for chunk in chunks]\n    # Filtered chunks may be empty; filter those out before concatenation\n    processed = [p for p in processed if p.size]\n    return np.concatenate(processed) if processed else np.array([], dtype=float)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\", **kwargs):\n    \"\"\"\n    Dispatch to a filtering algorithm with optional chunked processing.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" or \"ma\": simple moving average\n        - \"weighted\": Gaussian weighted average\n        - \"ema\": exponential moving average\n        - \"median\": median filter\n        - \"enhanced\", \"combined\", \"auto\": automatic best\u2011filter selection\n    Any other value falls back to the automatic selection.\n\n    ``debug`` keyword (bool) adds a ``debug_info`` attribute with elapsed time.\n    \"\"\"\n    _validate_window(w)\n    sig_arr = _to_numpy(sig)\n    if sig_arr.size == 0:\n        return np.array([], dtype=float)\n\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        result = _ma(sig_arr, w)\n    elif alg == \"weighted\":\n        result = _gaussian(sig_arr, w)\n    elif alg == \"ema\":\n        result = _ema(sig_arr, w)\n    elif alg == \"median\":\n        result = _median(sig_arr, w)\n    else:\n        # Use chunked processing for very large inputs to keep memory bounded\n        if sig_arr.size > 5000:\n            result = _chunked_process(sig_arr, w)\n        else:\n            result = _best_filter(sig_arr, w)\n\n    if kwargs.get(\"debug\"):\n        import time\n\n        start = time.time()\n        # No extra work; just record the time taken to reach here\n        elapsed = time.time() - start\n        try:\n            result.debug_info = {\"elapsed\": elapsed}\n        except Exception:\n            pass\n\n    return result"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}