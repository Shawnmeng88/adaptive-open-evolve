{"id": "c24dcfc5-c5d9-4324-9b26-18290f076e85", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _ma(x, w, wt):\n    \"\"\"Convolution helper for moving\u2011average\u2011style filters.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef simple_ma(x, w):\n    \"\"\"Uniform moving average.\"\"\"\n    return _ma(x, w, np.ones(w) / w)\n\n\ndef weighted_ma(x, w):\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return _ma(x, w, wt)\n\n\ndef ema(x, w):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    a = 2.0 / (w + 1)\n    out = np.empty_like(x)\n    out[0] = x[0]\n    for i in range(1, len(x)):\n        out[i] = a * x[i] + (1 - a) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef detrended_weighted(x, w):\n    \"\"\"Detrend with simple MA then apply weighted MA.\"\"\"\n    tr = simple_ma(x, w)\n    if tr.size == 0:\n        return np.empty(0, float)\n    full = np.concatenate((np.full(w - 1, tr[0]), tr))\n    return weighted_ma(x - full, w)\n\n\ndef gaussian_ma(x, w):\n    \"\"\"Moving average with Gaussian kernel.\"\"\"\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    sigma = max(w / 6.0, 0.1)\n    centre = (w - 1) / 2.0\n    i = np.arange(w, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, w, wt)\n\n\ndef median_ma(x, w):\n    \"\"\"Median filter \u2013 robust to spikes.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    out_len = x.size - w + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(x[i : i + w])\n    return out\n\n\ndef double_enhanced(x, w):\n    \"\"\"Two\u2011stage: detrended weighted then light uniform smoothing.\"\"\"\n    first = detrended_weighted(x, w)\n    if first.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(first, half)\n\n\ndef advanced_filter(x, w):\n    \"\"\"\n    Multi\u2011stage enhanced filter:\n    1. Detrended weighted smoothing.\n    2. Median filter to suppress spikes.\n    3. Light uniform smoothing with a smaller window.\n    \"\"\"\n    stage1 = detrended_weighted(x, w)\n    if stage1.size == 0:\n        return np.empty(0, float)\n    stage2 = median_ma(stage1, w)\n    if stage2.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(stage2, half)\n\n\ndef _residual_variance(orig: np.ndarray, filt: np.ndarray) -> float:\n    \"\"\"Mean\u2011squared residual between original and filtered signals.\"\"\"\n    m = min(len(orig), len(filt))\n    return np.inf if m == 0 else float(np.var(orig[:m] - filt[:m]))\n\n\ndef _select_best(x, w):\n    \"\"\"Pick the filter with the lowest residual variance (ties \u2192 longest output).\"\"\"\n    candidates = {\n        \"basic\": simple_ma(x, w),\n        \"weighted\": weighted_ma(x, w),\n        \"ema\": ema(x, w),\n        \"enhanced\": detrended_weighted(x, w),\n        \"gaussian\": gaussian_ma(x, w),\n        \"median\": median_ma(x, w),\n        \"double\": double_enhanced(x, w),\n        \"advanced\": advanced_filter(x, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, float)\n\n    # Find minimal residual variance\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    # Resolve ties by choosing the longest output\n    best_score = _residual_variance(x, candidates[best_key])\n    tied = [k for k, v in candidates.items()\n            if _residual_variance(x, v) == best_score]\n    if len(tied) > 1:\n        best_key = max(tied, key=lambda k: len(candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, algo=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm, falling back to automatic selection.\"\"\"\n    dispatch = {\n        \"basic\": simple_ma,\n        \"weighted\": weighted_ma,\n        \"ema\": ema,\n        \"enhanced\": detrended_weighted,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"double\": double_enhanced,\n        \"advanced\": advanced_filter,\n    }\n    if algo in dispatch:\n        return dispatch[algo](sig, w)\n    return _select_best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "9b02cf11-f940-4d70-8cfb-d4ea75cfd176", "generation": 10, "timestamp": 1764939166.15597, "iteration_found": 341, "metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.3624766970140094, "slope_changes": 68.6, "lag_error": 0.4099887786960041, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.20076318890739916, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.709225502436145, "accuracy_score": 0.20076318890739916, "efficiency_score": 1.0, "execution_time": 0.00038361549377441406, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.3624766970140094, "slope_changes": 68.6, "lag_error": 0.4099887786960041, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.20076318890739916, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.709225502436145, "accuracy_score": 0.20076318890739916, "efficiency_score": 1.0, "execution_time": 0.0005163669586181641, "success_rate": 1.0}, "island": 3}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\n**System Prompt \u2013\u202fGuidelines for Evolving `construct_packing()`**\n\nYou are tasked with improving the `construct_packing()` function (and any helper functions inside the evolve block). Follow these strict rules when generating your response:\n\n1. **Output ONLY the code that belongs between the markers**  \n   - `# EVOLVE-BLOCK-START` \u2026 `# EVOLVE-BLOCK-END`  \n   - Do **not** include the marker lines themselves.  \n   - Do **not** add any imports, top\u2011level statements, or code outside the block (e.g., `run_packing()`). The surrounding file will be merged automatically.\n\n2. **Maintain syntactic correctness**  \n   - The block must be a self\u2011contained, syntactically valid Python snippet that can be inserted directly into the existing file.  \n   - Preserve the original function signatures; only modify the bodies or add new helper functions **inside** the block.\n\n3. **Avoid the pitfalls that caused previous failures**  \n   - **Do not** make the prompt overly aggressive or vague about constraints.  \n   - **Do not** produce code that relies on undefined variables, missing imports, or external libraries not already present.  \n   - **Do not** repeat the same broken pattern across iterations (e.g., returning the wrong type, missing return statements, or using global state incorrectly).  \n\n4. **Strategies to improve `construct_packing()`**  \n   - **Incremental refinement:** start by preserving the existing algorithmic flow, then replace one sub\u2011step at a time with a clearer, more efficient version.  \n   - **Explicit type handling:** add type hints and runtime checks for inputs (e.g., ensure `items` is a list of `(weight, value)` tuples).  \n   - **Modular helpers:** if the current logic is monolithic, split it into small, well\u2011named helper functions (e.g., `_sort_items`, `_fill_bins`, `_validate_solution`). Keep these helpers inside the evolve block.  \n   - **Greedy + fallback:** implement a primary greedy packing strategy and, if it fails to meet a simple feasibility check, fall back to a deterministic secondary method (e.g., first\u2011fit decreasing).  \n   - **Early exit & validation:** after constructing a packing, run a quick validation pass that verifies capacity constraints; if it fails, return a default safe packing (e.g., empty bins) rather than raising an exception.  \n   - **Performance\u2011aware loops:** avoid unnecessary nested loops; use list comprehensions or `heapq` where appropriate, but only if the required imports already exist.  \n\n5. **Testing mindset**  \n   - Write the code as if it will be immediately executed by the hidden test harness. Assume typical inputs: a list of items, a bin capacity, and a maximum number of bins.  \n   - Return a data structure that matches the original contract (e.g., a list of bins, each bin being a list of item indices).  \n\n**Remember:** your entire response must be *only* the revised code body for the evolve block\u2014no explanatory text, no markdown, no markers. The system will insert it into the surrounding file automatically.", "user": "# Current Program Information\n- Fitness: 13.9292\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 13.9292 \u2192 13.9292. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n    Returns the filtered signal with length len(x) - window_size + 1.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        # Not enough data \u2013 return empty array for consistency\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"\n    Exponential weighted moving average.\n    Emphasises recent samples while preserving overall trend.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"\n    Classic exponential moving average (EMA).\n    Output length matches other filters (len(x) - window_size + 1).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Detrends the signal using a simple moving average, then applies\n    a weighted moving average to the detrended series.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    # Estimate local trend\n    trend = adaptive_filter(x, window_size)\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = x - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"\n    Compute variance of the residual (original - filtered).\n    Signals are aligned to the shortest length.\n    \"\"\"\n    min_len = min(len(original), len(filtered))\n    if min_len == 0:\n        return np.inf\n    residual = original[:min_len] - filtered[:min_len]\n    return float(np.var(residual))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"\n    Evaluate all available filters and return the one that yields the lowest\n    residual variance (i.e., highest estimated noise reduction).\n    \"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    # Choose filter with minimal residual variance relative to the raw signal\n    best_name = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_name]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatches to the requested filtering algorithm.\n    Supported types:\n        - \"basic\": simple moving average (adaptive_filter)\n        - \"weighted\": exponential weighted moving average (weighted_filter)\n        - \"ema\": classic exponential moving average (ema_filter)\n        - \"enhanced\": detrended weighted filter (enhanced_filter_with_trend_preservation)\n        - any other value: automatic selection of the best filter based on residual variance.\n    \"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if algorithm_type == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n\n    # Fallback: automatic selection\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive(x, w=20):\n    \"\"\"Simple moving average filter.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if w <= 0 or x.size < w:\n        return np.empty(0, dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef weighted(x, w=20):\n    \"\"\"Exponential weighted moving average filter.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if w <= 0 or x.size < w:\n        return np.empty(0, dtype=float)\n    weights = np.exp(np.linspace(-2, 0, w))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema(x, w=20):\n    \"\"\"Classic exponential moving average (EMA).\"\"\"\n    x = np.asarray(x, dtype=float)\n    if w <= 0 or x.size < w:\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(x)\n    e[0] = x[0]\n    for i in range(1, len(x)):\n        e[i] = alpha * x[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\n\ndef enhanced(x, w=20):\n    \"\"\"Detrend with simple moving average, then apply weighted filter.\"\"\"\n    trend = adaptive(x, w)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    return weighted(x - trend_full, w)\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original - filtered) over the overlapping region.\"\"\"\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(original[:n] - filtered[:n]))\n\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Dispatch to the requested filtering algorithm.\n    Supported types (case\u2011insensitive):\n        - \"basic\" / \"adaptive\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\"\n        - \"auto\" (or any unknown value) \u2192 automatic selection based on residual variance.\n    \"\"\"\n    alg = alg.lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive(sig, w)\n    if alg == \"weighted\":\n        return weighted(sig, w)\n    if alg == \"ema\":\n        return ema(sig, w)\n    if alg == \"enhanced\":\n        return enhanced(sig, w)\n\n    # Automatic selection (fallback)\n    candidates = {\n        \"basic\": adaptive(sig, w),\n        \"weighted\": weighted(sig, w),\n        \"ema\": ema(sig, w),\n        \"enhanced\": enhanced(sig, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(sig, candidates[k]))\n    return candidates[best_key]\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n    Returns the filtered signal with length len(x) - window_size + 1.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"\n    Exponential weighted moving average.\n    Emphasises recent samples while preserving overall trend.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"\n    Classic exponential moving average (EMA).\n    Output length matches other filters (len(x) - window_size + 1).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Detrends the signal using a simple moving average, then applies\n    an exponential weighted filter to the detrended series.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    # Estimate local trend\n    trend = adaptive_filter(x, window_size)\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate(\n        (np.full(window_size - 1, trend[0]), trend)\n    )\n    detrended = x - trend_full\n    # Apply weighted smoothing\n    return weighted_filter(detrended, window_size)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatches to the requested filtering algorithm.\n    Types:\n        - \"basic\": simple moving average\n        - \"ema\": exponential moving average\n        - \"enhanced\": detrended weighted filter\n    \"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"ema\":\n        return ema_filter(input_signal, window_size)\n    return enhanced_filter_with_trend_preservation(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_signal(x):\n    try:\n        a = np.asarray(x, float).ravel()\n    except Exception:\n        return np.empty(0, float)\n    return a\n\ndef _ma(x, w, wt):\n    a = _validate_signal(x)\n    if w <= 0:\n        raise ValueError(\"window must be positive\")\n    if a.size < w:\n        return np.empty(0, float)\n    return np.convolve(a, wt, mode=\"valid\")\n\ndef adaptive_filter(x, window_size=20):\n    return _ma(x, window_size, np.ones(window_size) / window_size)\n\ndef weighted_filter(x, window_size=20):\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\ndef ema_filter(x, window_size=20):\n    a = _validate_signal(x)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if a.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = _validate_signal(x) - trend_full\n    return weighted_filter(detrended, window_size)\n\ndef double_enhanced(x, window_size=20):\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    w2 = max(1, window_size // 2)\n    return adaptive_filter(first, w2)\n\ndef gaussian_filter(x, window_size=20):\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    sigma = max(window_size / 6.0, 0.1)\n    centre = (window_size - 1) / 2.0\n    i = np.arange(window_size, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\ndef median_filter(x, window_size=20):\n    a = _validate_signal(x)\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    if a.size < window_size:\n        return np.empty(0, float)\n    out_len = a.size - window_size + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(a[i:i + window_size])\n    return out\n\ndef advanced_filter(x, window_size=20):\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    medianed = median_filter(first, window_size)\n    if medianed.size == 0:\n        return np.empty(0, float)\n    w2 = max(1, window_size // 2)\n    return adaptive_filter(medianed, w2)\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return np.inf\n    return float(np.var(orig[:m] - filt[:m]))\n\ndef _select_best_filter(x, window_size):\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"double\": double_enhanced(x, window_size),\n        \"gaussian\": gaussian_filter(x, window_size),\n        \"median\": median_filter(x, window_size),\n        \"advanced\": advanced_filter(x, window_size),\n    }\n    best_var = min(_residual_variance(x, v) for v in candidates.values())\n    best_keys = [k for k, v in candidates.items() if _residual_variance(x, v) == best_var]\n    # tie\u2011break by longest output\n    best_key = max(best_keys, key=lambda k: len(candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter_with_trend_preservation,\n        \"double\": double_enhanced,\n        \"gaussian\": gaussian_filter,\n        \"median\": median_filter,\n        \"advanced\": advanced_filter,\n    }\n    alg = algorithm_type.lower()\n    if alg in dispatch:\n        return dispatch[alg](input_signal, window_size)\n    return _select_best_filter(_validate_signal(input_signal), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef simple_ma(x, w):\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef weighted_ma(x, w):\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    weights = np.exp(np.linspace(-2, 0, w))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema(x, w):\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    a = 2.0 / (w + 1)\n    out = np.empty_like(x)\n    out[0] = x[0]\n    for i in range(1, len(x)):\n        out[i] = a * x[i] + (1 - a) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef detrended_weighted(x, w):\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    trend = simple_ma(x, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = x - trend_full\n    return weighted_ma(detrended, w)\n\n\ndef process_signal(sig, w=20, algo=\"enhanced\"):\n    \"\"\"\n    Apply the selected filtering algorithm.\n    Supported algo values (case\u2011sensitive):\n        \"basic\", \"weighted\", \"ema\", \"enhanced\"\n    Any other value defaults to the enhanced (detrended weighted) filter.\n    \"\"\"\n    if algo == \"basic\":\n        return simple_ma(sig, w)\n    if algo == \"weighted\":\n        return weighted_ma(sig, w)\n    if algo == \"ema\":\n        return ema(sig, w)\n    if algo == \"enhanced\":\n        return detrended_weighted(sig, w)\n    return detrended_weighted(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _ma(x: np.ndarray, w: int, wt: np.ndarray) -> np.ndarray:\n    \"\"\"Convolution helper for moving\u2011average\u2011style filters.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Uniform moving average (basic filter).\"\"\"\n    return _ma(x, window_size, np.ones(window_size) / window_size)\n\n\ndef weighted_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef ema_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = np.asarray(x, float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Detrended weighted smoothing.\n    1. Compute a simple moving\u2011average trend.\n    2. Subtract the trend (extended to original length).\n    3. Apply a weighted moving average to the detrended signal.\n    \"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    # extend trend to original length by repeating edge values\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = np.asarray(x, float) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef gaussian_ma(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Moving average with a Gaussian kernel.\"\"\"\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    sigma = max(window_size / 6.0, 0.1)\n    centre = (window_size - 1) / 2.0\n    i = np.arange(window_size, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef median_ma(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Median filter \u2013 robust to spikes/outliers.\"\"\"\n    x = np.asarray(x, float)\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    out_len = x.size - window_size + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(x[i : i + window_size])\n    return out\n\n\ndef double_enhanced(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Two\u2011stage enhancement:\n    1. Detrended weighted smoothing.\n    2. Light uniform smoothing with a smaller window (\u2248 half size).\n    \"\"\"\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    half = max(1, window_size // 2)\n    return adaptive_filter(first, half)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Mean\u2011squared residual between original and filtered signals.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return np.inf\n    return float(np.var(original[:m] - filtered[:m]))\n\n\ndef advanced_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Multi\u2011stage enhanced filter:\n    1. Detrended weighted smoothing.\n    2. Median filter to suppress spikes/outliers.\n    3. Light uniform smoothing with a smaller window (\u2248 half size).\n    \"\"\"\n    # stage 1 \u2013 detrended weighted\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n\n    # stage 2 \u2013 median to remove occasional spikes\n    medianed = median_ma(first, window_size)\n    if medianed.size == 0:\n        return np.empty(0, float)\n\n    # stage 3 \u2013 final light smoothing\n    second_w = max(1, window_size // 2)\n    return adaptive_filter(medianed, second_w)\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"Pick the filter with the lowest residual variance, now also considering ``advanced_filter``.\"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"gaussian\": gaussian_ma(x, window_size),\n        \"median\": median_ma(x, window_size),\n        \"double\": double_enhanced(x, window_size),\n        \"advanced\": advanced_filter(x, window_size),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(input_signal, window_size: int = 20, algorithm_type: str = \"enhanced\") -> np.ndarray:\n    \"\"\"\n    Dispatch to a filtering algorithm.\n    Supported ``algorithm_type`` values:\n        - \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\"\n        - \"gaussian\"\n        - \"median\"\n        - \"double\"\n        - \"advanced\"\n        - any other value \u2192 automatic selection of the best filter.\n    \"\"\"\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter_with_trend_preservation,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"double\": double_enhanced,\n        \"advanced\": advanced_filter,\n    }\n    if algorithm_type in dispatch:\n        return dispatch[algorithm_type](input_signal, window_size)\n    # Fallback \u2013 choose the most effective filter automatically\n    return _select_best_filter(np.asarray(input_signal, float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\ndef _ma(x,w,wt):\n    x=np.asarray(x,float)\n    if w<=0:raise ValueError(\"window must be >0\")\n    if x.size<w:return np.empty(0,float)\n    return np.convolve(x,wt,mode='valid')\ndef simple_ma(x,w):return _ma(x,w,np.ones(w)/w)\ndef weighted_ma(x,w):\n    wt=np.exp(np.linspace(-2,0,w))\n    wt/=wt.sum()\n    return _ma(x,w,wt)\ndef ema(x,w):\n    x=np.asarray(x,float)\n    if w<=0:raise ValueError(\"window must be >0\")\n    if x.size<w:return np.empty(0,float)\n    a=2.0/(w+1)\n    out=np.empty_like(x)\n    out[0]=x[0]\n    for i in range(1,len(x)):out[i]=a*x[i]+(1-a)*out[i-1]\n    return out[w-1:]\ndef detrended_weighted(x,w):\n    tr=simple_ma(x,w)\n    if tr.size==0:return np.empty(0,float)\n    full=np.concatenate((np.full(w-1,tr[0]),tr))\n    return weighted_ma(x-full,w)\ndef _rv(o,f):\n    m=min(len(o),len(f))\n    return np.inf if m==0 else float(np.var(o[:m]-f[:m]))\ndef _best(x,w):\n    cand={'b':simple_ma(x,w),'w':weighted_ma(x,w),'e':ema(x,w),'d':detrended_weighted(x,w)}\n    return min(cand.values(),key=lambda y:_rv(x,y))\ndef process_signal(sig,w=20,algo='enhanced'):\n    funcs={'basic':simple_ma,'weighted':weighted_ma,'ema':ema,'enhanced':detrended_weighted}\n    return funcs[algo](sig,w) if algo in funcs else _best(sig,w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\ndef _validate_input(items, capacity):\n    if not isinstance(capacity, int) or capacity < 0:\n        raise ValueError(\"capacity must be a non\u2011negative integer\")\n    try:\n        items_list = list(items)\n    except TypeError:\n        raise ValueError(\"items must be an iterable of (weight, value) pairs\")\n    for idx, it in enumerate(items_list):\n        if not (isinstance(it, (list, tuple)) and len(it) == 2):\n            raise ValueError(f\"item at position {idx} is not a (weight, value) pair\")\n        w, v = it\n        if not (isinstance(w, (int, float)) and isinstance(v, (int, float))):\n            raise ValueError(f\"weight and value must be numbers (item {idx})\")\n        if w < 0:\n            raise ValueError(f\"weight cannot be negative (item {idx})\")\n    return items_list\n\n\ndef _greedy_knapsack(items, capacity):\n    # Items: list of (weight, value, original_index)\n    zero_weight = [i for i in items if i[0] == 0 and i[1] > 0]\n    non_zero = [i for i in items if i[0] > 0]\n    # sort by value/weight descending\n    non_zero.sort(key=lambda x: x[1] / x[0], reverse=True)\n    selected = [i[2] for i in zero_weight]\n    total_weight = 0\n    for w, v, idx in non_zero:\n        if total_weight + w <= capacity:\n            total_weight += w\n            selected.append(idx)\n    selected.sort()\n    return selected\n\n\ndef _dp_knapsack(items, capacity):\n    # items: list of (weight, value, original_index)\n    n = len(items)\n    dp = [0] * (capacity + 1)\n    prev = [None] * (capacity + 1)  # (prev_weight, item_index)\n\n    for i, (w, v, idx) in enumerate(items):\n        if w > capacity:\n            continue\n        for cap in range(capacity, w - 1, -1):\n            new_val = dp[cap - w] + v\n            if new_val > dp[cap]:\n                dp[cap] = new_val\n                prev[cap] = (cap - w, idx)\n\n    # backtrack to recover indices\n    cap = capacity\n    selected = []\n    while cap > 0 and prev[cap] is not None:\n        prev_cap, idx = prev[cap]\n        selected.append(idx)\n        cap = prev_cap\n    selected.sort()\n    return selected\n\n\ndef construct_packing(items, capacity):\n    \"\"\"\n    Return a list of original item indices that maximise total value\n    without exceeding ``capacity``.\n    \"\"\"\n    # Input validation\n    items_list = _validate_input(items, capacity)\n\n    if capacity == 0 or not items_list:\n        return []\n\n    # Attach original indices\n    indexed_items = [(float(w), float(v), i) for i, (w, v) in enumerate(items_list)]\n\n    # Choose algorithm\n    if len(indexed_items) <= 30 or capacity <= 10000:\n        return _dp_knapsack(indexed_items, capacity)\n    else:\n        return _greedy_knapsack(indexed_items, capacity)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Alternative runs_successfully approach, NumPy-based implementation\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _ma(x, w, wt):\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef simple_ma(x, w):\n    \"\"\"Uniform moving average.\"\"\"\n    return _ma(x, w, np.ones(w) / w)\n\n\ndef weighted_ma(x, w):\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return _ma(x, w, wt)\n\n\ndef ema(x, w):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    a = 2.0 / (w + 1)\n    out = np.empty_like(x)\n    out[0] = x[0]\n    for i in range(1, len(x)):\n        out[i] = a * x[i] + (1 - a) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef detrended_weighted(x, w):\n    \"\"\"Detrend with simple MA then apply weighted MA.\"\"\"\n    tr = simple_ma(x, w)\n    if tr.size == 0:\n        return np.empty(0, float)\n    full = np.concatenate((np.full(w - 1, tr[0]), tr))\n    return weighted_ma(x - full, w)\n\n\ndef gaussian_ma(x, w):\n    \"\"\"Moving average with Gaussian kernel.\"\"\"\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    sigma = max(w / 6.0, 0.1)\n    centre = (w - 1) / 2.0\n    i = np.arange(w, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, w, wt)\n\n\ndef median_ma(x, w):\n    \"\"\"Median filter \u2013 robust to spikes.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    out_len = x.size - w + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(x[i : i + w])\n    return out\n\n\ndef double_enhanced(x, w):\n    \"\"\"Two\u2011stage: detrended weighted then light uniform smoothing.\"\"\"\n    first = detrended_weighted(x, w)\n    if first.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(first, half)\n\n\ndef _rv(orig, filt):\n    \"\"\"Residual variance (lower is better).\"\"\"\n    m = min(len(orig), len(filt))\n    return np.inf if m == 0 else float(np.var(orig[:m] - filt[:m]))\n\n\ndef _best(x, w):\n    \"\"\"Select filter with smallest residual variance.\"\"\"\n    candidates = {\n        \"basic\": simple_ma(x, w),\n        \"weighted\": weighted_ma(x, w),\n        \"ema\": ema(x, w),\n        \"enhanced\": detrended_weighted(x, w),\n        \"gaussian\": gaussian_ma(x, w),\n        \"median\": median_ma(x, w),\n        \"double\": double_enhanced(x, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, float)\n    best_key = min(candidates, key=lambda k: _rv(x, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, algo=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm, falling back to automatic selection.\"\"\"\n    dispatch = {\n        \"basic\": simple_ma,\n        \"weighted\": weighted_ma,\n        \"ema\": ema,\n        \"enhanced\": detrended_weighted,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"double\": double_enhanced,\n    }\n    if algo in dispatch:\n        return dispatch[algo](sig, w)\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\n\ndef _ma(x, w, wt):\n    \"\"\"Convolution helper for moving\u2011average\u2011style filters.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef simple_ma(x, w):\n    \"\"\"Uniform moving average.\"\"\"\n    return _ma(x, w, np.ones(w) / w)\n\n\ndef weighted_ma(x, w):\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return _ma(x, w, wt)\n\n\ndef ema(x, w):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    a = 2.0 / (w + 1)\n    out = np.empty_like(x)\n    out[0] = x[0]\n    for i in range(1, len(x)):\n        out[i] = a * x[i] + (1 - a) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef detrended_weighted(x, w):\n    \"\"\"Detrend with simple MA then apply weighted MA.\"\"\"\n    tr = simple_ma(x, w)\n    if tr.size == 0:\n        return np.empty(0, float)\n    full = np.concatenate((np.full(w - 1, tr[0]), tr))\n    return weighted_ma(x - full, w)\n\n\ndef gaussian_ma(x, w):\n    \"\"\"Moving average with Gaussian kernel.\"\"\"\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    sigma = max(w / 6.0, 0.1)\n    centre = (w - 1) / 2.0\n    i = np.arange(w, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, w, wt)\n\n\ndef median_ma(x, w):\n    \"\"\"Median filter \u2013 robust to spikes.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be >0\")\n    if x.size < w:\n        return np.empty(0, float)\n    out_len = x.size - w + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(x[i : i + w])\n    return out\n\n\ndef double_enhanced(x, w):\n    \"\"\"Two\u2011stage: detrended weighted then light uniform smoothing.\"\"\"\n    first = detrended_weighted(x, w)\n    if first.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(first, half)\n\n\ndef advanced_filter(x, w):\n    \"\"\"\n    Multi\u2011stage enhanced filter:\n    1. Detrended weighted smoothing.\n    2. Median filter to suppress spikes.\n    3. Light uniform smoothing with a smaller window.\n    \"\"\"\n    stage1 = detrended_weighted(x, w)\n    if stage1.size == 0:\n        return np.empty(0, float)\n    stage2 = median_ma(stage1, w)\n    if stage2.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(stage2, half)\n\n\ndef _residual_variance(orig: np.ndarray, filt: np.ndarray) -> float:\n    \"\"\"Mean\u2011squared residual between original and filtered signals.\"\"\"\n    m = min(len(orig), len(filt))\n    return np.inf if m == 0 else float(np.var(orig[:m] - filt[:m]))\n\n\ndef _select_best(x, w):\n    \"\"\"Pick the filter with the lowest residual variance (ties \u2192 longest output).\"\"\"\n    candidates = {\n        \"basic\": simple_ma(x, w),\n        \"weighted\": weighted_ma(x, w),\n        \"ema\": ema(x, w),\n        \"enhanced\": detrended_weighted(x, w),\n        \"gaussian\": gaussian_ma(x, w),\n        \"median\": median_ma(x, w),\n        \"double\": double_enhanced(x, w),\n        \"advanced\": advanced_filter(x, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, float)\n\n    # Find minimal residual variance\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    # Resolve ties by choosing the longest output\n    best_score = _residual_variance(x, candidates[best_key])\n    tied = [k for k, v in candidates.items()\n            if _residual_variance(x, v) == best_score]\n    if len(tied) > 1:\n        best_key = max(tied, key=lambda k: len(candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, algo=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm, falling back to automatic selection.\"\"\"\n    dispatch = {\n        \"basic\": simple_ma,\n        \"weighted\": weighted_ma,\n        \"ema\": ema,\n        \"enhanced\": detrended_weighted,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"double\": double_enhanced,\n        \"advanced\": advanced_filter,\n    }\n    if algo in dispatch:\n        return dispatch[algo](sig, w)\n    return _select_best(sig, w)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}