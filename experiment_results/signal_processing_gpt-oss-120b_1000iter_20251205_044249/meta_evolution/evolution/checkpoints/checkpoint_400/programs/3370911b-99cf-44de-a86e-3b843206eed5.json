{"id": "3370911b-99cf-44de-a86e-3b843206eed5", "code": "# EVOLVE-BLOCK-START\ndef _validate_signal(x, window_size, *, require_odd=False):\n    \"\"\"\n    Convert *x* to a 1\u2011D float array and validate *window_size*.\n\n    If ``require_odd`` is True and the supplied size is even, it is\n    incremented to the next odd number (a common requirement for median\u2011type\n    filters).  Empty inputs raise a ValueError.\n    \"\"\"\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.ndim != 1:\n        raise ValueError(\"Input signal must be one\u2011dimensional\")\n    if require_odd and window_size % 2 == 0:\n        window_size += 1\n    if arr.size < window_size:\n        raise ValueError(f\"Input length ({arr.size}) < window_size ({window_size})\")\n    if arr.size == 0:\n        raise ValueError(\"Input signal is empty\")\n    return arr, window_size\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (uniform weights).\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    if w == 1:\n        return arr.copy()\n    kernel = np.full(w, 1.0 / w, dtype=float)\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(size):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, size))\n    return w / w.sum()\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average that emphasises recent samples.\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    if w == 1:\n        return arr.copy()\n    # reverse so newest sample gets the largest weight\n    return np.convolve(arr, _exp_weights(w)[::-1], mode=\"valid\")\n\n\ndef median_filter(x, window_size=20):\n    \"\"\"Median filter (odd window) using stride tricks for speed.\"\"\"\n    arr, w = _validate_signal(x, window_size, require_odd=True)\n    if w == 1:\n        return arr.copy()\n    shape = (arr.size - w + 1, w)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\n@lru_cache(maxsize=None)\ndef _gaussian_kernel(size):\n    \"\"\"Cached Gaussian kernel (odd size, sum\u202f=\u202f1).\"\"\"\n    if size % 2 == 0:\n        size += 1\n    x = np.arange(size) - (size - 1) / 2.0\n    sigma = size / 6.0\n    g = np.exp(-0.5 * (x / sigma) ** 2)\n    return g / g.sum()\n\n\ndef gaussian_filter(x, window_size=20):\n    \"\"\"Gaussian smoothing filter.\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    return np.convolve(arr, _gaussian_kernel(w), mode=\"valid\")\n\n\n@lru_cache(maxsize=None)\ndef _savgol_coeffs(window_size, polyorder):\n    \"\"\"Cached Savitzky\u2011Golay convolution coefficients.\"\"\"\n    if polyorder >= window_size:\n        raise ValueError(\"polyorder must be less than window_size\")\n    half = window_size // 2\n    A = np.vander(np.arange(-half, half + 1), N=polyorder + 1, increasing=True)\n    return np.linalg.pinv(A)[0]\n\n\ndef savgol_filter(x, window_size=20, polyorder=2):\n    \"\"\"Savitzky\u2011Golay filter (polynomial smoothing).\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    coeffs = _savgol_coeffs(w, polyorder)\n    return np.convolve(arr, coeffs, mode=\"valid\")\n\n\ndef trimmed_mean_filter(x, window_size=20, trim_ratio=0.1):\n    \"\"\"Trimmed\u2011mean filter \u2013 discards a fraction of extreme values per window.\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    k = int(trim_ratio * w)\n    if k * 2 >= w:  # nothing left after trimming\n        raise ValueError(\"trim_ratio too large for given window_size\")\n    shape = (arr.size - w + 1, w)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    sorted_win = np.sort(windows, axis=1)\n    trimmed = sorted_win[:, k : w - k]\n    return trimmed.mean(axis=1)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\", **kwargs):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` (case\u2011insensitive):\n        enhanced, adaptive, median, gaussian, savgol, trimmed, hybrid\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    if algo == \"gaussian\":\n        return gaussian_filter(input_signal, window_size)\n    if algo == \"savgol\":\n        return savgol_filter(input_signal, window_size, polyorder=kwargs.get(\"polyorder\", 2))\n    if algo == \"trimmed\":\n        return trimmed_mean_filter(input_signal, window_size, trim_ratio=kwargs.get(\"trim_ratio\", 0.1))\n    if algo == \"hybrid\":\n        # adaptive pass followed by exponential\u2011weighted smoothing\n        interim = adaptive_filter(input_signal, window_size)\n        return enhanced_filter_with_trend_preservation(interim, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "97206d13-6f9e-401d-8824-bc8f6f33b92f", "generation": 6, "timestamp": 1764938011.587638, "iteration_found": 190, "metrics": {"runs_successfully": 0.0, "error": "Stage 1 failed: name 'lru_cache' is not defined"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.0003962516784667969, "success_rate": 1.0}, "island": 0}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are a code\u2011evolution assistant tasked with improving the `construct_packing()` function (and any helper functions it calls) that lives inside the **evolve block** of the program.  \n\n**Your output must contain ONLY the Python code that belongs between the markers**  \n\n```\n# EVOLVE-BLOCK-START\n...your revised code here...\n# EVOLVE-BLOCK-END\n```  \n\nDo **not** include the marker lines themselves, any import statements, the surrounding `run_packing()` driver, or any other code outside the evolve block. The system will automatically splice your contribution into the existing file.\n\n### What you must do\n1. **Preserve the existing public API** \u2013 the function signatures of `construct_packing()` and any helper it calls must remain the same, so the rest of the program can invoke them unchanged.  \n2. **Correct logical errors** \u2013 ensure that the packing algorithm correctly:\n   - respects the capacity constraints of each container,\n   - places each item exactly once,\n   - returns a data structure matching the expected format (e.g., a list of containers, each containing a list of item IDs and the remaining free space).\n3. **Improve efficiency** \u2013 replace na\u00efve nested loops with a single pass where possible, use dictionary look\u2011ups for item lookup, and avoid unnecessary list copying.\n4. **Add clear inline comments** \u2013 each non\u2011trivial step should have a concise comment explaining *why* the code does what it does, not just *what* it does.\n5. **Handle edge cases** \u2013 gracefully handle empty input, items that individually exceed any container\u2019s capacity, and duplicate item IDs.\n\n### Strategies you may try (choose any that fit the current code)\n- **Greedy first\u2011fit decreasing**: sort items by size descending, then place each item into the first container that can accommodate it.\n- **Bin\u2011packing with a min\u2011heap**: keep a heap of containers keyed by remaining space to find the best\u2011fit container in O(log\u202fn) time.\n- **Pre\u2011validation pass**: before packing, scan items to reject or flag any that are larger than the largest container, returning a clear error structure.\n- **In\u2011place mutation**: work directly on the passed\u2011in containers list to avoid copying large structures; return the mutated list for consistency.\n- **Early exit**: if all items are placed before iterating through every container, break out of loops to save time.\n\n### What you must **not** do (these caused previous failures)\n- Produce code that exceeds the evolve block (e.g., adding imports, redefining `run_packing`, or writing top\u2011level statements).  \n- Remove or rename existing function definitions, which breaks the external interface.  \n- Write overly aggressive code that assumes unlimited memory or time (e.g., exhaustive search over all permutations).  \n- Omit required return values or change their type/structure.  \n- Generate code that does not compile or contains syntax errors.  \n\n### Output format reminder\n- **Only** output the revised Python code that belongs between `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.  \n- Do **not** include the marker lines themselves, any surrounding code, or any explanatory text outside the code block.  \n\nProceed to rewrite `construct_packing()` (and any helpers it uses) according to the guidelines above.", "user": "# Current Program Information\n- Fitness: 13.7277\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 13.7277 \u2192 13.7277. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4200, output_length: 91.0000, overall_score: 0.3899, slope_changes: 66.0000, lag_error: 0.8697, avg_error: 1.2418, false_reversals: 55.2000, correlation: 0.3768, noise_reduction: 0.0000, smoothness_score: 0.2326, responsiveness_score: 0.5349, accuracy_score: 0.3768, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4200, output_length: 91.0000, overall_score: 0.3899, slope_changes: 66.0000, lag_error: 0.8697, avg_error: 1.2418, false_reversals: 55.2000, correlation: 0.3768, noise_reduction: 0.0000, smoothness_score: 0.2326, responsiveness_score: 0.5349, accuracy_score: 0.3768, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4200, output_length: 91.0000, overall_score: 0.3899, slope_changes: 66.0000, lag_error: 0.8697, avg_error: 1.2418, false_reversals: 55.2000, correlation: 0.3768, noise_reduction: 0.0000, smoothness_score: 0.2326, responsiveness_score: 0.5349, accuracy_score: 0.3768, efficiency_score: 1.0000, execution_time: 0.0018, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 13.7278)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nReal-Time Adaptive Signal Processing Algorithm for Non-Stationary Time Series\n\nThis algorithm implements a sliding window approach to filter volatile, non-stationary\ntime series data while minimizing noise and preserving signal dynamics.\n\"\"\"\nimport numpy as np\nfrom scipy import signal\nfrom collections import deque\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Adaptive signal processing algorithm using sliding window approach.\n\n    Args:\n        x: Input signal (1D array of real-valued samples)\n        window_size: Size of the sliding window (W samples)\n\n    Returns:\n        y: Filtered output signal with length = len(x) - window_size + 1\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n\n    # Initialize output array\n    output_length = len(x) - window_size + 1\n    y = np.zeros(output_length)\n\n    # Simple moving average as baseline\n    for i in range(output_length):\n        window = x[i : i + window_size]\n\n        # Basic moving average filter\n        y[i] = np.mean(window)\n\n    return y\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Enhanced version with trend preservation using weighted moving average.\n\n    Args:\n        x: Input signal (1D array of real-valued samples)\n        window_size: Size of the sliding window\n\n    Returns:\n        y: Filtered output signal\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input signal length ({len(x)}) must be >= window_size ({window_size})\")\n\n    output_length = len(x) - window_size + 1\n    y = np.zeros(output_length)\n\n    # Create weights that emphasize recent samples\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights = weights / np.sum(weights)\n\n    for i in range(output_length):\n        window = x[i : i + window_size]\n\n        # Weighted moving average with exponential weights\n        y[i] = np.sum(window * weights)\n\n    return y\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Main signal processing function that applies the selected algorithm.\n\n    Args:\n        input_signal: Input time series data\n        window_size: Window size for processing\n        algorithm_type: Type of algorithm to use (\"basic\" or \"enhanced\")\n\n    Returns:\n        Filtered signal\n    \"\"\"\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    else:\n        return adaptive_filter(input_signal, window_size)\n\n\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4200), Performs well on output_length (91.0000), Performs well on overall_score (0.3899), Performs well on slope_changes (66.0000), Performs well on lag_error (0.8697), Performs well on avg_error (1.2418), Performs well on false_reversals (55.2000), Performs well on correlation (0.3768), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2326), Performs well on responsiveness_score (0.5349), Performs well on accuracy_score (0.3768), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0018), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_input(x, window_size):\n    \"\"\"\n    Validate and normalise the input signal and window size.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        1\u2011D sequence of numeric values.\n    window_size : int or castable to int\n        Desired length of the sliding window; must be positive.\n\n    Returns\n    -------\n    np.ndarray\n        A 1\u2011D float array of the input data.\n\n    Raises\n    ------\n    ValueError\n        If ``x`` is not one\u2011dimensional, if ``window_size`` is not a positive\n        integer, or if the signal is shorter than the window.\n    \"\"\"\n    # coerce window size to int and check positivity\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        raise ValueError(f\"len({a.size}) < window_size ({w})\")\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"\n    Cached exponential weighting vector for a given window length.\n    The vector is right\u2011biased (larger weight for newer samples) and sums to 1.\n    \"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving\u2011average filter (uniform weights).\n\n    Returns a ``valid`` convolution, i.e. length ``len(x) - window_size + 1``.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasises recent samples.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias in convolution\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter using a stride\u2011trick view for efficiency.\n    An odd window size is enforced (incremented if even).\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w % 2 == 0:\n        w += 1  # enforce odd size\n    if w == 1:\n        return a.copy()\n    # Create a sliding window view without copies\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"\n    Hybrid filter: first a simple moving average, then the exponential\u2011weighted filter.\n    \"\"\"\n    first_pass = adaptive_filter(x, window_size)\n    # The second stage works on the already\u2011filtered signal; reuse the same window size\n    return enhanced_filter_with_trend_preservation(first_pass, window_size)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"median\"   : median filter\n        - \"hybrid\"   : adaptive followed by enhanced\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    window_size : int, optional\n        Length of the sliding window (default 20).\n    algorithm_type : str, optional\n        Identifier of the algorithm to use.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported ``algorithm_type`` is supplied.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4200), Performs well on output_length (91.0000), Performs well on overall_score (0.3899), Performs well on slope_changes (66.0000), Performs well on lag_error (0.8697), Performs well on avg_error (1.2418), Performs well on false_reversals (55.2000), Performs well on correlation (0.3768), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2326), Performs well on responsiveness_score (0.5349), Performs well on accuracy_score (0.3768), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n\n    Args:\n        x: 1\u2011D array\u2011like input signal.\n        window_size: Size of the sliding window.\n\n    Returns:\n        Filtered signal (valid convolution, length = len(x) - window_size + 1).\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input length ({len(x)}) < window_size ({window_size})\")\n    kernel = np.full(window_size, 1.0 / window_size, dtype=float)\n    return np.convolve(x, kernel, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasizes recent samples.\n\n    The weight vector is reversed before convolution because `np.convolve`\n    flips the second argument.\n\n    Args:\n        x: 1\u2011D array\u2011like input signal.\n        window_size: Size of the sliding window.\n\n    Returns:\n        Filtered signal (valid convolution).\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input length ({len(x)}) < window_size ({window_size})\")\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(x, w[::-1], mode=\"valid\")\n\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter that reduces impulsive noise while preserving edges.\n\n    Args:\n        x: 1\u2011D array\u2011like input signal.\n        window_size: Size of the sliding window (must be odd for a true median).\n\n    Returns:\n        Filtered signal (valid window, length = len(x) - window_size + 1).\n    \"\"\"\n    if len(x) < window_size:\n        raise ValueError(f\"Input length ({len(x)}) < window_size ({window_size})\")\n    # Ensure integer window size\n    w = int(window_size)\n    # Compute median for each sliding window\n    med = [np.median(x[i : i + w]) for i in range(len(x) - w + 1)]\n    return np.asarray(med, dtype=float)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filter.\n\n    Supported algorithm_type values:\n        - \"enhanced\": exponential\u2011weighted moving average (default)\n        - \"adaptive\": simple moving average\n        - \"median\"  : median filter\n\n    Args:\n        input_signal: 1\u2011D array\u2011like signal to be filtered.\n        window_size: Size of the sliding window.\n        algorithm_type: Identifier of the filtering algorithm.\n\n    Returns:\n        Filtered signal as a NumPy array.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4200), Performs well on output_length (91.0000), Performs well on overall_score (0.3899), Performs well on slope_changes (66.0000), Performs well on lag_error (0.8697), Performs well on avg_error (1.2418), Performs well on false_reversals (55.2000), Performs well on correlation (0.3768), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2326), Performs well on responsiveness_score (0.5349), Performs well on accuracy_score (0.3768), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\n\"\"\"\nCompact adaptive signal processing.\n\"\"\"\nimport numpy as np\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Filter `sig` with either an exponential\u2011weighted (enhanced) or simple\n    moving\u2011average (adaptive) filter.\n\n    Returns a NumPy array of length len(sig) - window_size + 1.\n    \"\"\"\n    a = np.asarray(sig, dtype=float).ravel()\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if a.size < window_size:\n        raise ValueError(f\"Input length ({a.size}) < window_size ({window_size})\")\n    if algorithm_type.lower() == \"enhanced\":\n        w = np.exp(np.linspace(-2, 0, window_size))\n        w /= w.sum()\n        return np.convolve(a, w[::-1], mode=\"valid\")\n    # adaptive (simple moving average)\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 13.7277)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Filter `sig` using one of three algorithms:\n    - \"enhanced\": exponential\u2011weighted moving average\n    - \"adaptive\": simple moving average\n    - \"median\": median filter (odd window)\n    \"\"\"\n    a = np.asarray(sig, float)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        raise ValueError(\"signal shorter than window size\")\n    alg = alg.lower()\n    if alg.startswith(\"a\"):  # adaptive\n        return np.convolve(a, np.ones(w) / w, \"valid\")\n    if alg.startswith(\"m\"):  # median\n        if w % 2 == 0:\n            w += 1\n        s = a.strides[0]\n        shape = (a.size - w + 1, w)\n        win = np.lib.stride_tricks.as_strided(a, shape, (s, s))\n        return np.median(win, axis=1)\n    # enhanced (default)\n    wv = np.exp(np.linspace(-2, 0, w))\n    wv /= wv.sum()\n    return np.convolve(a, wv[::-1], \"valid\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_input(x, window_size):\n    \"\"\"\n    Validate and normalise the input signal and window size.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        1\u2011D sequence of numeric values.\n    window_size : int or castable to int\n        Desired length of the sliding window; must be positive.\n\n    Returns\n    -------\n    np.ndarray\n        A 1\u2011D float array of the input data.\n\n    Raises\n    ------\n    ValueError\n        If ``x`` is not one\u2011dimensional, if ``window_size`` is not a positive\n        integer, or if the signal is shorter than the window.\n    \"\"\"\n    # coerce window size to int and check positivity\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    a = np.asarray(x, dtype=float)\n    if a.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if a.size < w:\n        raise ValueError(f\"len({a.size}) < window_size ({w})\")\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"\n    Cached exponential weighting vector for a given window length.\n    The vector is right\u2011biased (larger weight for newer samples) and sums to 1.\n    \"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving\u2011average filter (uniform weights).\n\n    Returns a ``valid`` convolution, i.e. length ``len(x) - window_size + 1``.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasises recent samples.\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias in convolution\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef median_filter(x, window_size=20):\n    \"\"\"\n    Median filter using a stride\u2011trick view for efficiency.\n    An odd window size is enforced (incremented if even).\n    \"\"\"\n    a, w = _validate_input(x, window_size)\n    if w % 2 == 0:\n        w += 1  # enforce odd size\n    if w == 1:\n        return a.copy()\n    # Create a sliding window view without copies\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"\n    Hybrid filter: first a simple moving average, then the exponential\u2011weighted filter.\n    \"\"\"\n    first_pass = adaptive_filter(x, window_size)\n    # The second stage works on the already\u2011filtered signal; reuse the same window size\n    return enhanced_filter_with_trend_preservation(first_pass, window_size)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"median\"   : median filter\n        - \"hybrid\"   : adaptive followed by enhanced\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    window_size : int, optional\n        Length of the sliding window (default 20).\n    algorithm_type : str, optional\n        Identifier of the algorithm to use.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported ``algorithm_type`` is supplied.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\n# Cache for exponential weight vectors to avoid recomputation\n_weights_cache = {}\n\ndef _validate_window(x, window_size):\n    \"\"\"Validate input array and window size, returning a float ndarray.\"\"\"\n    if window_size <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n    arr = np.asarray(x, dtype=float)\n    if arr.size < window_size:\n        raise ValueError(f\"Input length ({arr.size}) < window_size ({window_size})\")\n    return arr\n\ndef _exp_weights(window_size):\n    \"\"\"Return a right\u2011biased exponential weight vector that sums to 1.\"\"\"\n    # Use cache for speed on repeated calls with the same size\n    if window_size in _weights_cache:\n        return _weights_cache[window_size]\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    _weights_cache[window_size] = w\n    return w\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter using a uniform kernel.\"\"\"\n    arr = _validate_window(x, window_size)\n    kernel = np.ones(window_size, dtype=float) / window_size\n    return np.convolve(arr, kernel, mode=\"valid\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average that emphasizes recent samples.\"\"\"\n    arr = _validate_window(x, window_size)\n    w = _exp_weights(window_size)\n    # Reverse weights so that the most recent sample gets the largest weight\n    return np.convolve(arr, w[::-1], mode=\"valid\")\n\ndef median_filter(x, window_size=21):\n    \"\"\"Median filter (odd window size) using stride tricks for speed.\"\"\"\n    if window_size % 2 == 0:\n        window_size += 1  # force odd size\n    arr = _validate_window(x, window_size)\n    shape = (arr.size - window_size + 1, window_size)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"adaptive\" : simple moving average\n        - \"median\"   : median filter\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n# ----------------------------------------------------------------------\n# Helper utilities\n# ----------------------------------------------------------------------\ndef _validate_signal(x, w, *, require_odd=False):\n    \"\"\"\n    Convert input to a 1\u2011D float array and validate window size.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        Input signal.\n    w : int\n        Desired window length (must be positive).\n    require_odd : bool, optional\n        If True, enforce an odd window size (increment by one if even).\n\n    Returns\n    -------\n    tuple[np.ndarray, int]\n        Normalised signal and possibly adjusted window size.\n\n    Raises\n    ------\n    ValueError\n        If validation fails.\n    \"\"\"\n    try:\n        w = int(w)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {w!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if require_odd and w % 2 == 0:\n        w += 1\n    if arr.size < w:\n        raise ValueError(f\"len({arr.size}) < window_size ({w})\")\n    return arr, w\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\n\n# ----------------------------------------------------------------------\n# Core filters\n# ----------------------------------------------------------------------\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (uniform weights).\"\"\"\n    a, w = _validate_signal(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"\n    Exponential\u2011weighted moving average that emphasizes recent samples.\n    The newest sample receives the largest weight.\n    \"\"\"\n    a, w = _validate_signal(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = _exp_weights(w)[::-1]          # reverse for right\u2011bias\n    return np.convolve(a, kernel, mode=\"valid\")\n\n\ndef median_filter(x, window_size=21):\n    \"\"\"Median filter with an odd window size using stride tricks.\"\"\"\n    a, w = _validate_signal(x, window_size, require_odd=True)\n    if w == 1:\n        return a.copy()\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"\n    Two\u2011stage filter: adaptive (uniform) followed by enhanced (exponential).\n    Often yields stronger noise reduction while preserving trends.\n    \"\"\"\n    first = adaptive_filter(x, window_size)\n    return enhanced_filter(first, window_size)\n\n\ndef _auto_select(x, window_size):\n    \"\"\"\n    Automatic selection based on signal variance.\n    High variance \u2192 median (robust to outliers); otherwise \u2192 enhanced.\n    \"\"\"\n    a, _ = _validate_signal(x, window_size)\n    if np.var(a) > 0.5:\n        return median_filter(a, window_size)\n    return enhanced_filter(a, window_size)\n\n\n# ----------------------------------------------------------------------\n# Public dispatcher\n# ----------------------------------------------------------------------\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\"\n        - \"enhanced\"\n        - \"median\"\n        - \"hybrid\"\n        - \"auto\"   (variance\u2011based automatic selection)\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n    \"\"\"\n    name = algorithm_type.lower()\n    if name == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if name == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    if name == \"median\":\n        return median_filter(sig, window_size)\n    if name == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    if name == \"auto\":\n        return _auto_select(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n\ndef _validate_signal(x, window_size, *, require_odd=False):\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"Input signal must be one\u2011dimensional\")\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if require_odd and window_size % 2 == 0:\n        raise ValueError(\"median_filter requires an odd window_size\")\n    if arr.size < window_size:\n        raise ValueError(f\"Input length ({arr.size}) < window_size ({window_size})\")\n    return arr\n\n\ndef adaptive_filter(x, window_size=20):\n    arr = _validate_signal(x, window_size)\n    kernel = np.full(window_size, 1.0 / window_size, dtype=float)\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    arr = _validate_signal(x, window_size)\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(arr, w[::-1], mode=\"valid\")\n\n\ndef median_filter(x, window_size=20):\n    arr = _validate_signal(x, window_size, require_odd=True)\n    shape = (arr.size - window_size + 1, window_size)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\n@lru_cache(maxsize=None)\ndef _gaussian_kernel(size):\n    if size <= 0:\n        raise ValueError(\"Kernel size must be positive\")\n    if size % 2 == 0:\n        size += 1\n    x = np.arange(size) - (size - 1) / 2.0\n    sigma = size / 6.0\n    g = np.exp(-0.5 * (x / sigma) ** 2)\n    return g / g.sum()\n\n\ndef gaussian_filter(x, window_size=20):\n    arr = _validate_signal(x, window_size)\n    kernel = _gaussian_kernel(window_size)\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\n@lru_cache(maxsize=None)\ndef _savgol_coeffs(window_size, polyorder):\n    if polyorder >= window_size:\n        raise ValueError(\"polyorder must be less than window_size\")\n    half = window_size // 2\n    # design matrix\n    A = np.vander(np.arange(-half, half + 1), N=polyorder + 1, increasing=True)\n    # pseudo\u2011inverse for smoothing (first row gives convolution coeffs)\n    coeffs = np.linalg.pinv(A)[0]\n    return coeffs\n\n\ndef savgol_filter(x, window_size=20, polyorder=2):\n    arr = _validate_signal(x, window_size)\n    coeffs = _savgol_coeffs(window_size, polyorder)\n    return np.convolve(arr, coeffs, mode=\"valid\")\n\n\ndef trimmed_mean_filter(x, window_size=20, trim_ratio=0.1):\n    arr = _validate_signal(x, window_size)\n    k = int(trim_ratio * window_size)\n    shape = (arr.size - window_size + 1, window_size)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    sorted_win = np.sort(windows, axis=1)\n    trimmed = sorted_win[:, k : window_size - k]\n    return trimmed.mean(axis=1)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\", **kwargs):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` (case\u2011insensitive):\n        enhanced, adaptive, median, gaussian, savgol, trimmed, hybrid\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    if algo == \"gaussian\":\n        return gaussian_filter(input_signal, window_size)\n    if algo == \"savgol\":\n        poly = kwargs.get(\"polyorder\", 2)\n        return savgol_filter(input_signal, window_size, polyorder=poly)\n    if algo == \"trimmed\":\n        ratio = kwargs.get(\"trim_ratio\", 0.1)\n        return trimmed_mean_filter(input_signal, window_size, trim_ratio=ratio)\n    if algo == \"hybrid\":\n        interim = adaptive_filter(input_signal, window_size)\n        return enhanced_filter_with_trend_preservation(interim, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["def _validate_signal(x, window_size, *, require_odd=False):\n    \"\"\"\n    Convert *x* to a 1\u2011D float array and validate *window_size*.\n\n    If ``require_odd`` is True and the supplied size is even, it is\n    incremented to the next odd number (a common requirement for median\u2011type\n    filters).  Empty inputs raise a ValueError.\n    \"\"\"\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.ndim != 1:\n        raise ValueError(\"Input signal must be one\u2011dimensional\")\n    if require_odd and window_size % 2 == 0:\n        window_size += 1\n    if arr.size < window_size:\n        raise ValueError(f\"Input length ({arr.size}) < window_size ({window_size})\")\n    if arr.size == 0:\n        raise ValueError(\"Input signal is empty\")\n    return arr, window_size\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (uniform weights).\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    if w == 1:\n        return arr.copy()\n    kernel = np.full(w, 1.0 / w, dtype=float)\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(size):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, size))\n    return w / w.sum()\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average that emphasises recent samples.\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    if w == 1:\n        return arr.copy()\n    # reverse so newest sample gets the largest weight\n    return np.convolve(arr, _exp_weights(w)[::-1], mode=\"valid\")\n\n\ndef median_filter(x, window_size=20):\n    \"\"\"Median filter (odd window) using stride tricks for speed.\"\"\"\n    arr, w = _validate_signal(x, window_size, require_odd=True)\n    if w == 1:\n        return arr.copy()\n    shape = (arr.size - w + 1, w)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\n@lru_cache(maxsize=None)\ndef _gaussian_kernel(size):\n    \"\"\"Cached Gaussian kernel (odd size, sum\u202f=\u202f1).\"\"\"\n    if size % 2 == 0:\n        size += 1\n    x = np.arange(size) - (size - 1) / 2.0\n    sigma = size / 6.0\n    g = np.exp(-0.5 * (x / sigma) ** 2)\n    return g / g.sum()\n\n\ndef gaussian_filter(x, window_size=20):\n    \"\"\"Gaussian smoothing filter.\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    return np.convolve(arr, _gaussian_kernel(w), mode=\"valid\")\n\n\n@lru_cache(maxsize=None)\ndef _savgol_coeffs(window_size, polyorder):\n    \"\"\"Cached Savitzky\u2011Golay convolution coefficients.\"\"\"\n    if polyorder >= window_size:\n        raise ValueError(\"polyorder must be less than window_size\")\n    half = window_size // 2\n    A = np.vander(np.arange(-half, half + 1), N=polyorder + 1, increasing=True)\n    return np.linalg.pinv(A)[0]\n\n\ndef savgol_filter(x, window_size=20, polyorder=2):\n    \"\"\"Savitzky\u2011Golay filter (polynomial smoothing).\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    coeffs = _savgol_coeffs(w, polyorder)\n    return np.convolve(arr, coeffs, mode=\"valid\")\n\n\ndef trimmed_mean_filter(x, window_size=20, trim_ratio=0.1):\n    \"\"\"Trimmed\u2011mean filter \u2013 discards a fraction of extreme values per window.\"\"\"\n    arr, w = _validate_signal(x, window_size)\n    k = int(trim_ratio * w)\n    if k * 2 >= w:  # nothing left after trimming\n        raise ValueError(\"trim_ratio too large for given window_size\")\n    shape = (arr.size - w + 1, w)\n    strides = (arr.strides[0], arr.strides[0])\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n    sorted_win = np.sort(windows, axis=1)\n    trimmed = sorted_win[:, k : w - k]\n    return trimmed.mean(axis=1)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\", **kwargs):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` (case\u2011insensitive):\n        enhanced, adaptive, median, gaussian, savgol, trimmed, hybrid\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algo == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if algo == \"median\":\n        return median_filter(input_signal, window_size)\n    if algo == \"gaussian\":\n        return gaussian_filter(input_signal, window_size)\n    if algo == \"savgol\":\n        return savgol_filter(input_signal, window_size, polyorder=kwargs.get(\"polyorder\", 2))\n    if algo == \"trimmed\":\n        return trimmed_mean_filter(input_signal, window_size, trim_ratio=kwargs.get(\"trim_ratio\", 0.1))\n    if algo == \"hybrid\":\n        # adaptive pass followed by exponential\u2011weighted smoothing\n        interim = adaptive_filter(input_signal, window_size)\n        return enhanced_filter_with_trend_preservation(interim, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}