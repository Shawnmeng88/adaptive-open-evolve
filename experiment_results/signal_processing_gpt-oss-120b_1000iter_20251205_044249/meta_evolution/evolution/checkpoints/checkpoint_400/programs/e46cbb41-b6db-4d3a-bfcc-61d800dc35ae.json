{"id": "e46cbb41-b6db-4d3a-bfcc-61d800dc35ae", "code": "# EVOLVE-BLOCK-START\ndef _to_arr(x, w):\n    a = np.asarray(x, float)\n    return a if a.size >= w else None\n\n\ndef adaptive_filter(x, w=20):\n    a = _to_arr(x, w)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\") if a is not None else np.empty(0, float)\n\n\ndef weighted_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return np.convolve(a, wt, mode=\"valid\")\n\n\ndef ema_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef enhanced_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    trend = adaptive_filter(a, w)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    weighted = weighted_filter(detrended, w)\n    if weighted.size == 0:\n        return weighted\n    half = max(1, w // 2)\n    return adaptive_filter(weighted, half)\n\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n])) if n else float(\"inf\")\n\n\ndef _select_best_filter(x, w):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, float)\n    best = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best]\n\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter,\n    }\n    f = dispatch.get(alg.lower())\n    return f(sig, w) if f else _select_best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "784590a4-4bbd-4c31-9aad-d1ceb7f373de", "generation": 6, "timestamp": 1764938498.981768, "iteration_found": 256, "metrics": {"runs_successfully": 0.0, "error": "Stage 1 error: name 'np' is not defined"}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.3624766970140094, "slope_changes": 68.6, "lag_error": 0.4099887786960041, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.20076318890739916, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.709225502436145, "accuracy_score": 0.20076318890739916, "efficiency_score": 1.0, "execution_time": 0.00035572052001953125, "success_rate": 1.0}, "island": 3}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are an expert Python programmer tasked with evolving the `construct_packing()` function (and any helper functions it calls) inside the **evolve block**.  \n\n**OUTPUT REQUIREMENTS**  \n- **Only** output the source code that belongs **between** the lines `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.  \n- **Do not** include the marker comments themselves, any imports, the surrounding `run_packing()` driver, or any other code outside that block.  \n- The system will automatically splice your output into the preserved file, so any extra text will cause a failure.\n\n**WHAT TO AVOID (based on previous failures)**  \n- Do **not** write a vague or overly aggressive prompt that leaves the model guessing about constraints.  \n- Do **not** produce code that relies on undefined global variables, external state, or side\u2011effects outside the block.  \n- Do **not** ignore edge\u2011case handling (empty input lists, zero\u2011size items, duplicate IDs, etc.).  \n- Do **not** return the wrong type (e.g., a list when a dict is expected) or forget to return a value at all.  \n- Do **not** leave debugging `print` statements or commented\u2011out code that could break execution.  \n- Do **not** repeat the same logic that previously generated invalid programs (e.g., recursive calls without a base case, mutable default arguments, or unguarded list comprehensions that can raise exceptions).  \n\n**STRATEGIC GUIDELINES FOR A VALID SOLUTION**  \n\n1. **Clarify the contract**:  \n   - `construct_packing(items, max_weight, max_volume)` receives a list of item dictionaries (`{'id': ..., 'weight': ..., 'volume': ...}`) and two numeric limits.  \n   - It must return a list of \u201cboxes\u201d, each box being a dict with keys `items`, `total_weight`, and `total_volume`.  \n   - Every input item must appear in exactly one box, and each box must respect both limits.\n\n2. **Deterministic greedy algorithm** (simple, reliable, easy to verify):  \n   - Sort items descending by a combined metric such as `weight + volume` or by the tighter constraint (`max_weight / weight` vs `max_volume / volume`).  \n   - Iterate items, trying to place each into the first existing box that can accommodate it; if none can, create a new box.  \n   - This produces a feasible packing (not necessarily optimal) and guarantees termination.\n\n3. **Edge\u2011case handling**:  \n   - If `items` is empty \u2192 return an empty list.  \n   - Validate that each item's `weight` \u2264 `max_weight` and `volume` \u2264 `max_volume`; if not, raise a clear `ValueError` with the offending item\u2019s `id`.  \n   - Guard against non\u2011numeric limits or missing keys with explicit checks and informative error messages.\n\n4. **Helper functions** (keep them inside the evolve block):  \n   - `_fits(item, box, max_weight, max_volume) \u2192 bool` \u2013 returns `True` only if adding the item keeps the box within both limits.  \n   - `_create_box(item) \u2192 dict` \u2013 builds a new box containing the first item.  \n   - Use these helpers to keep `construct_packing` concise and readable.\n\n5. **Immutability & safety**:  \n   - Never modify the original `items` list; work on a shallow copy if needed.  \n   - Do not use mutable default arguments.  \n   - Return fresh box dictionaries; do not reuse the same dict for multiple boxes.\n\n6. **Testing mindset**:  \n   - After writing the function, mentally run it on at least two tiny scenarios:  \n     a) `items=[{'id':1,'weight':2,'volume':3}]`, limits larger than the item \u2192 expect one box containing the item.  \n     b) Three items that together exceed one box but can be split \u2192 expect two boxes.  \n   - Ensure the returned structure matches the described schema.\n\n7. **Performance considerations (optional)**:  \n   - If you wish to improve beyond the basic greedy approach, you may implement a simple \u201cbest\u2011fit decreasing\u201d variant: choose the box that would leave the least remaining space after insertion. Keep the implementation short and deterministic.\n\n**Final Reminder**  \nYour entire response must be **only** the Python code that belongs inside the evolve block, no extra commentary, no markers, no surrounding context. Produce clean, PEP\u20118\u2011compliant code that fulfills the contract and passes all validation checks.", "user": "# Current Program Information\n- Fitness: 13.9292\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 13.9292 \u2192 13.9292. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n    Returns the filtered signal with length len(x) - window_size + 1.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        # Not enough data \u2013 return empty array for consistency\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"\n    Exponential weighted moving average.\n    Emphasises recent samples while preserving overall trend.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"\n    Classic exponential moving average (EMA).\n    Output length matches other filters (len(x) - window_size + 1).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Detrends the signal using a simple moving average, then applies\n    a weighted moving average to the detrended series.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    # Estimate local trend\n    trend = adaptive_filter(x, window_size)\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = x - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"\n    Compute variance of the residual (original - filtered).\n    Signals are aligned to the shortest length.\n    \"\"\"\n    min_len = min(len(original), len(filtered))\n    if min_len == 0:\n        return np.inf\n    residual = original[:min_len] - filtered[:min_len]\n    return float(np.var(residual))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"\n    Evaluate all available filters and return the one that yields the lowest\n    residual variance (i.e., highest estimated noise reduction).\n    \"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    # Choose filter with minimal residual variance relative to the raw signal\n    best_name = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_name]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatches to the requested filtering algorithm.\n    Supported types:\n        - \"basic\": simple moving average (adaptive_filter)\n        - \"weighted\": exponential weighted moving average (weighted_filter)\n        - \"ema\": classic exponential moving average (ema_filter)\n        - \"enhanced\": detrended weighted filter (enhanced_filter_with_trend_preservation)\n        - any other value: automatic selection of the best filter based on residual variance.\n    \"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if algorithm_type == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n\n    # Fallback: automatic selection\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive(x, w=20):\n    \"\"\"Simple moving average filter.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if w <= 0 or x.size < w:\n        return np.empty(0, dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef weighted(x, w=20):\n    \"\"\"Exponential weighted moving average filter.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if w <= 0 or x.size < w:\n        return np.empty(0, dtype=float)\n    weights = np.exp(np.linspace(-2, 0, w))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema(x, w=20):\n    \"\"\"Classic exponential moving average (EMA).\"\"\"\n    x = np.asarray(x, dtype=float)\n    if w <= 0 or x.size < w:\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(x)\n    e[0] = x[0]\n    for i in range(1, len(x)):\n        e[i] = alpha * x[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\n\ndef enhanced(x, w=20):\n    \"\"\"Detrend with simple moving average, then apply weighted filter.\"\"\"\n    trend = adaptive(x, w)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    return weighted(x - trend_full, w)\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original - filtered) over the overlapping region.\"\"\"\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(original[:n] - filtered[:n]))\n\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Dispatch to the requested filtering algorithm.\n    Supported types (case\u2011insensitive):\n        - \"basic\" / \"adaptive\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\"\n        - \"auto\" (or any unknown value) \u2192 automatic selection based on residual variance.\n    \"\"\"\n    alg = alg.lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive(sig, w)\n    if alg == \"weighted\":\n        return weighted(sig, w)\n    if alg == \"ema\":\n        return ema(sig, w)\n    if alg == \"enhanced\":\n        return enhanced(sig, w)\n\n    # Automatic selection (fallback)\n    candidates = {\n        \"basic\": adaptive(sig, w),\n        \"weighted\": weighted(sig, w),\n        \"ema\": ema(sig, w),\n        \"enhanced\": enhanced(sig, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(sig, candidates[k]))\n    return candidates[best_key]\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n    Returns the filtered signal with length len(x) - window_size + 1.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"\n    Exponential weighted moving average.\n    Emphasises recent samples while preserving overall trend.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"\n    Classic exponential moving average (EMA).\n    Output length matches other filters (len(x) - window_size + 1).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Detrends the signal using a simple moving average, then applies\n    an exponential weighted filter to the detrended series.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if len(x) < window_size:\n        raise ValueError(\"signal shorter than window\")\n    # Estimate local trend\n    trend = adaptive_filter(x, window_size)\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate(\n        (np.full(window_size - 1, trend[0]), trend)\n    )\n    detrended = x - trend_full\n    # Apply weighted smoothing\n    return weighted_filter(detrended, window_size)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatches to the requested filtering algorithm.\n    Types:\n        - \"basic\": simple moving average\n        - \"ema\": exponential moving average\n        - \"enhanced\": detrended weighted filter\n    \"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"ema\":\n        return ema_filter(input_signal, window_size)\n    return enhanced_filter_with_trend_preservation(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _ma(x: np.ndarray, w: int, wt: np.ndarray) -> np.ndarray:\n    \"\"\"Convolution helper for moving\u2011average\u2011style filters.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Uniform moving average (basic filter).\"\"\"\n    return _ma(x, window_size, np.ones(window_size) / window_size)\n\n\ndef weighted_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef ema_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = np.asarray(x, float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Detrended weighted smoothing.\n    1. Compute a simple moving\u2011average trend.\n    2. Subtract the trend (extended to original length).\n    3. Apply a weighted moving average to the detrended signal.\n    \"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    # extend trend to original length by repeating edge values\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = np.asarray(x, float) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef gaussian_ma(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Moving average with a Gaussian kernel.\"\"\"\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    sigma = max(window_size / 6.0, 0.1)\n    centre = (window_size - 1) / 2.0\n    i = np.arange(window_size, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef median_ma(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Median filter \u2013 robust to spikes/outliers.\"\"\"\n    x = np.asarray(x, float)\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    out_len = x.size - window_size + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(x[i : i + window_size])\n    return out\n\n\ndef double_enhanced(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Two\u2011stage enhancement:\n    1. Detrended weighted smoothing.\n    2. Light uniform smoothing with a smaller window (\u2248 half size).\n    \"\"\"\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    half = max(1, window_size // 2)\n    return adaptive_filter(first, half)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Mean\u2011squared residual between original and filtered signals.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return np.inf\n    return float(np.var(original[:m] - filtered[:m]))\n\n\ndef advanced_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Multi\u2011stage enhanced filter:\n    1. Detrended weighted smoothing.\n    2. Median filter to suppress spikes/outliers.\n    3. Light uniform smoothing with a smaller window (\u2248 half size).\n    \"\"\"\n    # stage 1 \u2013 detrended weighted\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n\n    # stage 2 \u2013 median to remove occasional spikes\n    medianed = median_ma(first, window_size)\n    if medianed.size == 0:\n        return np.empty(0, float)\n\n    # stage 3 \u2013 final light smoothing\n    second_w = max(1, window_size // 2)\n    return adaptive_filter(medianed, second_w)\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"Pick the filter with the lowest residual variance, now also considering ``advanced_filter``.\"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"gaussian\": gaussian_ma(x, window_size),\n        \"median\": median_ma(x, window_size),\n        \"double\": double_enhanced(x, window_size),\n        \"advanced\": advanced_filter(x, window_size),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(input_signal, window_size: int = 20, algorithm_type: str = \"enhanced\") -> np.ndarray:\n    \"\"\"\n    Dispatch to a filtering algorithm.\n    Supported ``algorithm_type`` values:\n        - \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\"\n        - \"gaussian\"\n        - \"median\"\n        - \"double\"\n        - \"advanced\"\n        - any other value \u2192 automatic selection of the best filter.\n    \"\"\"\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter_with_trend_preservation,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"double\": double_enhanced,\n        \"advanced\": advanced_filter,\n    }\n    if algorithm_type in dispatch:\n        return dispatch[algorithm_type](input_signal, window_size)\n    # Fallback \u2013 choose the most effective filter automatically\n    return _select_best_filter(np.asarray(input_signal, float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_array(x):\n    return np.asarray(x, dtype=float)\n\ndef _window_ok(arr, w):\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    return arr.size >= w\n\ndef adaptive_filter(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(a, w, mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    trend = adaptive_filter(a, window_size)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, window_size)\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best_filter(x, window_size):\n    a = _to_array(x)\n    candidates = {\n        \"basic\": adaptive_filter(a, window_size),\n        \"weighted\": weighted_filter(a, window_size),\n        \"ema\": ema_filter(a, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(a, window_size),\n    }\n    # discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = algorithm_type.lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    return _select_best_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.9292, Type: Migrant)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_array(x):\n    return np.asarray(x, dtype=float)\n\ndef _window_ok(arr, w):\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    return arr.size >= w\n\ndef adaptive_filter(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    return np.convolve(a, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(a, w, mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a = _to_array(x)\n    if not _window_ok(a, window_size):\n        return np.empty(0, dtype=float)\n    trend = adaptive_filter(a, window_size)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, window_size)\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best_filter(x, window_size):\n    a = _to_array(x)\n    candidates = {\n        \"basic\": adaptive_filter(a, window_size),\n        \"weighted\": weighted_filter(a, window_size),\n        \"ema\": ema_filter(a, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(a, window_size),\n    }\n    # discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = algorithm_type.lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    return _select_best_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _ma(x: np.ndarray, w: int, wt: np.ndarray) -> np.ndarray:\n    \"\"\"Convolution helper for moving\u2011average style filters.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Uniform moving average.\"\"\"\n    return _ma(x, window_size, np.ones(window_size) / window_size)\n\n\ndef weighted_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef ema_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = np.asarray(x, float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Detrend with a simple MA then apply a weighted MA.\"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = np.asarray(x, float) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef gaussian_ma(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Moving average with a Gaussian kernel.\"\"\"\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    sigma = max(window_size / 6.0, 0.1)\n    centre = (window_size - 1) / 2.0\n    i = np.arange(window_size, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef median_ma(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Median filter \u2013 robust to spikes/outliers.\"\"\"\n    x = np.asarray(x, float)\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    # use stride tricks for an efficient sliding window view\n    shape = (x.size - window_size + 1, window_size)\n    strides = (x.strides[0], x.strides[0])\n    windows = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef double_enhanced(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Two\u2011stage: detrended weighted smoothing then light uniform smoothing.\"\"\"\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    half = max(1, window_size // 2)\n    return adaptive_filter(first, half)\n\n\ndef advanced_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Multi\u2011stage: detrended weighted \u2192 median \u2192 light uniform smoothing.\"\"\"\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    medianed = median_ma(first, window_size)\n    if medianed.size == 0:\n        return np.empty(0, float)\n    second_w = max(1, window_size // 2)\n    return adaptive_filter(medianed, second_w)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Mean\u2011squared residual between original and filtered signals.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return np.inf\n    return float(np.var(original[:m] - filtered[:m]))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"gaussian\": gaussian_ma(x, window_size),\n        \"median\": median_ma(x, window_size),\n        \"double\": double_enhanced(x, window_size),\n        \"advanced\": advanced_filter(x, window_size),\n    }\n    return min(candidates.values(), key=lambda y: _residual_variance(x, y))\n\n\ndef process_signal(input_signal, window_size: int = 20, algorithm_type: str = \"enhanced\") -> np.ndarray:\n    \"\"\"Dispatch to a filtering algorithm, falling back to the best automatic choice.\"\"\"\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter_with_trend_preservation,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"double\": double_enhanced,\n        \"advanced\": advanced_filter,\n    }\n    if algorithm_type in dispatch:\n        return dispatch[algorithm_type](input_signal, window_size)\n    return _select_best_filter(np.asarray(input_signal, float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n# EVOLVE-BLOCK-START\ndef _validate_signal(x):\n    \"\"\"Convert input to a 1\u2011D float array; return empty array for invalid input.\"\"\"\n    try:\n        arr = np.asarray(x, dtype=float).ravel()\n    except Exception:\n        return np.empty(0, float)\n    if arr.size == 0:\n        return np.empty(0, float)\n    return arr\n\n\ndef _ma(x: np.ndarray, w: int, wt: np.ndarray) -> np.ndarray:\n    x = _validate_signal(x)\n    if w <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Uniform moving average (basic filter).\"\"\"\n    return _ma(x, window_size, np.ones(window_size) / window_size)\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = _validate_signal(x)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Detrend with a simple MA then apply a weighted MA.\"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = _validate_signal(x) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef gaussian_ma(x, window_size=20):\n    \"\"\"Moving average with a Gaussian kernel.\"\"\"\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    sigma = max(window_size / 6.0, 0.1)\n    centre = (window_size - 1) / 2.0\n    i = np.arange(window_size, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef median_ma(x, window_size=20):\n    \"\"\"Median filter \u2013 robust to spikes/outliers.\"\"\"\n    x = _validate_signal(x)\n    if window_size <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    out_len = x.size - window_size + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(x[i : i + window_size])\n    return out\n\n\ndef double_enhanced(x, window_size=20):\n    \"\"\"Two\u2011stage: enhanced filter then light uniform smoothing.\"\"\"\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    second_w = max(1, window_size // 2)\n    return adaptive_filter(first, second_w)\n\n\ndef advanced_filter(x, window_size=20):\n    \"\"\"\n    Multi\u2011stage filter:\n    1. Detrended weighted smoothing.\n    2. Median filter to suppress spikes.\n    3. Light uniform smoothing.\n    \"\"\"\n    first = enhanced_filter_with_trend_preservation(x, window_size)\n    if first.size == 0:\n        return np.empty(0, float)\n    medianed = median_ma(first, window_size)\n    if medianed.size == 0:\n        return np.empty(0, float)\n    final_w = max(1, window_size // 2)\n    return adaptive_filter(medianed, final_w)\n\n\ndef _residual_variance(orig: np.ndarray, filt: np.ndarray) -> float:\n    \"\"\"Mean\u2011squared residual between original and filtered signals.\"\"\"\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return np.inf\n    return float(np.var(orig[:m] - filt[:m]))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"\n    Choose the filter with the lowest residual variance.\n    Preference is given to the longest non\u2011empty result when variances tie.\n    \"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"double\": double_enhanced(x, window_size),\n        \"gaussian\": gaussian_ma(x, window_size),\n        \"median\": median_ma(x, window_size),\n        \"advanced\": advanced_filter(x, window_size),\n    }\n\n    # Compute residual variance for each candidate\n    scores = {k: _residual_variance(x, v) for k, v in candidates.items()}\n\n    # Find minimal variance\n    min_var = min(scores.values())\n    # Filter candidates with minimal variance\n    best_keys = [k for k, v in scores.items() if v == min_var]\n\n    # If multiple, pick the one yielding the longest output\n    best_key = max(best_keys, key=lambda k: len(candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n    Supported types: \"basic\", \"weighted\", \"ema\", \"enhanced\",\n    \"double\", \"gaussian\", \"median\", \"advanced\".\n    Any other value \u2192 automatic selection of the best filter.\n    \"\"\"\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter_with_trend_preservation,\n        \"double\": double_enhanced,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"advanced\": advanced_filter,\n    }\n    if algorithm_type in dispatch:\n        return dispatch[algorithm_type](input_signal, window_size)\n    # Fallback \u2013 choose the most effective filter automatically\n    return _select_best_filter(_validate_signal(input_signal), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Alternative runs_successfully approach, NumPy-based implementation\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_arr(x, w):\n    a = np.asarray(x, float)\n    return a if a.size >= w else None\n\ndef adaptive_filter(x, w=20):\n    a = _to_arr(x, w)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\") if a is not None else np.empty(0, float)\n\ndef weighted_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return np.convolve(a, wt, mode=\"valid\")\n\ndef ema_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\ndef enhanced_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    trend = adaptive_filter(a, w)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    return weighted_filter(a - trend_full, w)\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter,\n    }\n    f = dispatch.get(alg.lower())\n    return f(sig, w) if f else enhanced_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["def _to_arr(x, w):\n    a = np.asarray(x, float)\n    return a if a.size >= w else None\n\n\ndef adaptive_filter(x, w=20):\n    a = _to_arr(x, w)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\") if a is not None else np.empty(0, float)\n\n\ndef weighted_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return np.convolve(a, wt, mode=\"valid\")\n\n\ndef ema_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef enhanced_filter(x, w=20):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    trend = adaptive_filter(a, w)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    weighted = weighted_filter(detrended, w)\n    if weighted.size == 0:\n        return weighted\n    half = max(1, w // 2)\n    return adaptive_filter(weighted, half)\n\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n])) if n else float(\"inf\")\n\n\ndef _select_best_filter(x, w):\n    a = _to_arr(x, w)\n    if a is None:\n        return np.empty(0, float)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, float)\n    best = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best]\n\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter,\n    }\n    f = dispatch.get(alg.lower())\n    return f(sig, w) if f else _select_best_filter(sig, w)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}