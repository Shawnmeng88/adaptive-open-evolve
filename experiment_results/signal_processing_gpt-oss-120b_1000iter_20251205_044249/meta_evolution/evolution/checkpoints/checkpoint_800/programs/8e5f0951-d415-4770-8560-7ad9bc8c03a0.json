{"id": "8e5f0951-d415-4770-8560-7ad9bc8c03a0", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the hybrid filter.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    return hybrid_filter(input_signal, window_size)\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    First\u2011Fit Decreasing (FFD) bin\u2011packing.\n\n    Args:\n        items: List of non\u2011negative item sizes.\n        capacity: Positive integer representing bin capacity.\n\n    Returns:\n        A list of bins; each bin is a list of original item indices that fit\n        within the given capacity.  Items that are larger than the capacity\n        are placed in their own bin (cannot be combined with others).\n\n    The algorithm runs in O(n log n) time due to the initial sort.\n    \"\"\"\n    if capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not items:\n        return []\n\n    # Pair each item with its original index for stable output\n    indexed = [(idx, size) for idx, size in enumerate(items)]\n    # Sort by size descending (largest first)\n    indexed.sort(key=lambda pair: pair[1], reverse=True)\n\n    bins: List[List[int]] = []          # stores indices per bin\n    remaining: List[int] = []           # remaining capacity per bin\n\n    for idx, size in indexed:\n        if size < 0:\n            raise ValueError(\"item sizes must be non\u2011negative\")\n        # Items larger than the bin capacity get their own bin\n        if size > capacity:\n            bins.append([idx])\n            remaining.append(0)\n            continue\n\n        placed = False\n        for b_idx, space in enumerate(remaining):\n            if size <= space:\n                bins[b_idx].append(idx)\n                remaining[b_idx] -= size\n                placed = True\n                break\n\n        if not placed:\n            # Open a new bin\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    # Optional: sort indices inside each bin for readability\n    for bin_ in bins:\n        bin_.sort()\n    return bins\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "c8195913-67c8-4555-824b-804002f45080", "generation": 8, "timestamp": 1764942959.3029592, "iteration_found": 787, "metrics": {"runs_successfully": 1.0, "composite_score": 0.41069177655857453, "output_length": 91.0, "overall_score": 0.36931604257091033, "slope_changes": 70.2, "lag_error": 1.0017800350739763, "avg_error": 1.30369374594483, "false_reversals": 59.0, "correlation": 0.30346716971522975, "noise_reduction": 0.0, "smoothness_score": 0.22172949002217296, "responsiveness_score": 0.49955538694492213, "accuracy_score": 0.30346716971522975, "efficiency_score": 1.0, "execution_time": 0.0004741668701171875, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.41069177655857453, "output_length": 91.0, "overall_score": 0.36931604257091033, "slope_changes": 70.2, "lag_error": 1.0017800350739763, "avg_error": 1.30369374594483, "false_reversals": 59.0, "correlation": 0.30346716971522975, "noise_reduction": 0.0, "smoothness_score": 0.22172949002217296, "responsiveness_score": 0.49955538694492213, "accuracy_score": 0.30346716971522975, "efficiency_score": 1.0, "execution_time": 0.00040121078491210936, "success_rate": 1.0}, "island": 4}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\n**System Message \u2013 Guidance for Evolving `construct_packing()`**\n\nYou are given a Python file that contains a special editable region delimited by the comments  \n\n```\n# EVOLVE-BLOCK-START\n... existing code ...\n# EVOLVE-BLOCK-END\n```  \n\nYour task is to **replace only the code that appears between those two markers** with a new implementation that improves the `construct_packing()` function (and any helper functions defined inside the block).  \n\n**Output Requirements (must be obeyed exactly)**  \n- Output **only** the Python code that belongs between `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.  \n- **Do not** include the marker comments themselves.  \n- **Do not** output any other code (imports, `run_packing()`, etc.).  \n- The surrounding file will be automatically re\u2011assembled by the evaluation harness.\n\n**What to Focus On**  \n1. **Correctness** \u2013 The new `construct_packing()` must return a packing structure that satisfies the problem\u2019s constraints (e.g., all items placed, no overlap, respects capacity limits).  \n2. **Clarity** \u2013 Use descriptive variable names, short helper functions, and inline comments that explain non\u2011trivial steps.  \n3. **Efficiency** \u2013 Prefer linear\u2011time or O(n\u202flog\u202fn) approaches; avoid nested loops that scale quadratically with the number of items unless absolutely necessary.  \n4. **Robust Edge\u2011Case Handling** \u2013 Guard against empty input, items that exactly fill a bin, and items that cannot be packed (return a clear indicator).  \n5. **Determinism** \u2013 Do not rely on random choices unless you also provide a deterministic fallback; the evaluator runs the code multiple times and expects consistent results.\n\n**Specific Strategies to Try**  \n- **Greedy Bin\u2011First Fit**: Sort items by descending size and place each into the first bin that has enough remaining capacity. This often yields a good baseline.  \n- **Two\u2011Pointer Pairing**: For 1\u2011dimensional bin packing, sort items and then pair the smallest with the largest that fits together; this reduces the number of bins.  \n- **Chunked Packing Helper**: Write a small helper `fit_items_into_bin(items, capacity)` that returns the subset of items that fit and the remaining items; call it iteratively to build the full packing.  \n- **Early Validation**: Before entering the main loop, compute the total size of all items and compare it to the total available capacity; if the total exceeds capacity, return an explicit \u201ccannot pack\u201d result.  \n- **Unit Tests Within the Block** *(optional but allowed)*: You may include a short `if __name__ == \"__main__\":` block that runs a few sanity checks. This code will be ignored by the outer harness because it lies outside the markers.\n\n**What Must Be Avoided (explicit prohibitions)**  \n- **Over\u2011aggressive prompting**: Do not write a vague \u201crewrite everything\u201d instruction. Be precise about the function to modify and the constraints to respect.  \n- **Ambiguous constraints**: Never assume hidden requirements; always state the assumptions you are making in comments.  \n- **Repeated invalid programs**: Do not submit code that fails to compile or raises syntax errors. Ensure the code is syntactically correct and all referenced names are defined within the block.  \n- **Unnecessary complexity**: Do not introduce heavyweight libraries (e.g., `numpy`, `pandas`) or deep recursion that could cause stack overflows.  \n- **Leaving placeholder `pass` statements**: Every function defined in the block must contain a working implementation.\n\n**Reminder**  \nYour entire response will replace the content inside the evolve block. Follow the output format strictly, and concentrate on delivering a clean, correct, and efficient implementation of `construct_packing()` and its helpers. Good luck!", "user": "# Current Program Information\n- Fitness: 14.2259\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.2259 \u2192 14.2259. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0005, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0005, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _arr(x): return np.asarray(x, float).ravel()\ndef _valid(a, w): return w > 0 and a.size >= w\ndef _weights(w):\n    w = int(w)\n    wt = np.exp(np.linspace(-2, 0, w))\n    return wt / wt.sum()\n\ndef _simple(sig, w):\n    a = _arr(sig)\n    return np.convolve(a, np.ones(w) / w, \"valid\") if _valid(a, w) else np.empty(0)\n\ndef _weighted(sig, w):\n    a = _arr(sig)\n    return np.convolve(a, _weights(w), \"valid\") if _valid(a, w) else np.empty(0)\n\ndef _ema(sig, w):\n    a = _arr(sig)\n    if not _valid(a, w):\n        return np.empty(0)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, a.size):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\ndef _hybrid(sig, w):\n    s = _simple(sig, w)\n    v = _weighted(sig, w)\n    return (s + v) / 2.0 if s.size and v.size else s if s.size else v\n\ndef _var(orig, filt):\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n])) if n else float(\"inf\")\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    a = str(alg).lower()\n    if a == \"weighted\":\n        return _weighted(sig, w)\n    if a in (\"adaptive\", \"basic\", \"simple\"):\n        return _simple(sig, w)\n    if a == \"ema\":\n        return _ema(sig, w)\n    if a in (\"enhanced\", \"hybrid\"):\n        return _hybrid(sig, w)\n    # auto\u2011selection based on residual variance\n    arr = _arr(sig)\n    cand = {\n        \"simple\": _simple(arr, w),\n        \"weighted\": _weighted(arr, w),\n        \"ema\": _ema(arr, w),\n        \"hybrid\": _hybrid(arr, w),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0)\n    best = min(cand, key=lambda k: _var(arr, cand[k]))\n    return cand[best]\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef _conv(a, kernel):\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return _conv(a, np.ones(w) / w)\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return _conv(a, _exp_weights(w))\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _hybrid_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": _hybrid_filter(x, window_size),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0, dtype=float)\n    best = min(cand, key=lambda k: _residual_variance(x, cand[k]))\n    return cand[best]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return _hybrid_filter(input_signal, window_size)\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0005), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(a, w):\n    \"\"\"Return True if window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and a.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weights that sum to 1.\"\"\"\n    w = int(w)\n    wts = np.exp(np.linspace(-2, 0, w))\n    return wts / wts.sum()\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Uniform moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    kernel = np.full(w, 1.0 / w, dtype=float)\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef weighted_filter(x, w=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, w=20):\n    \"\"\"Exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, w=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a_f = adaptive_filter(x, w)\n    w_f = weighted_filter(x, w)\n    if a_f.size == 0 and w_f.size == 0:\n        return np.empty(0, dtype=float)\n    if a_f.size == 0:\n        return w_f\n    if w_f.size == 0:\n        return a_f\n    return (a_f + w_f) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean squared error between original (truncated) and filtered signal.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best(x, w):\n    \"\"\"Choose the filter with smallest residual variance against the original.\"\"\"\n    a = _to_array(x)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    w : int, optional\n        Window size (>0). Default 20.\n    alg : str, optional\n        Algorithm selector (case\u2011insensitive):\n        - \"adaptive\"\n        - \"weighted\"\n        - \"ema\"\n        - \"hybrid\" or \"enhanced\"\n        - \"auto\" (or any unknown value) \u2013 automatically pick the best filter.\n    \"\"\"\n    a = _to_array(sig)\n    key = str(alg).lower()\n    if key == \"adaptive\":\n        return adaptive_filter(a, w)\n    if key == \"weighted\":\n        return weighted_filter(a, w)\n    if key == \"ema\":\n        return ema_filter(a, w)\n    if key in (\"hybrid\", \"enhanced\"):\n        return hybrid_filter(a, w)\n    # Auto\u2011selection for \"auto\" or any other value\n    return _select_best(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0005), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the hybrid filter.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback\n    return hybrid_filter(input_signal, window_size)\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    First\u2011Fit Decreasing bin\u2011packing.\n\n    Args:\n        items: List of non\u2011negative item sizes.\n        capacity: Positive bin capacity.\n\n    Returns:\n        A list of bins, each bin is a list of original item indices that fit\n        within the given capacity.\n    \"\"\"\n    if capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not items:\n        return []\n\n    # Pair each item with its original index\n    indexed_items = [(i, size) for i, size in enumerate(items)]\n    # Sort items by size descending\n    indexed_items.sort(key=lambda x: x[1], reverse=True)\n\n    bins: List[List[int]] = []\n    remaining: List[int] = []\n\n    for idx, size in indexed_items:\n        if size > capacity:\n            # Item too large: allocate its own bin (cannot be packed)\n            bins.append([idx])\n            remaining.append(0)\n            continue\n\n        placed = False\n        for b_idx, cap_left in enumerate(remaining):\n            if size <= cap_left:\n                bins[b_idx].append(idx)\n                remaining[b_idx] -= size\n                placed = True\n                break\n        if not placed:\n            # Open a new bin\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    # Optional: sort indices inside each bin for readability\n    for b in bins:\n        b.sort()\n    return bins\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:m] - filt[:m]))\n\ndef _select_best_filter(x, window_size):\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": _hybrid_filter(x, window_size),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg == \"basic\" or alg == \"adaptive\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return _hybrid_filter(input_signal, window_size)\n    # fallback: automatic selection based on residual variance\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\ntry:\n    from scipy import signal\nexcept Exception:  # pragma: no cover\n    signal = None\n\n\ndef _odd_window(window_size: int) -> int:\n    \"\"\"Return an odd window size (required for Savitzky\u2011Golay).\"\"\"\n    return window_size if window_size % 2 else window_size + 1\n\n\ndef _as_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (baseline). Returns empty array if window is invalid.\"\"\"\n    x = _as_array(x)\n    if window_size <= 0 or x.size < window_size:\n        return np.empty(0, dtype=float)\n    # Convolution with a uniform kernel gives the moving average.\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Weighted moving\u2011average filter that emphasizes recent samples.\n    Detrends the signal with a simple moving average before weighting.\n    \"\"\"\n    x = _as_array(x)\n    if window_size <= 0 or x.size < window_size:\n        return np.empty(0, dtype=float)\n\n    # Baseline trend (simple moving average)\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n\n    # Pad trend to original length (repeat first value for the leading samples)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n\n    # Detrended signal\n    detrended = x - trend_full\n\n    # Exponential weights\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n\n    # Weighted moving average on detrended data\n    return np.convolve(detrended, weights, mode=\"valid\")\n\n\ndef savgol_filter_with_trend_preservation(x, window_size=21, polyorder=2):\n    \"\"\"\n    Savitzky\u2011Golay filter (low\u2011order polynomial smoothing) that keeps\n    local trends while reducing noise.\n    Returns empty array if SciPy is unavailable or parameters are invalid.\n    \"\"\"\n    if signal is None:\n        return np.empty(0, dtype=float)\n\n    x = _as_array(x)\n    if window_size < 3:\n        return np.empty(0, dtype=float)\n\n    odd_ws = _odd_window(window_size)\n    if x.size < odd_ws:\n        return np.empty(0, dtype=float)\n\n    # Apply filter (full\u2011length output)\n    filtered_full = signal.savgol_filter(x, odd_ws, polyorder, mode=\"interp\")\n    # Trim the transient to keep output length consistent with other filters\n    start = odd_ws - 1\n    return filtered_full[start:]\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"\n    Compute variance of the residual (original - filtered) over the overlapping region.\n    Returns ``inf`` for empty overlap.\n    \"\"\"\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(original[:n] - filtered[:n]))\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Parameters\n    ----------\n    input_signal : array\u2011like\n        Raw 1\u2011D signal.\n    window_size : int, optional\n        Base window size for the chosen algorithm.\n    algorithm_type : str, optional\n        \"basic\", \"adaptive\", \"enhanced\", \"savgol\", \"auto\" or any other value.\n        Unknown values trigger automatic selection based on residual variance.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal (may be empty if the chosen method cannot be applied).\n    \"\"\"\n    alg = algorithm_type.lower()\n\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if alg == \"savgol\":\n        return savgol_filter_with_trend_preservation(input_signal, window_size, polyorder=2)\n    if alg == \"auto\":\n        # Explicit auto\u2011selection request\n        pass  # fall through to auto\u2011selection logic below\n\n    # ----- Automatic selection (fallback) -----\n    candidates = {\n        \"basic\": adaptive_filter(input_signal, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(input_signal, window_size),\n        \"savgol\": savgol_filter_with_trend_preservation(input_signal, window_size, polyorder=2),\n    }\n\n    # Keep only non\u2011empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n\n    # Choose the candidate with minimal residual variance relative to the raw signal\n    best_key = min(candidates, key=lambda k: _residual_variance(input_signal, candidates[k]))\n    return candidates[best_key]\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 14.2259, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef _conv(a, kernel):\n    return np.convolve(a, kernel, mode=\"valid\")\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return _conv(a, np.ones(w) / w)\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return _conv(a, _exp_weights(w))\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _hybrid_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": _hybrid_filter(x, window_size),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0, dtype=float)\n    best = min(cand, key=lambda k: _residual_variance(x, cand[k]))\n    return cand[best]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"enhanced\").lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return _hybrid_filter(input_signal, window_size)\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.7886, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_array(x):\n    \"\"\"Convert input to 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(w):\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Simple moving\u2011average (valid convolution).\"\"\"\n    _validate_window(w)\n    a = _to_array(x)\n    if a.size < w:\n        return np.empty(0, dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef weighted_filter(x, w=20):\n    \"\"\"Exponential\u2011weighted moving average.\"\"\"\n    _validate_window(w)\n    a = _to_array(x)\n    if a.size < w:\n        return np.empty(0, dtype=float)\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return np.convolve(a, wt, mode=\"valid\")\n\n\ndef ema_filter(x, w=20):\n    \"\"\"Classic exponential moving average (EMA).\"\"\"\n    _validate_window(w)\n    a = _to_array(x)\n    if a.size < w:\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef median_filter(x, w=20):\n    \"\"\"Median filter using a sliding window (odd size).\"\"\"\n    _validate_window(w)\n    a = _to_array(x)\n    if a.size < w:\n        return np.empty(0, dtype=float)\n    if w % 2 == 0:  # enforce odd window for symmetric median\n        w += 1\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef enhanced_filter_with_trend_preservation(x, w=20):\n    \"\"\"\n    Detrend using a simple moving average, smooth the detrended signal,\n    then re\u2011add the aligned trend segment.\n    \"\"\"\n    _validate_window(w)\n    a = _to_array(x)\n    if a.size < w:\n        return np.empty(0, dtype=float)\n\n    # Estimate local trend\n    trend = adaptive_filter(a, w)\n    if trend.size == 0:\n        return np.empty(0, dtype=float)\n\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n\n    # Remove trend, smooth residual, and add back aligned trend\n    detrended = a - trend_full\n    smoothed = weighted_filter(detrended, w)\n    # Align trend segment with smoothed output\n    aligned_trend = trend_full[w - 1 :]\n    return smoothed + aligned_trend\n\n\ndef _correlation_score(orig, filt):\n    \"\"\"Absolute Pearson correlation over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n < 2:\n        return -np.inf\n    corr = np.corrcoef(orig[:n], filt[:n])[0, 1]\n    return abs(corr)\n\n\ndef _select_best_filter(x, w):\n    \"\"\"\n    Evaluate all available filters and return the one with the highest\n    absolute correlation to the original signal.\n    \"\"\"\n    a = _to_array(x)\n    candidates = {\n        \"basic\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"enhanced\": enhanced_filter_with_trend_preservation(a, w),\n        \"median\": median_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n\n    best_key = max(candidates, key=lambda k: _correlation_score(a, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Dispatch to the requested filtering algorithm.\n    Supported algorithms:\n        - \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\"\n        - \"median\"\n        - any other value \u2192 automatic selection based on correlation.\n    \"\"\"\n    alg = alg.lower()\n    if alg == \"basic\":\n        return adaptive_filter(sig, w)\n    if alg == \"weighted\":\n        return weighted_filter(sig, w)\n    if alg == \"ema\":\n        return ema_filter(sig, w)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, w)\n    if alg == \"median\":\n        return median_filter(sig, w)\n    # fallback to automatic selection\n    return _select_best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the hybrid filter.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    # Fallback\n    return hybrid_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Check that window size is a positive int and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached exponential weighting vector that sums to 1.\"\"\"\n    w = int(w)\n    weights = np.exp(np.linspace(-2, 0, w))\n    return weights / weights.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Recursive exponential moving\u2011average (EMA) filter.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Average of adaptive and weighted filters (same length).\"\"\"\n    a = adaptive_filter(x, window_size)\n    b = weighted_filter(x, window_size)\n    if a.size == 0:\n        return b\n    if b.size == 0:\n        return a\n    return (a + b) / 2.0\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual over the overlapping region.\"\"\"\n    n = min(len(orig), len(filt))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\ndef _auto_select(x, window_size=20):\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\"\n        - \"weighted\"\n        - \"ema\"\n        - \"enhanced\" / \"hybrid\"\n        - \"auto\"\n    Unknown values fall back to the hybrid filter.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return hybrid_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    return hybrid_filter(input_signal, window_size)\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    First\u2011Fit Decreasing (FFD) bin\u2011packing.\n\n    Args:\n        items: List of non\u2011negative item sizes.\n        capacity: Positive integer representing bin capacity.\n\n    Returns:\n        A list of bins; each bin is a list of original item indices that fit\n        within the given capacity.  Items that are larger than the capacity\n        are placed in their own bin (cannot be combined with others).\n\n    The algorithm runs in O(n log n) time due to the initial sort.\n    \"\"\"\n    if capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not items:\n        return []\n\n    # Pair each item with its original index for stable output\n    indexed = [(idx, size) for idx, size in enumerate(items)]\n    # Sort by size descending (largest first)\n    indexed.sort(key=lambda pair: pair[1], reverse=True)\n\n    bins: List[List[int]] = []          # stores indices per bin\n    remaining: List[int] = []           # remaining capacity per bin\n\n    for idx, size in indexed:\n        if size < 0:\n            raise ValueError(\"item sizes must be non\u2011negative\")\n        # Items larger than the bin capacity get their own bin\n        if size > capacity:\n            bins.append([idx])\n            remaining.append(0)\n            continue\n\n        placed = False\n        for b_idx, space in enumerate(remaining):\n            if size <= space:\n                bins[b_idx].append(idx)\n                remaining[b_idx] -= size\n                placed = True\n                break\n\n        if not placed:\n            # Open a new bin\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    # Optional: sort indices inside each bin for readability\n    for bin_ in bins:\n        bin_.sort()\n    return bins"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}