{"id": "52bc9c53-9367-4543-9eb7-50a207683c1b", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_input(x, window_size):\n    \"\"\"Validate ``window_size`` and convert ``x`` to a 1\u2011D float array.\n\n    Returns (arr, ws). If ``arr`` is shorter than ``ws`` an empty array is\n    returned so callers can handle the degenerate case without raising.\n    \"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    ws = int(window_size)\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    \"\"\"Cached exponential weighting vector (right\u2011biased).\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef _moving_average(arr, ws):\n    \"\"\"Fast simple moving average using cumulative sum.\"\"\"\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[ws:] - csum[:-ws]) / ws\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    return _moving_average(arr, ws)\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (enhanced).\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    w = _exp_weights(ws)[::-1]  # reverse for np.convolve semantics\n    return np.convolve(arr, w, mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    \"\"\"Recursive exponential smoothing; output length matches other filters.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _savgol_filter(x, window_size, polyorder=3):\n    \"\"\"Savitzky\u2011Golay filter with safe fallback to ``enhanced_filter``.\"\"\"\n    try:\n        from scipy.signal import savgol_filter\n        w = int(window_size)\n        if w % 2 == 0:\n            w += 1\n        w = max(w, polyorder + 2)\n        return savgol_filter(x, window_length=w, polyorder=polyorder, mode=\"interp\")\n    except Exception:\n        return enhanced_filter(x, window_size)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Apply ``enhanced_filter`` then a simple moving average for extra smoothing.\"\"\"\n    first = enhanced_filter(x, window_size)\n    if first.size == 0:\n        return first\n    arr, ws = _prepare_input(first, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    return _moving_average(arr, ws)\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original - filtered) aligned to the shortest length.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef _select_best_filter(sig, window_size):\n    \"\"\"Choose the filter yielding the smallest residual variance.\"\"\"\n    candidates = {\n        \"adaptive\": adaptive_filter(sig, window_size),\n        \"enhanced\": enhanced_filter(sig, window_size),\n        \"smooth\": exponential_smoothing(sig, window_size),\n        \"savgol\": _savgol_filter(sig, window_size),\n        \"hybrid\": _hybrid_filter(sig, window_size),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(sig, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average\n        - \"smooth\"   : recursive exponential smoothing\n        - \"savgol\"   : Savitzky\u2011Golay (fallback to enhanced)\n        - \"hybrid\"   : enhanced followed by simple moving average\n        - \"auto\" or any unknown value : automatically pick the best filter\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    if alg == \"smooth\":\n        return exponential_smoothing(sig, window_size)\n    if alg == \"savgol\":\n        return _savgol_filter(sig, window_size)\n    if alg == \"hybrid\":\n        return _hybrid_filter(sig, window_size)\n    # \"auto\" or any unsupported type falls back to automatic selection\n    return _select_best_filter(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "717ff683-81e5-4409-b6ed-1245273a6292", "generation": 3, "timestamp": 1764938867.3364859, "iteration_found": 299, "metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.0003214359283447266, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.0005305290222167968, "success_rate": 1.0}, "island": 1}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are a specialized code\u2011evolution assistant. The only portion of the program you may edit is the code that lies between the markers `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`. **Your output must contain *exactly* the new or modified lines that belong inside that region \u2013 do not output the markers themselves, any surrounding imports, the `run_packing()` driver, or any other file content.**  \n\n### Goal\nImprove the `construct_packing()` function and any helper functions defined inside the evolve block so that the packing algorithm produces a valid solution for the given bin\u2011packing problem and scores higher than the current baseline.\n\n### Strict Constraints (must never be violated)\n1. **Output only the code that belongs between the markers.**  \n2. **Do not include the marker lines themselves.**  \n3. **Do not add, remove, or modify code outside the evolve block** (e.g., imports, global constants, the `run_packing()` wrapper).  \n4. **Do not change the public signature of `construct_packing`** (it must remain `def construct_packing(items, bins):`).  \n5. **All produced code must be syntactically correct Python** and must not raise exceptions for the test harness.  \n\n### What to Avoid (previous failures)\n- Writing a full program or any code outside the evolve block.  \n- Omitting the required function signatures or returning the wrong type.  \n- Introducing syntax errors, undefined variables, or missing imports that are not already present.  \n- Repeating the same naive or incomplete heuristic that led to 13 consecutive invalid programs.  \n\n### Suggested Strategies (domain\u2011specific)\n1. **Pre\u2011process the input**: sort `items` in descending order of size; sort `bins` by capacity (largest first).  \n2. **Apply a classic heuristic** such as *First\u2011Fit Decreasing* (FFD) or *Best\u2011Fit Decreasing* (BFD).  \n3. **Maintain per\u2011bin state**: current load, remaining capacity, and a list of assigned item indices.  \n4. **When an item does not fit any bin**, try a simple *item\u2011swap* or *bin\u2011split* fallback:  \n   - Look for a bin that can accommodate the item after moving a smaller item to another bin with spare space.  \n   - If no swap is possible, place the item in the bin with the most remaining capacity (even if it exceeds the limit) and mark the solution as \u201cover\u2011packed\u201d \u2013 the evaluator will penalize this, but it prevents a crash.  \n5. **Return the packing** as a list of lists where each inner list contains the indices of items placed in the corresponding bin. Ensure the length of the outer list equals `len(bins)`.  \n\n### Implementation Tips\n- Use local helper functions inside the evolve block (e.g., `_first_fit_decreasing`, `_try_swap`).  \n- Keep the code concise; avoid unnecessary loops that could cause timeouts.  \n- Add inline comments only if they help clarify the algorithm \u2013 they will be part of the output.  \n- Do **not** import new modules; rely only on built\u2011in types and functions already available.  \n\nProceed to rewrite or augment the code inside the evolve block according to these guidelines. Remember: **output only the modified/added lines that belong between the markers.**", "user": "# Current Program Information\n- Fitness: 13.7277\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 13.9292 \u2192 13.7277. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Unknown changes\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4429, output_length: 91.0000, overall_score: 0.3625, slope_changes: 68.6000, lag_error: 0.4100, avg_error: 1.1151, false_reversals: 56.6000, correlation: 0.2008, noise_reduction: 0.0000, smoothness_score: 0.2257, responsiveness_score: 0.7092, accuracy_score: 0.2008, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0003, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _validate_signal(x, window_size):\n    \"\"\"Validate and convert input to a 1\u2011D float array.\"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size == 0:\n        return arr\n    if arr.size < window_size:\n        return np.empty(0, dtype=float)\n    return arr\n\ndef _moving_average(arr, w):\n    \"\"\"Fast moving\u2011average using cumulative sum (valid convolution).\"\"\"\n    if w <= 1:\n        return arr.copy()\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[w:] - csum[:-w]) / w\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    arr = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return np.array([], dtype=float)\n    return _moving_average(arr, window_size)\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the selected algorithm (all map to simple moving average).\"\"\"\n    return adaptive_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0003), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"\n    Simple moving average filter using convolution.\n    Returns the filtered signal with length len(x) - window_size + 1.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        # Not enough data \u2013 return empty array for consistency\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"\n    Exponential weighted moving average.\n    Emphasises recent samples while preserving overall trend.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    weights = np.exp(np.linspace(-2, 0, window_size))\n    weights /= weights.sum()\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"\n    Classic exponential moving average (EMA).\n    Output length matches other filters (len(x) - window_size + 1).\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Detrends the signal using a simple moving average, then applies\n    a weighted moving average to the detrended series.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    # Estimate local trend\n    trend = adaptive_filter(x, window_size)\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = x - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"\n    Compute variance of the residual (original - filtered).\n    Signals are aligned to the shortest length.\n    \"\"\"\n    min_len = min(len(original), len(filtered))\n    if min_len == 0:\n        return np.inf\n    residual = original[:min_len] - filtered[:min_len]\n    return float(np.var(residual))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"\n    Evaluate all available filters and return the one that yields the lowest\n    residual variance (i.e., highest estimated noise reduction).\n    \"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    # Choose filter with minimal residual variance relative to the raw signal\n    best_name = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_name]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatches to the requested filtering algorithm.\n    Supported types:\n        - \"basic\": simple moving average (adaptive_filter)\n        - \"weighted\": exponential weighted moving average (weighted_filter)\n        - \"ema\": classic exponential moving average (ema_filter)\n        - \"enhanced\": detrended weighted filter (enhanced_filter_with_trend_preservation)\n        - any other value: automatic selection of the best filter based on residual variance.\n    \"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if algorithm_type == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n\n    # Fallback: automatic selection\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _validate_signal(x, window_size):\n    \"\"\"Validate input and return a 1\u2011D float ndarray.\n\n    Raises:\n        ValueError: if ``window_size`` is not a positive integer.\n    Returns:\n        np.ndarray: the flattened input array (may be empty).\n    \"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size == 0:\n        return arr\n    if arr.size < window_size:\n        # Consistent with previous behaviour: return empty array when not enough data\n        return np.empty(0, dtype=float)\n    return arr\n\n\ndef _moving_average(arr, w):\n    \"\"\"Fast moving\u2011average using cumulative sum (valid convolution).\"\"\"\n    if w <= 1:\n        return arr.copy()\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[w:] - csum[:-w]) / w\n\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    \"\"\"Cache exponential weighting vector for a given window size.\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Uniform simple moving\u2011average filter.\"\"\"\n    arr = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    return _moving_average(arr, window_size)\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (EWMA).\"\"\"\n    arr = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    return np.convolve(arr, _exp_weights(window_size), mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (EMA). Output length matches other filters.\"\"\"\n    arr = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Detrend using a simple moving average then apply EWMA.\"\"\"\n    arr = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    # Estimate local trend with a basic moving average\n    trend = _moving_average(arr, window_size)\n    # Pad trend to original length (repeat first trend value)\n    pad_len = window_size - 1\n    trend_full = np.concatenate((np.full(pad_len, trend[0]), trend))\n    detrended = arr - trend_full\n    return np.convolve(detrended, _exp_weights(window_size), mode=\"valid\")\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original - filtered) aligned to the shortest length.\"\"\"\n    min_len = min(len(original), len(filtered))\n    if min_len == 0:\n        return float(\"inf\")\n    residual = original[:min_len] - filtered[:min_len]\n    return float(np.var(residual))\n\n\ndef _select_best_filter(x, window_size):\n    \"\"\"Choose the filter with the smallest residual variance.\"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\" / \"adaptive\" : simple moving average\n        - \"weighted\"           : exponential\u2011weighted moving average\n        - \"ema\"                : classic exponential moving average\n        - \"enhanced\"           : detrended weighted filter\n        - any other value      : automatic selection of the best filter\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n\n    # Fallback to automatic selection\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4429), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (68.6000), Performs well on lag_error (0.4100), Performs well on avg_error (1.1151), Performs well on false_reversals (56.6000), Performs well on correlation (0.2008), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2257), Performs well on responsiveness_score (0.7092), Performs well on accuracy_score (0.2008), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 13.9292)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n\ndef _validate_signal(x, window_size):\n    \"\"\"Validate the input signal and window size.\n\n    Returns a tuple (array, int) where ``array`` is a 1\u2011D float ndarray and\n    ``int`` is the validated window size.\n    \"\"\"\n    try:\n        ws = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    return arr, ws\n\n\ndef _moving_average(arr, w):\n    \"\"\"Simple moving\u2011average using cumulative sums (valid convolution).\"\"\"\n    if arr.size < w:\n        return np.empty(0, dtype=float)\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[w:] - csum[:-w]) / w\n\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    \"\"\"Cache exponential weighting vector for a given window size.\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Uniform moving\u2011average filter.\"\"\"\n    arr, ws = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    return _moving_average(arr, ws)\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (EWMA).\"\"\"\n    arr, ws = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    if arr.size < ws:\n        return np.empty(0, dtype=float)\n    return np.convolve(arr, _exp_weights(ws), mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (EMA). Output length matches other filters.\"\"\"\n    arr, ws = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    if arr.size < ws:\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (ws + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[ws - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Detrend with a simple moving average then apply EWMA.\"\"\"\n    arr, ws = _validate_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    if arr.size < ws:\n        return np.empty(0, dtype=float)\n\n    # Estimate local trend using the basic moving average\n    trend = adaptive_filter(arr, ws)\n\n    # Pad trend to original length (repeat first trend value)\n    pad_len = ws - 1\n    trend_full = np.concatenate((np.full(pad_len, trend[0]), trend))\n\n    detrended = arr - trend_full\n    return weighted_filter(detrended, ws)\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original - filtered) aligned to the shortest length.\"\"\"\n    min_len = min(len(original), len(filtered))\n    if min_len == 0:\n        return float(\"inf\")\n    residual = original[:min_len] - filtered[:min_len]\n    return float(np.var(residual))\n\n\ndef _select_best_filter(x, window_size):\n    \"\"\"Choose the filter with the smallest residual variance.\"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the requested filtering algorithm (case\u2011insensitive).\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n\n    # Unknown type \u2013 fall back to automatic selection\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.4402, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom typing import Any\n\ndef _validate_signal(x: Any, window_size: int) -> np.ndarray:\n    \"\"\"Validate input array and window size.\"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"Input signal must be a 1\u2011D array\")\n    if arr.size < window_size:\n        raise ValueError(f\"Input length ({arr.size}) < window_size ({window_size})\")\n    return arr\n\n\ndef _moving_average(arr: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"Fast moving\u2011average using cumulative sum.\"\"\"\n    cumsum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (cumsum[window_size:] - cumsum[:-window_size]) / window_size\n\n\ndef adaptive_filter(x: Any, window_size: int = 20) -> np.ndarray:\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    arr = _validate_signal(x, window_size)\n    return _moving_average(arr, window_size)\n\n\ndef enhanced_filter_with_trend_preservation(x: Any, window_size: int = 20) -> np.ndarray:\n    \"\"\"Exponential\u2011weighted moving\u2011average filter.\"\"\"\n    arr = _validate_signal(x, window_size)\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(arr, w, mode=\"valid\")\n\n\ndef exponential_smoothing(x: Any, window_size: int = 20) -> np.ndarray:\n    \"\"\"Recursive exponential smoothing; output aligned with other filters.\"\"\"\n    arr = _validate_signal(x, window_size)\n    alpha = 2.0 / (window_size + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[window_size - 1 :]\n\n\n# Optional Savitzky\u2011Golay filter if SciPy is available\ntry:\n    from scipy.signal import savgol_filter\n\n    _HAS_SAVGOL = True\nexcept Exception:\n    _HAS_SAVGOL = False\n\n\ndef _savgol_filter(x: Any, window_size: int = 20, polyorder: int = 3) -> np.ndarray:\n    \"\"\"Savitzky\u2011Golay filter with safe parameter handling.\"\"\"\n    arr = _validate_signal(x, window_size)\n    if not _HAS_SAVGOL:\n        return adaptive_filter(arr, window_size)\n\n    w = int(window_size)\n    if w % 2 == 0:\n        w += 1\n    if w < polyorder + 2:\n        w = polyorder + 2\n        if w % 2 == 0:\n            w += 1\n    return savgol_filter(arr, window_length=w, polyorder=polyorder, mode=\"interp\")\n\n\ndef process_signal(\n    input_signal: Any, window_size: int = 20, algorithm_type: str = \"enhanced\"\n) -> np.ndarray:\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported algorithm_type values:\n        \"basic\"    \u2013 simple moving average\n        \"enhanced\" \u2013 exponential\u2011weighted moving average\n        \"exp\"      \u2013 exponential smoothing\n        \"savgol\"   \u2013 Savitzky\u2011Golay (fallback to basic)\n        \"hybrid\"   \u2013 weighted filter followed by exponential smoothing\n    \"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algorithm_type == \"exp\":\n        return exponential_smoothing(input_signal, window_size)\n    if algorithm_type == \"savgol\":\n        return _savgol_filter(input_signal, window_size)\n    if algorithm_type == \"hybrid\":\n        weighted = enhanced_filter_with_trend_preservation(input_signal, window_size)\n        return exponential_smoothing(weighted, window_size)\n    raise ValueError(f\"Unknown algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 9.4583, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import Tuple\n\ndef _prepare_input(x, window: int) -> Tuple[np.ndarray, int]:\n    ws = int(window)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=32)\ndef _exp_weights(ws: int) -> np.ndarray:\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    arr, ws = _prepare_input(x, window_size)\n    return np.convolve(arr, np.ones(ws) / ws, mode=\"valid\") if arr.size else arr\n\ndef _apply_savgol(signal: np.ndarray, base_ws: int, polyorder: int = 3) -> np.ndarray:\n    try:\n        from scipy import signal as scipy_signal\n    except Exception:\n        return np.convolve(signal, np.ones(base_ws) / base_ws, mode=\"valid\")\n    var = np.var(signal)\n    mean = np.mean(np.abs(signal)) + 1e-9\n    scale = var / mean\n    win = int(max(polyorder + 2, min(base_ws * 2, base_ws * scale)))\n    if win % 2 == 0:\n        win += 1\n    win = max(win, polyorder + 2)\n    if win % 2 == 0:\n        win += 1\n    return scipy_signal.savgol_filter(signal, win, polyorder, mode=\"interp\")\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    arr, ws = _prepare_input(x, window_size)\n    if not arr.size:\n        return arr\n    ewma = np.convolve(arr, _exp_weights(ws), mode=\"valid\")\n    sg = _apply_savgol(ewma, ws)\n    return np.convolve(sg, np.ones(5) / 5, mode=\"valid\") if sg.size >= 5 else sg\n\ndef exponential_smoothing(x, window_size: int = 20) -> np.ndarray:\n    arr, ws = _prepare_input(x, window_size)\n    if not arr.size:\n        return arr\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef process_signal(input_signal, window_size: int = 20, algorithm_type: str = \"enhanced\") -> np.ndarray:\n    alg = algorithm_type.lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if alg == \"exp\":\n        return exponential_smoothing(input_signal, window_size)\n    if alg == \"savgol\":\n        arr, ws = _prepare_input(input_signal, window_size)\n        return _apply_savgol(arr, ws) if arr.size else arr\n    if alg == \"hybrid\":\n        tmp = enhanced_filter_with_trend_preservation(input_signal, window_size)\n        return exponential_smoothing(tmp, window_size)\n    return enhanced_filter_with_trend_preservation(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (87.000)\n\n### Inspiration 3 (Score: 10.7861, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import Callable, Dict, Tuple\n\n\ndef _validate_window(window_size: int) -> int:\n    \"\"\"Validate that ``window_size`` is a positive integer.\"\"\"\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError(f\"window_size must be an integer, got {type(window_size).__name__}\")\n    ws = int(window_size)\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    return ws\n\n\ndef _prepare_signal(x, window_size: int) -> np.ndarray:\n    \"\"\"\n    Convert *x* to a 1\u2011D ``np.ndarray`` of ``float`` and enforce the minimum\n    length required for a ``valid`` convolution.\n\n    If the input is shorter than ``window_size`` an empty array is returned,\n    allowing callers to handle the edge case gracefully.\n    \"\"\"\n    ws = _validate_window(window_size)\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"Input signal must be one\u2011dimensional\")\n    if arr.size < ws:\n        return np.array([], dtype=float)\n    return arr\n\n\n@lru_cache(maxsize=32)\ndef _gaussian_weights(window_size: int) -> np.ndarray:\n    \"\"\"\n    Return a normalized Gaussian kernel of length ``window_size``.\n    The kernel is centred and uses ``sigma = max(window/6, 0.1)`` so that\n    \u00b13\u03c3 \u2248 window size.\n    \"\"\"\n    ws = _validate_window(window_size)\n    sigma = max(ws / 6.0, 0.1)\n    centre = (ws - 1) / 2.0\n    idx = np.arange(ws)\n    w = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return w / w.sum()\n\n\n@lru_cache(maxsize=32)\ndef _exp_weights(window_size: int) -> np.ndarray:\n    \"\"\"\n    Return a normalized exponential\u2011decay weighting vector.\n    Used by the enhanced filter when SciPy is unavailable.\n    \"\"\"\n    ws = _validate_window(window_size)\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Simple moving\u2011average filter using a cumulative\u2011sum implementation.\"\"\"\n    arr = _prepare_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    ws = len(arr) if isinstance(window_size, np.ndarray) else window_size\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[ws:] - csum[:-ws]) / ws\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Gaussian\u2011weighted moving average.  If SciPy's Savitzky\u2011Golay is available,\n    that implementation is preferred via ``_savgol_filter``; otherwise we fall back\n    to a Gaussian kernel.\n    \"\"\"\n    arr = _prepare_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    w = _gaussian_weights(window_size)\n    return np.convolve(arr, w, mode=\"valid\")\n\n\ndef _savgol_filter(x, window_size: int = 20, polyorder: int = 3) -> np.ndarray:\n    \"\"\"\n    Apply SciPy's Savitzky\u2011Golay filter with safe parameter handling.\n    If SciPy cannot be imported or the parameters are invalid, fall back to\n    the Gaussian\u2011weighted enhanced filter.\n    \"\"\"\n    arr = _prepare_signal(x, window_size)\n    if arr.size == 0:\n        return arr\n    try:\n        from scipy import signal\n\n        w = int(window_size)\n        if w % 2 == 0:\n            w += 1\n        w = max(w, polyorder + 2)\n        if w % 2 == 0:\n            w += 1\n        # Ensure the window does not exceed the signal length\n        w = min(w, arr.size if arr.size % 2 else arr.size - 1)\n        return signal.savgol_filter(arr, window_length=w, polyorder=polyorder, mode=\"interp\")\n    except Exception:\n        # Graceful fallback\n        return enhanced_filter_with_trend_preservation(arr, window_size)\n\n\ndef hybrid_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"\n    Two\u2011stage filter: first a Gaussian\u2011weighted MA, then a simple MA.\n    The second convolution further reduces the length, mirroring the\n    behaviour of the original implementation.\n    \"\"\"\n    first_pass = enhanced_filter_with_trend_preservation(x, window_size)\n    if first_pass.size == 0:\n        return first_pass\n    ws = _validate_window(window_size)\n    return np.convolve(first_pass, np.ones(ws) / ws, mode=\"valid\")\n\n\ndef _dispatch_map() -> Dict[str, Callable]:\n    \"\"\"Mapping from algorithm name to the corresponding function.\"\"\"\n    return {\n        \"basic\": adaptive_filter,\n        \"enhanced\": enhanced_filter_with_trend_preservation,\n        \"savgol\": _savgol_filter,\n        \"hybrid\": hybrid_filter,\n    }\n\n\ndef process_signal(\n    input_signal,\n    window_size: int = 20,\n    algorithm_type: str = \"enhanced\",\n) -> np.ndarray:\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Parameters\n    ----------\n    input_signal : array\u2011like\n        Raw time\u2011series data.\n    window_size : int, optional\n        Length of the sliding window (default 20).\n    algorithm_type : str, optional\n        One of ``\"basic\"``, ``\"enhanced\"``, ``\"savgol\"``, ``\"hybrid\"``.\n        Case\u2011insensitive; defaults to ``\"enhanced\"``.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal; may be empty if the input is shorter than the window.\n    \"\"\"\n    alg = algorithm_type.lower()\n    func = _dispatch_map().get(alg)\n    if func is None:\n        raise ValueError(\n            f\"Unsupported algorithm_type '{algorithm_type}'. \"\n            \"Supported types: basic, enhanced, savgol, hybrid.\"\n        )\n    return func(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_input(x, window_size):\n    \"\"\"Validate ``window_size`` and convert ``x`` to a 1\u2011D float array.\n\n    Returns (arr, ws). If ``arr`` is shorter than ``ws`` an empty array is\n    returned so callers can handle the degenerate case without raising.\n    \"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    ws = int(window_size)\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    \"\"\"Cached exponential weighting vector (right\u2011biased).\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef _moving_average(arr, ws):\n    \"\"\"Fast simple moving average using cumulative sum.\"\"\"\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[ws:] - csum[:-ws]) / ws\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    return _moving_average(arr, ws)\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (enhanced).\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    w = _exp_weights(ws)[::-1]  # reverse for np.convolve semantics\n    return np.convolve(arr, w, mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    \"\"\"Recursive exponential smoothing; output length matches other filters.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average (default)\n        - \"smooth\"   : recursive exponential smoothing\n    Any unknown value falls back to the enhanced filter.\n    \"\"\"\n    alg = algorithm_type.lower()\n    if alg == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    if alg == \"smooth\":\n        return exponential_smoothing(sig, window_size)\n    # Fallback for unsupported types\n    return enhanced_filter(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\n\ndef _prepare_input(x, window_size):\n    \"\"\"Validate ``window_size`` and convert ``x`` to a 1\u2011D float array.\n\n    Returns (arr, ws). If ``arr`` is shorter than ``ws`` an empty array is\n    returned so callers can handle the degenerate case without raising.\n    \"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    ws = int(window_size)\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    \"\"\"Cached exponential weighting vector (right\u2011biased).\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef _moving_average(arr, ws):\n    \"\"\"Fast simple moving average using cumulative sum.\"\"\"\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[ws:] - csum[:-ws]) / ws\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    return _moving_average(arr, ws)\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (enhanced).\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    w = _exp_weights(ws)[::-1]  # reverse for np.convolve semantics\n    return np.convolve(arr, w, mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    \"\"\"Recursive exponential smoothing; output length matches other filters.\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(arr)\n    y[0] = arr[0]\n    for i in range(1, arr.size):\n        y[i] = alpha * arr[i] + (1.0 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _savgol_filter(x, window_size, polyorder=3):\n    \"\"\"Savitzky\u2011Golay filter with safe fallback to ``enhanced_filter``.\"\"\"\n    try:\n        from scipy.signal import savgol_filter\n        w = int(window_size)\n        if w % 2 == 0:\n            w += 1\n        w = max(w, polyorder + 2)\n        return savgol_filter(x, window_length=w, polyorder=polyorder, mode=\"interp\")\n    except Exception:\n        return enhanced_filter(x, window_size)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Apply ``enhanced_filter`` then a simple moving average for extra smoothing.\"\"\"\n    first = enhanced_filter(x, window_size)\n    if first.size == 0:\n        return first\n    arr, ws = _prepare_input(first, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    return _moving_average(arr, ws)\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original - filtered) aligned to the shortest length.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef _select_best_filter(sig, window_size):\n    \"\"\"Choose the filter yielding the smallest residual variance.\"\"\"\n    candidates = {\n        \"adaptive\": adaptive_filter(sig, window_size),\n        \"enhanced\": enhanced_filter(sig, window_size),\n        \"smooth\": exponential_smoothing(sig, window_size),\n        \"savgol\": _savgol_filter(sig, window_size),\n        \"hybrid\": _hybrid_filter(sig, window_size),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(sig, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" : simple moving average\n        - \"enhanced\" : exponential\u2011weighted moving average\n        - \"smooth\"   : recursive exponential smoothing\n        - \"savgol\"   : Savitzky\u2011Golay (fallback to enhanced)\n        - \"hybrid\"   : enhanced followed by simple moving average\n        - \"auto\" or any unknown value : automatically pick the best filter\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    if alg == \"smooth\":\n        return exponential_smoothing(sig, window_size)\n    if alg == \"savgol\":\n        return _savgol_filter(sig, window_size)\n    if alg == \"hybrid\":\n        return _hybrid_filter(sig, window_size)\n    # \"auto\" or any unsupported type falls back to automatic selection\n    return _select_best_filter(sig, window_size)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}