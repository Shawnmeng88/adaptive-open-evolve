{"id": "c6f77790-4d8e-4d5a-b606-b7b54a7d6989", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import Callable\n\ndef _to_pos_int(v, name):\n    iv = int(v)\n    if iv <= 0:\n        raise ValueError(f\"{name} must be positive\")\n    return iv\n\ndef _prepare(x, w):\n    w = _to_pos_int(w, \"window_size\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp(w):\n    w = int(w)\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\n@lru_cache(maxsize=None)\ndef _gauss(w):\n    w = int(w)\n    mid = (w - 1) / 2.0\n    sigma = max(w / 6.0, 1e-8)\n    idx = np.arange(w)\n    g = np.exp(-0.5 * ((idx - mid) / sigma) ** 2)\n    return g / g.sum()\n\ndef adaptive(x, w=20):\n    a, w = _prepare(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, \"valid\")\n\ndef enhanced(x, w=20):\n    a, w = _prepare(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp(w)[::-1], \"valid\")\n\ndef gaussian(x, w=20):\n    a, w = _prepare(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _gauss(w)[::-1], \"valid\")\n\ndef median(x, w=21):\n    w = _to_pos_int(w, \"window_size\")\n    if w % 2 == 0:\n        w += 1\n    a, _ = _prepare(x, w)\n    if a.size == 0:\n        return a\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    win = np.lib.stride_tricks.as_strided(a, shape, strides)\n    return np.median(win, axis=1)\n\ndef robust(x, w=20):\n    return enhanced(median(x, w), w)\n\ndef hybrid(x, w=20):\n    return enhanced(adaptive(x, w), w)\n\n_algos = {\n    \"adaptive\": adaptive,\n    \"enhanced\": enhanced,\n    \"gaussian\": gaussian,\n    \"median\": median,\n    \"robust\": robust,\n    \"hybrid\": hybrid,\n}\n\ndef _auto(x):\n    v = np.var(np.asarray(x, float))\n    return median if v > 2 else gaussian if v > 0.5 else enhanced\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    if algorithm_type is None:\n        algorithm_type = \"enhanced\"\n    key = str(algorithm_type).lower()\n    if key == \"auto\":\n        func = _auto(sig)\n    else:\n        func = _algos.get(key, enhanced)\n    return func(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "ef6b0ac7-b430-4557-b6b9-61ef8c30a81a", "generation": 7, "timestamp": 1764942811.2327628, "iteration_found": 771, "metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.0005272865295410156, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.42000321782831856, "output_length": 91.0, "overall_score": 0.38987798748202884, "slope_changes": 66.0, "lag_error": 0.8696516134600849, "avg_error": 1.241821020768923, "false_reversals": 55.2, "correlation": 0.3768253622186232, "noise_reduction": 0.0, "smoothness_score": 0.23255813953488372, "responsiveness_score": 0.5348590041057663, "accuracy_score": 0.3768253622186232, "efficiency_score": 1.0, "execution_time": 0.0002841472625732422, "success_rate": 1.0}, "island": 0}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\n**System Prompt \u2013\u202fEvolve Block Guidance**\n\nYou are to modify only the code that resides between the markers `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`. **Do not output the markers themselves** and **do not include any code outside that region** (imports, `run_packing()`, etc.). The system will automatically splice your output into the preserved surrounding code.\n\n### Core Task\nImprove the `construct_packing()` function and any helper functions defined inside the evolve block so that they correctly build a feasible packing plan for the given items and containers.\n\n### Mandatory Output Rules (must be obeyed)\n- **Output ONLY the code** that should replace the existing contents between the markers.  \n- **Do NOT** include the `# EVOLVE-BLOCK-START` / `# EVOLVE-BLOCK-END` lines.  \n- **Do NOT** add, remove, or modify any code outside the evolve block.  \n\nViolating these rules will cause the program to be rejected.\n\n### What Must NOT Be Done (avoid these failures)\n- **Do not** write overly aggressive or speculative code that ignores the existing data structures (e.g., redefining `Item` or `Container`).  \n- **Do not** produce code that is syntactically invalid, contains undefined names, or relies on external libraries not already imported.  \n- **Do not** create deep, uncontrolled recursion that can cause a stack overflow for moderate\u2011size inputs.  \n- **Do not** leave placeholder `pass` statements or `TODO` comments; every path must be executable.  \n- **Do not** change the function signatures of `construct_packing` or its helpers; keep the public API identical.  \n\n### Concrete Strategies to Try (use one or combine several)\n\n1. **Greedy First\u2011Fit with Backtracking**  \n   - Sort items by decreasing volume (or weight) to place the largest items first.  \n   - Iterate over containers, trying to fit the current item using a helper `can_place(item, container, state)`.  \n   - If an item cannot be placed in any container, backtrack: remove the last placed item and try the next container option.  \n   - Limit backtracking depth by aborting early when the remaining free volume across all containers is insufficient for the remaining items.\n\n2. **State\u2011Tracking Helper Functions**  \n   - `can_place(item, container, state) \u2192 bool` \u2013 checks volume/weight limits and any custom constraints.  \n   - `add_item(item, container, state) \u2192 None` \u2013 updates the mutable `state` (e.g., `state['used_vol'][cid]`).  \n   - `remove_item(item, container, state) \u2192 None` \u2013 reverts the changes made by `add_item`.  \n   Using these helpers keeps `construct_packing` readable and isolates side\u2011effects.\n\n3. **Iterative Packing Loop with Early Exit**  \n   - Use a `while unplaced_items:` loop rather than recursion.  \n   - Inside the loop, attempt to place the next item; if no container can accommodate it, break and return the best\u2011so\u2011far packing (or raise a clear \u201cNo feasible packing\u201d exception).  \n   - This avoids recursion limits and makes debugging easier.\n\n4. **Memoization of Failed Sub\u2011states**  \n   - Store a frozenset of `(item_ids, container_usage)` that has already been proven unsolvable.  \n   - Before trying a placement, check the memo; if the state is cached, skip the expensive trial.  \n   - This dramatically reduces duplicate work for inputs with many similar items.\n\n5. **Robust Edge\u2011Case Handling**  \n   - If `items` is empty, return an empty packing list immediately.  \n   - If any single item exceeds *all* container capacities, raise a descriptive `ValueError`.  \n   - Validate that container capacities are positive numbers before the main loop.\n\n6. **Clear Documentation & Type Hints**  \n   - Add a concise docstring to `construct_packing` describing the algorithm, its inputs, and its return value.  \n   - Use Python type hints for all parameters and return types to help the evaluator catch mismatches early.\n\n### Example Skeleton (you may adapt and flesh out)\n\n```python\ndef construct_packing(items: List[Item], containers: List[Container]) -> PackingResult:\n    \"\"\"\n    Build a feasible packing using a greedy first\u2011fit with limited backtracking.\n    Returns a PackingResult (or raises ValueError if impossible).\n    \"\"\"\n    # 1\ufe0f\u20e3 Sort items by decreasing volume (or weight)\n    sorted_items = sorted(items, key=lambda i: i.volume, reverse=True)\n\n    # 2\ufe0f\u20e3 Initialise mutable state tracking used volume/weight per container\n    state = {\n        \"used_vol\": {c.id: 0 for c in containers},\n        \"used_wt\":  {c.id: 0 for c in containers},\n        \"placements\": [],                     # list of (item.id, container.id)\n    }\n\n    # 3\ufe0f\u20e3 Helper memo for failed states\n    failed_states: Set[frozenset] = set()\n\n    # 4\ufe0f\u20e3 Main packing loop (iterative with explicit backtrack stack)\n    stack: List[Tuple[int, List[Tuple[int, int]]]] = [(0, [])]  # (next_item_index, current_placements)\n\n    while stack:\n        idx, placements = stack.pop()\n        if idx == len(sorted_items):\n            # All items placed \u2013 fill result and return\n            state[\"placements\"] = placements\n            return build_result_from_state(state, containers)\n\n        item = sorted_items[idx]\n        placed = False\n\n        for container in containers:\n            if can_place(item, container, state):\n                # Record placement\n                add_item(item, container, state)\n                new_placements = placements + [(item.id, container.id)]\n\n                # Encode current sub\u2011state for memoization\n                sub_state_key = frozenset(new_placements)\n                if sub_state_key not in failed_states:\n                    stack.append((idx + 1, new_placements))\n                else:\n                    # Previously proven dead\u2011end \u2013 skip\n                    remove_item(item, container, state)\n                    continue\n\n                placed = True\n                # Do NOT break here; we push all viable branches onto the stack\n                # to allow limited backtracking later.\n                remove_item(item, container, state)\n\n        if not placed:\n            # No container could accept this item \u2013 mark this sub\u2011state as dead\n            dead_key = frozenset(placements)\n            failed_states.add(dead_key)\n\n    # If we exit the loop, no feasible packing was found\n    raise ValueError(\"No feasible packing exists for the given items and containers.\")\n```\n\nYou may restructure the above, add missing helper implementations, and adjust the algorithm (e.g., switch to pure recursion if you prefer) as long as you respect the rules above.\n\n**Remember:** Only output the final, syntactically correct Python code that should replace the content between the markers. No extra text, no markers, no surrounding code. Good luck!", "user": "# Current Program Information\n- Fitness: 13.7277\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 15.2834 \u2192 13.7277. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4001, output_length: 91.0000, overall_score: 0.3625, slope_changes: 80.4000, lag_error: 0.9635, avg_error: 1.2731, false_reversals: 65.8000, correlation: 0.3131, noise_reduction: 0.0000, smoothness_score: 0.1992, responsiveness_score: 0.5093, accuracy_score: 0.3131, efficiency_score: 1.0000, execution_time: 0.0008, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4001, output_length: 91.0000, overall_score: 0.3625, slope_changes: 80.4000, lag_error: 0.9635, avg_error: 1.2731, false_reversals: 65.8000, correlation: 0.3131, noise_reduction: 0.0000, smoothness_score: 0.1992, responsiveness_score: 0.5093, accuracy_score: 0.3131, efficiency_score: 1.0000, execution_time: 0.0009, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4026, output_length: 91.0000, overall_score: 0.3643, slope_changes: 81.0000, lag_error: 0.8922, avg_error: 1.1866, false_reversals: 66.8000, correlation: 0.3180, noise_reduction: 0.0000, smoothness_score: 0.1980, responsiveness_score: 0.5285, accuracy_score: 0.3180, efficiency_score: 1.0000, execution_time: 0.0009, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 15.3756)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _combined_filter(x, ws):\n    \"\"\"Inverse\u2011variance weighted blend of several filters.\"\"\"\n    cand = {\n        \"basic\": adaptive_filter(x, ws),\n        \"weighted\": weighted_filter(x, ws),\n        \"ema\": ema_filter(x, ws),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, ws),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0, dtype=float)\n    vars_ = {k: _residual_variance(x, v) for k, v in cand.items()}\n    for k, v in vars_.items():\n        if v == 0.0:\n            return cand[k][: len(x) - ws + 1]\n    keys = sorted(vars_)\n    inv = np.array([1.0 / vars_[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    combined = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return combined\n\ndef _select_best_filter(x, window_size):\n    \"\"\"Select the best filter via inverse\u2011variance blend.\"\"\"\n    return _combined_filter(x, window_size)\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return _combined_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4026), Performs well on output_length (91.0000), Performs well on overall_score (0.3643), Performs well on slope_changes (81.0000), Performs well on lag_error (0.8922), Performs well on avg_error (1.1866), Performs well on false_reversals (66.8000), Performs well on correlation (0.3180), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.1980), Performs well on responsiveness_score (0.5285), Performs well on accuracy_score (0.3180), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0009), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 15.2834)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _to_array(x):\n    \"\"\"1\u2011D float array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    w = int(w)\n    wts = np.exp(np.linspace(-2, 0, w))\n    return wts / wts.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _residual_variance(orig, filt):\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n])) if n else float(\"inf\")\n\ndef _combined_filter(x, window_size=20):\n    \"\"\"Inverse\u2011variance weighted blend of adaptive, weighted, and EMA filters.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n\n    cand = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0, dtype=float)\n\n    vari = {k: _residual_variance(a, v) for k, v in cand.items()}\n    # perfect match shortcut\n    for k, v in vari.items():\n        if v == 0.0:\n            return cand[k][: len(a) - w + 1]\n\n    keys = sorted(vari)\n    inv = np.array([1.0 / vari[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    blended = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return blended\n\ndef _auto_select(x, window_size=20):\n    a = _to_array(x)\n    w = int(window_size)\n    cand = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"combined\": _combined_filter(a, w),\n    }\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0, dtype=float)\n    best = min(cand, key=lambda k: _residual_variance(a, cand[k]))\n    return cand[best]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm.\"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"enhanced\", \"hybrid\"):\n        return _combined_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _auto_select(input_signal, window_size)\n    return _combined_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4001), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (80.4000), Performs well on lag_error (0.9635), Performs well on avg_error (1.2731), Performs well on false_reversals (65.8000), Performs well on correlation (0.3131), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.1992), Performs well on responsiveness_score (0.5093), Performs well on accuracy_score (0.3131), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0009), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 15.2834)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import List\n\ndef _prep(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _expw(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef weighted_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _expw(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(a)\n    y[0] = a[0]\n    for i in range(1, a.size):\n        y[i] = alpha * a[i] + (1 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _resvar(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    d = orig[:m] - filt[:m]\n    return float(np.mean(d * d))\n\ndef _combined(x, ws):\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n    cand = {\n        \"basic\": adaptive_filter(a, ws),\n        \"weighted\": weighted_filter(a, ws),\n        \"exp\": exponential_smoothing(a, ws),\n    }\n    vars = {k: _resvar(a, v) for k, v in cand.items() if v.size}\n    if not vars:\n        return np.empty(0, dtype=float)\n    for k, v in vars.items():\n        if v == 0.0:\n            return cand[k][:len(a) - ws + 1]\n    keys = sorted(vars)\n    inv = np.array([1.0 / vars[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    combined = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return combined\n\ndef _best(x, ws):\n    return _combined(x, ws)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    alg = str(algorithm_type).lower()\n    if alg == \"basic\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return _combined(sig, window_size)\n    if alg == \"exp\":\n        return exponential_smoothing(sig, window_size)\n    return _best(sig, window_size)\n\ndef construct_packing(items: List[int], capacity: int) -> List[List[int]]:\n    \"\"\"\n    Best\u2011Fit Decreasing bin\u2011packing.\n\n    Returns a list of bins, each bin being a list of original item indices.\n    \"\"\"\n    if capacity <= 0:\n        raise ValueError(\"capacity must be a positive integer\")\n    if not items:\n        return []\n\n    indexed = []\n    for idx, size in enumerate(items):\n        if not isinstance(size, (int, float)):\n            raise TypeError(\"item sizes must be numeric\")\n        if size < 0:\n            raise ValueError(\"item sizes must be non\u2011negative\")\n        indexed.append((idx, float(size)))\n\n    indexed.sort(key=lambda x: x[1], reverse=True)\n\n    bins: List[List[int]] = []\n    remaining: List[float] = []\n\n    for idx, size in indexed:\n        if size > capacity:\n            bins.append([idx])\n            remaining.append(0.0)\n            continue\n\n        best_bin = -1\n        best_rem = float(\"inf\")\n        for b_idx, cap_left in enumerate(remaining):\n            if size <= cap_left and cap_left - size < best_rem:\n                best_bin = b_idx\n                best_rem = cap_left - size\n\n        if best_bin >= 0:\n            bins[best_bin].append(idx)\n            remaining[best_bin] -= size\n        else:\n            bins.append([idx])\n            remaining.append(capacity - size)\n\n    for b in bins:\n        b.sort()\n    return bins\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4001), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (80.4000), Performs well on lag_error (0.9635), Performs well on avg_error (1.2731), Performs well on false_reversals (65.8000), Performs well on correlation (0.3131), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.1992), Performs well on responsiveness_score (0.5093), Performs well on accuracy_score (0.3131), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0008), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 15.0527)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prep(x, ws):\n    \"\"\"Validate and prepare input array.\"\"\"\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _expw(ws):\n    \"\"\"Exponential weights that sum to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (uniform weights).\"\"\"\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving\u2011average filter.\"\"\"\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _expw(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    \"\"\"Classic EMA (causal).\"\"\"\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(a)\n    y[0] = a[0]\n    for i in range(1, a.size):\n        y[i] = alpha * a[i] + (1 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _median_filter(x, ws):\n    \"\"\"Median filter using stride tricks (odd window enforced).\"\"\"\n    if ws % 2 == 0:\n        ws += 1\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n    shape = (a.size - ws + 1, ws)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef _resvar(orig, filt):\n    \"\"\"Mean squared error between two equal\u2011length signals.\"\"\"\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    d = orig[:m] - filt[:m]\n    return float(np.mean(d * d))\n\ndef _combined(x, ws):\n    \"\"\"Inverse\u2011variance weighted blend of several candidate filters.\"\"\"\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n\n    # generate candidate filters\n    cand = {\n        \"adaptive\": adaptive_filter(a, ws),\n        \"weighted\": weighted_filter(a, ws),\n        \"exp\": exponential_smoothing(a, ws),\n        \"median\": _median_filter(a, ws),\n    }\n\n    # keep only non\u2011empty results\n    cand = {k: v for k, v in cand.items() if v.size}\n    if not cand:\n        return np.empty(0, dtype=float)\n\n    # compute residual variances\n    vars_ = {k: _resvar(a, v) for k, v in cand.items()}\n\n    # perfect match shortcut\n    for k, v in vars_.items():\n        if v == 0.0:\n            return cand[k][: len(a) - ws + 1]\n\n    # inverse\u2011variance weighting (deterministic order)\n    keys = sorted(vars_)\n    inv = np.array([1.0 / vars_[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    blended = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return blended\n\ndef _best(x, ws):\n    \"\"\"Return the single best filter (lowest residual variance).\"\"\"\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n\n    candidates = {\n        \"adaptive\": adaptive_filter(a, ws),\n        \"weighted\": weighted_filter(a, ws),\n        \"exp\": exponential_smoothing(a, ws),\n        \"median\": _median_filter(a, ws),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n\n    # select by minimal residual variance\n    best_key = min(candidates, key=lambda k: _resvar(a, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the requested filtering algorithm.\"\"\"\n    alg = str(algorithm_type).lower()\n    if alg == \"basic\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(sig, window_size)\n    if alg == \"exp\":\n        return exponential_smoothing(sig, window_size)\n    if alg == \"median\":\n        return _median_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return _combined(sig, window_size)\n    if alg == \"auto\":\n        return _best(sig, window_size)\n    # fallback to the blended enhanced filter\n    return _combined(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 15.2834)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prep(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _expw(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef weighted_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _expw(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(a)\n    y[0] = a[0]\n    for i in range(1, a.size):\n        y[i] = alpha * a[i] + (1 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _resvar(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    d = orig[:m] - filt[:m]\n    return float(np.mean(d * d))\n\ndef _combined(x, ws):\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n    # generate candidate filters\n    cand = {\n        \"basic\": adaptive_filter(a, ws),\n        \"weighted\": weighted_filter(a, ws),\n        \"exp\": exponential_smoothing(a, ws),\n    }\n    # compute residual variances, keep only non\u2011empty results\n    vars = {k: _resvar(a, v) for k, v in cand.items() if v.size}\n    if not vars:\n        return np.empty(0, dtype=float)\n    # perfect match shortcut\n    for k, v in vars.items():\n        if v == 0.0:\n            return cand[k][:len(a) - ws + 1]\n    # inverse\u2011variance weighting (deterministic order)\n    keys = sorted(vars)\n    inv = np.array([1.0 / vars[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    combined = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return combined\n\ndef _best(x, ws):\n    return _combined(x, ws)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    alg = str(algorithm_type).lower()\n    if alg == \"basic\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return _combined(sig, window_size)\n    if alg == \"exp\":\n        return exponential_smoothing(sig, window_size)\n    return _best(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import Tuple, Union\n\ndef _prepare_signal(\n    x: Union[np.ndarray, list, tuple],\n    window_size: int,\n    *,\n    require_odd: bool = False,\n) -> Tuple[np.ndarray, int]:\n    \"\"\"Validate input and convert to a 1\u2011D float array.\n\n    Parameters\n    ----------\n    x : array\u2011like\n        Input signal.\n    window_size : int\n        Desired sliding\u2011window length (must be positive).\n    require_odd : bool, optional\n        If True, enforce an odd window size (increment by one if even).\n\n    Returns\n    -------\n    tuple[np.ndarray, int]\n        Normalised signal array and (possibly adjusted) window size.\n\n    Raises\n    ------\n    ValueError\n        If validation fails.\n    \"\"\"\n    try:\n        w = int(window_size)\n    except Exception as exc:\n        raise ValueError(f\"window_size must be an integer, got {window_size!r}\") from exc\n    if w <= 0:\n        raise ValueError(f\"window_size must be positive, got {w}\")\n\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input signal must be one\u2011dimensional\")\n    if require_odd and w % 2 == 0:\n        w += 1\n    if arr.size < w:\n        raise ValueError(f\"len({arr.size}) < window_size ({w})\")\n    return arr, w\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(n: int) -> np.ndarray:\n    \"\"\"Cached exponential weighting vector (right\u2011biased).\"\"\"\n    w = np.exp(np.linspace(-2, 0, n))\n    return w / w.sum()\n\n\ndef adaptive_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Simple moving\u2011average filter (uniform weights).\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if w == 1:\n        return a.copy()\n    kernel = np.ones(w, dtype=float) / w\n    return np.convolve(a, kernel, mode=\"valid\")\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Exponential\u2011weighted moving average that emphasises recent samples.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if w == 1:\n        return a.copy()\n    weights = _exp_weights(w)[::-1]  # reverse for right\u2011bias in convolution\n    return np.convolve(a, weights, mode=\"valid\")\n\n\ndef median_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Median filter (odd window enforced).\"\"\"\n    a, w = _prepare_signal(x, window_size, require_odd=True)\n    if w == 1:\n        return a.copy()\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef hybrid_filter(x, window_size: int = 20) -> np.ndarray:\n    \"\"\"Adaptive filter followed by the exponential\u2011weighted filter.\"\"\"\n    first_pass = adaptive_filter(x, window_size)\n    return enhanced_filter_with_trend_preservation(first_pass, window_size)\n\n\ndef _auto_select(x, window_size: int) -> np.ndarray:\n    \"\"\"Choose a filter based on signal variance.\n\n    - High variance \u2192 median filter (robust to outliers).\n    - Otherwise \u2192 enhanced exponential\u2011weighted filter.\n    \"\"\"\n    a, _ = _prepare_signal(x, window_size)\n    var = np.var(a)\n    # Threshold chosen empirically; keeps deterministic behaviour.\n    if var > 0.5:\n        return median_filter(a, window_size)\n    return enhanced_filter_with_trend_preservation(a, window_size)\n\n\ndef process_signal(\n    sig,\n    window_size: int = 20,\n    algorithm_type: str = \"enhanced\",\n) -> np.ndarray:\n    \"\"\"Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\"\n        - \"enhanced\"\n        - \"median\"\n        - \"hybrid\"\n        - \"auto\"      (variance\u2011based automatic selection)\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    window_size : int, optional\n        Sliding\u2011window length.\n    algorithm_type : str, optional\n        Identifier of the algorithm to use.\n\n    Returns\n    -------\n    np.ndarray\n        Filtered signal.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported ``algorithm_type`` is supplied.\n    \"\"\"\n    algo = algorithm_type.lower()\n    if algo == \"adaptive\":\n        return adaptive_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    if algo == \"auto\":\n        return _auto_select(sig, window_size)\n    raise ValueError(f\"Unsupported algorithm_type: {algorithm_type}\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Filter `sig` using one of three algorithms:\n    - \"enhanced\": exponential\u2011weighted moving average\n    - \"adaptive\": simple moving average\n    - \"median\": median filter (odd window)\n    \"\"\"\n    a = np.asarray(sig, float)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        raise ValueError(\"signal shorter than window size\")\n    alg = alg.lower()\n    if alg.startswith(\"a\"):  # adaptive\n        return np.convolve(a, np.ones(w) / w, \"valid\")\n    if alg.startswith(\"m\"):  # median\n        if w % 2 == 0:\n            w += 1\n        s = a.strides[0]\n        shape = (a.size - w + 1, w)\n        win = np.lib.stride_tricks.as_strided(a, shape, (s, s))\n        return np.median(win, axis=1)\n    # enhanced (default)\n    wv = np.exp(np.linspace(-2, 0, w))\n    wv /= wv.sum()\n    return np.convolve(a, wv[::-1], \"valid\")\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 14.0152, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _arr(x):\n    return np.asarray(x, float).ravel()\n\ndef _ok(a, w):\n    return w > 0 and a.size >= w\n\ndef _simple(x, w):\n    a = _arr(x)\n    return np.convolve(a, np.ones(w) / w, \"valid\") if _ok(a, w) else np.empty(0)\n\ndef _weighted(x, w):\n    a = _arr(x)\n    if not _ok(a, w):\n        return np.empty(0)\n    wts = np.exp(np.linspace(-2, 0, w))\n    wts /= wts.sum()\n    return np.convolve(a, wts, \"valid\")\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    k = str(alg).lower()\n    return _weighted(sig, w) if k == \"weighted\" else _simple(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n\ndef _to_int(value, name):\n    \"\"\"Convert *value* to a positive int, raising a clear ValueError on failure.\"\"\"\n    try:\n        iv = int(value)\n    except Exception as exc:\n        raise ValueError(f\"{name} must be an integer, got {value!r}\") from exc\n    if iv <= 0:\n        raise ValueError(f\"{name} must be positive, got {iv}\")\n    return iv\n\n\ndef _validate_input(x, window_size):\n    \"\"\"Return a 1\u2011D float array and a validated window size.\"\"\"\n    w = _to_int(window_size, \"window_size\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < w:\n        raise ValueError(f\"Input length ({arr.size}) < window_size ({w})\")\n    return arr, w\n\n\n@lru_cache(maxsize=None)\ndef _exp_weights(w):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w_arr = np.exp(np.linspace(-2, 0, w))\n    return w_arr / w_arr.sum()\n\n\n@lru_cache(maxsize=None)\ndef _gauss_weights(w):\n    \"\"\"Cached zero\u2011phase Gaussian kernel (symmetric, sums to 1).\"\"\"\n    mid = (w - 1) / 2.0\n    sigma = max(w / 6.0, 1e-8)\n    idx = np.arange(w)\n    g = np.exp(-0.5 * ((idx - mid) / sigma) ** 2)\n    return g / g.sum()\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average (causal).\"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (right\u2011biased).\"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    # reverse to give more weight to recent samples\n    return np.convolve(a, _exp_weights(w)[::-1], mode=\"valid\")\n\n\ndef gaussian_filter(x, window_size=20):\n    \"\"\"Zero\u2011phase Gaussian weighted moving average.\"\"\"\n    a, w = _validate_input(x, window_size)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _gauss_weights(w)[::-1], mode=\"valid\")\n\n\ndef median_filter(x, window_size=21):\n    \"\"\"Median filter using stride tricks (odd window).\"\"\"\n    w = _to_int(window_size, \"window_size\")\n    if w % 2 == 0:\n        w += 1\n    a, _ = _validate_input(x, w)\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\n\ndef robust_filter(x, window_size=20):\n    \"\"\"\n    Median filter (outlier suppression) followed by exponential\u2011weighted smoothing.\n    Provides a good trade\u2011off between detail preservation and noise reduction.\n    \"\"\"\n    # median may change effective length; we keep the same window for the second stage\n    med = median_filter(x, window_size)\n    return enhanced_filter(med, window_size)\n\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Adaptive filter followed by exponential\u2011weighted filter.\"\"\"\n    return enhanced_filter(adaptive_filter(x, window_size), window_size)\n\n\n_algorithms = {\n    \"adaptive\": adaptive_filter,\n    \"enhanced\": enhanced_filter,\n    \"gaussian\": gaussian_filter,\n    \"median\": median_filter,\n    \"hybrid\": hybrid_filter,\n    \"robust\": robust_filter,\n}\n\n\ndef _auto_select(x):\n    \"\"\"Select a filter based on signal variance.\"\"\"\n    arr = np.asarray(x, dtype=float).ravel()\n    var = np.var(arr)\n    if var > 2.0:\n        return median_filter\n    if var > 0.5:\n        return gaussian_filter\n    return enhanced_filter\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\"\n        - \"enhanced\"\n        - \"gaussian\"\n        - \"median\"\n        - \"hybrid\"\n        - \"robust\"\n        - \"auto\"   \u2013 chooses based on signal variance\n    Unknown values fall back to the enhanced filter for robustness.\n    \"\"\"\n    if algorithm_type is None:\n        algorithm_type = \"enhanced\"\n    key = algorithm_type.lower()\n    if key == \"auto\":\n        algo = _auto_select(input_signal)\n    else:\n        algo = _algorithms.get(key, enhanced_filter)  # safe fallback\n    return algo(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\nfrom typing import Callable\n\ndef _to_pos_int(v, name):\n    iv = int(v)\n    if iv <= 0:\n        raise ValueError(f\"{name} must be positive\")\n    return iv\n\ndef _prepare(x, w):\n    w = _to_pos_int(w, \"window_size\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\n@lru_cache(maxsize=None)\ndef _exp(w):\n    w = int(w)\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\n@lru_cache(maxsize=None)\ndef _gauss(w):\n    w = int(w)\n    mid = (w - 1) / 2.0\n    sigma = max(w / 6.0, 1e-8)\n    idx = np.arange(w)\n    g = np.exp(-0.5 * ((idx - mid) / sigma) ** 2)\n    return g / g.sum()\n\ndef adaptive(x, w=20):\n    a, w = _prepare(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, \"valid\")\n\ndef enhanced(x, w=20):\n    a, w = _prepare(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp(w)[::-1], \"valid\")\n\ndef gaussian(x, w=20):\n    a, w = _prepare(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _gauss(w)[::-1], \"valid\")\n\ndef median(x, w=21):\n    w = _to_pos_int(w, \"window_size\")\n    if w % 2 == 0:\n        w += 1\n    a, _ = _prepare(x, w)\n    if a.size == 0:\n        return a\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    win = np.lib.stride_tricks.as_strided(a, shape, strides)\n    return np.median(win, axis=1)\n\ndef robust(x, w=20):\n    return enhanced(median(x, w), w)\n\ndef hybrid(x, w=20):\n    return enhanced(adaptive(x, w), w)\n\n_algos = {\n    \"adaptive\": adaptive,\n    \"enhanced\": enhanced,\n    \"gaussian\": gaussian,\n    \"median\": median,\n    \"robust\": robust,\n    \"hybrid\": hybrid,\n}\n\ndef _auto(x):\n    v = np.var(np.asarray(x, float))\n    return median if v > 2 else gaussian if v > 0.5 else enhanced\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    if algorithm_type is None:\n        algorithm_type = \"enhanced\"\n    key = str(algorithm_type).lower()\n    if key == \"auto\":\n        func = _auto(sig)\n    else:\n        func = _algos.get(key, enhanced)\n    return func(sig, window_size)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}