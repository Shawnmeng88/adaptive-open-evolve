{"id": "2ceb4587-bd5c-4f70-b43f-9fab007ec69a", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_signal(x, w):\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n    a = np.asarray(x, dtype=float).ravel()\n    w = int(w)\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef _moving_average(a, w):\n    if w == 1:\n        return a.copy()\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    return _moving_average(a, w)\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, a.size):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\ndef median_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w % 2 == 0:\n        w += 1\n    if w == 1:\n        return a.copy()\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    first = adaptive_filter(x, window_size)\n    return weighted_filter(first, window_size)\n\ndef combined_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    adv = _moving_average(a, w)\n    wgt = np.convolve(a, _exp_weights(w), mode=\"valid\")\n    orig = a[w - 1 :]\n    var_adv = _residual_variance(orig, adv)\n    var_wgt = _residual_variance(orig, wgt)\n    if var_adv == 0 and var_wgt == 0:\n        return adv\n    inv_adv = 1.0 / var_adv if var_adv > 0 else 0.0\n    inv_wgt = 1.0 / var_wgt if var_wgt > 0 else 0.0\n    total = inv_adv + inv_wgt\n    if total == 0.0:\n        return (adv + wgt) / 2.0\n    return (inv_adv * adv + inv_wgt * wgt) / total\n\ndef enhanced_filter(x, window_size=20):\n    return combined_filter(x, window_size)\n\ndef _select_best_filter(x, window_size):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    cand = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"median\": median_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    best = min(cand, key=lambda k: _residual_variance(a, cand[k]))\n    return cand[best]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(sig, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(sig, window_size)\n    if alg == \"ema\":\n        return ema_filter(sig, window_size)\n    if alg == \"median\":\n        return median_filter(sig, window_size)\n    if alg == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    a, w = _prepare_signal(sig, window_size)\n    return _select_best_filter(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "71b8a626-96ee-4b48-a97c-5e04ba7c52fa", "generation": 9, "timestamp": 1764942989.924319, "iteration_found": 791, "metrics": {"runs_successfully": 1.0, "composite_score": 0.40531618708992745, "output_length": 91.0, "overall_score": 0.36615629786154524, "slope_changes": 75.4, "lag_error": 0.9943481482335418, "avg_error": 1.2929943724687032, "false_reversals": 63.0, "correlation": 0.31050550925785025, "noise_reduction": 0.0, "smoothness_score": 0.20964360587002095, "responsiveness_score": 0.5014169671858607, "accuracy_score": 0.31050550925785025, "efficiency_score": 1.0, "execution_time": 0.0006054878234863281, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.36247669701400953, "slope_changes": 68.6, "lag_error": 0.4099887786960039, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.2007631889073993, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.7092255024361451, "accuracy_score": 0.2007631889073993, "efficiency_score": 1.0, "execution_time": 0.00030922889709472656, "success_rate": 1.0}, "island": 1}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\n**System Message \u2013 Guidance for Evolving `construct_packing()`**\n\nYou are given a Python file that contains a special editable region delimited by the comments  \n\n```\n# EVOLVE-BLOCK-START\n... existing code ...\n# EVOLVE-BLOCK-END\n```  \n\nYour task is to **replace only the code that appears between those two markers** with a new implementation that improves the `construct_packing()` function (and any helper functions defined inside the block).  \n\n**Output Requirements (must be obeyed exactly)**  \n- Output **only** the Python code that belongs between `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END`.  \n- **Do not** include the marker comments themselves.  \n- **Do not** output any other code (imports, `run_packing()`, etc.).  \n- The surrounding file will be automatically re\u2011assembled by the evaluation harness.\n\n**What to Focus On**  \n1. **Correctness** \u2013 The new `construct_packing()` must return a packing structure that satisfies the problem\u2019s constraints (e.g., all items placed, no overlap, respects capacity limits).  \n2. **Clarity** \u2013 Use descriptive variable names, short helper functions, and inline comments that explain non\u2011trivial steps.  \n3. **Efficiency** \u2013 Prefer linear\u2011time or O(n\u202flog\u202fn) approaches; avoid nested loops that scale quadratically with the number of items unless absolutely necessary.  \n4. **Robust Edge\u2011Case Handling** \u2013 Guard against empty input, items that exactly fill a bin, and items that cannot be packed (return a clear indicator).  \n5. **Determinism** \u2013 Do not rely on random choices unless you also provide a deterministic fallback; the evaluator runs the code multiple times and expects consistent results.\n\n**Specific Strategies to Try**  \n- **Greedy Bin\u2011First Fit**: Sort items by descending size and place each into the first bin that has enough remaining capacity. This often yields a good baseline.  \n- **Two\u2011Pointer Pairing**: For 1\u2011dimensional bin packing, sort items and then pair the smallest with the largest that fits together; this reduces the number of bins.  \n- **Chunked Packing Helper**: Write a small helper `fit_items_into_bin(items, capacity)` that returns the subset of items that fit and the remaining items; call it iteratively to build the full packing.  \n- **Early Validation**: Before entering the main loop, compute the total size of all items and compare it to the total available capacity; if the total exceeds capacity, return an explicit \u201ccannot pack\u201d result.  \n- **Unit Tests Within the Block** *(optional but allowed)*: You may include a short `if __name__ == \"__main__\":` block that runs a few sanity checks. This code will be ignored by the outer harness because it lies outside the markers.\n\n**What Must Be Avoided (explicit prohibitions)**  \n- **Over\u2011aggressive prompting**: Do not write a vague \u201crewrite everything\u201d instruction. Be precise about the function to modify and the constraints to respect.  \n- **Ambiguous constraints**: Never assume hidden requirements; always state the assumptions you are making in comments.  \n- **Repeated invalid programs**: Do not submit code that fails to compile or raises syntax errors. Ensure the code is syntactically correct and all referenced names are defined within the block.  \n- **Unnecessary complexity**: Do not introduce heavyweight libraries (e.g., `numpy`, `pandas`) or deep recursion that could cause stack overflows.  \n- **Leaving placeholder `pass` statements**: Every function defined in the block must contain a working implementation.\n\n**Reminder**  \nYour entire response will replace the content inside the evolve block. Follow the output format strictly, and concentrate on delivering a clean, correct, and efficient implementation of `construct_packing()` and its helpers. Good luck!", "user": "# Current Program Information\n- Fitness: 13.9292\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.7995 \u2192 13.9292. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4053, output_length: 91.0000, overall_score: 0.3662, slope_changes: 75.4000, lag_error: 0.9943, avg_error: 1.2930, false_reversals: 63.0000, correlation: 0.3105, noise_reduction: 0.0000, smoothness_score: 0.2096, responsiveness_score: 0.5014, accuracy_score: 0.3105, efficiency_score: 1.0000, execution_time: 0.0004, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4053, output_length: 91.0000, overall_score: 0.3662, slope_changes: 75.4000, lag_error: 0.9943, avg_error: 1.2930, false_reversals: 63.0000, correlation: 0.3105, noise_reduction: 0.0000, smoothness_score: 0.2096, responsiveness_score: 0.5014, accuracy_score: 0.3105, efficiency_score: 1.0000, execution_time: 0.0004, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4001, output_length: 91.0000, overall_score: 0.3625, slope_changes: 80.4000, lag_error: 0.9635, avg_error: 1.2731, false_reversals: 65.8000, correlation: 0.3131, noise_reduction: 0.0000, smoothness_score: 0.1992, responsiveness_score: 0.5093, accuracy_score: 0.3131, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 15.2834)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prep(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _expw(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef weighted_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _expw(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(a)\n    y[0] = a[0]\n    for i in range(1, a.size):\n        y[i] = alpha * a[i] + (1 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _resvar(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    d = orig[:m] - filt[:m]\n    return float(np.mean(d * d))\n\ndef _combined(x, ws):\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n    # generate candidate filters\n    cand = {\n        \"basic\": adaptive_filter(a, ws),\n        \"weighted\": weighted_filter(a, ws),\n        \"exp\": exponential_smoothing(a, ws),\n    }\n    # compute residual variances, keep only non\u2011empty results\n    vars = {k: _resvar(a, v) for k, v in cand.items() if v.size}\n    if not vars:\n        return np.empty(0, dtype=float)\n    # perfect match shortcut\n    for k, v in vars.items():\n        if v == 0.0:\n            return cand[k][:len(a) - ws + 1]\n    # inverse\u2011variance weighting (deterministic order)\n    keys = sorted(vars)\n    inv = np.array([1.0 / vars[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    combined = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return combined\n\ndef _best(x, ws):\n    return _combined(x, ws)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    alg = str(algorithm_type).lower()\n    if alg == \"basic\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return _combined(sig, window_size)\n    if alg == \"exp\":\n        return exponential_smoothing(sig, window_size)\n    return _best(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4001), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (80.4000), Performs well on lag_error (0.9635), Performs well on avg_error (1.2731), Performs well on false_reversals (65.8000), Performs well on correlation (0.3131), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.1992), Performs well on responsiveness_score (0.5093), Performs well on accuracy_score (0.3131), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.7995)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _as_array(x, ws):\n    \"\"\"\n    Convert input to a 1\u2011D float array and validate ``ws``.\n    Returns a tuple ``(array, window)``; if the array is shorter than the\n    window an empty array is returned (no exception is raised).\n    \"\"\"\n    w = int(ws)\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\ndef _moving_average(arr: np.ndarray, w: int) -> np.ndarray:\n    \"\"\"Fast simple moving average using cumulative sum (valid convolution).\"\"\"\n    if w <= 1:\n        return arr.copy()\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[w:] - csum[:-w]) / w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w: int) -> np.ndarray:\n    \"\"\"Cached exponential weighting vector (right\u2011biased).\"\"\"\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (valid output length).\"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    return _moving_average(arr, w)\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (valid output length).\"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    return np.convolve(arr, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (output aligns with other filters).\"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Deterministic hybrid filter:\n    - Compute simple moving average and exponential\u2011weighted average.\n    - Blend them with weights inversely proportional to their residual variance\n      against the original (truncated) signal.\n    This often yields higher correlation and better noise reduction than a\n    single filter while keeping the output length ``n\u2011w+1``.\n    \"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n\n    # Individual filter outputs (both length n\u2011w+1)\n    adv = _moving_average(arr, w)\n    wgt = np.convolve(arr, _exp_weights(w), mode=\"valid\")\n\n    # Align original signal to filtered length for variance calculation\n    orig_trunc = arr[w - 1 :]\n\n    # Residual variances (protect against division by zero)\n    var_adv = np.var(orig_trunc - adv) if adv.size else float(\"inf\")\n    var_wgt = np.var(orig_trunc - wgt) if wgt.size else float(\"inf\")\n\n    # If both perfect, just return one of them\n    if var_adv == 0 and var_wgt == 0:\n        return adv\n\n    inv_adv = 1.0 / var_adv if var_adv > 0 else 0.0\n    inv_wgt = 1.0 / var_wgt if var_wgt > 0 else 0.0\n    total = inv_adv + inv_wgt\n\n    if total == 0.0:\n        # Fallback to simple average when variances are infinite\n        return (adv + wgt) / 2.0\n\n    # Weighted combination\n    return (inv_adv * adv + inv_wgt * wgt) / total\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Variance of the residual (original - filtered) over the overlapping region.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef _select_best_filter(x, window_size):\n    \"\"\"\n    Evaluate all available filters and return the one with the lowest residual\n    variance (i.e., the best noise\u2011reduction estimate).\n    \"\"\"\n    candidates = {\n        \"adaptive\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    best_name = min(candidates, key=lambda k: _residual_variance(\n        np.asarray(x, dtype=float).ravel(), candidates[k]\n    ))\n    return candidates[best_name]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        \"basic\" / \"adaptive\" \u2013 simple moving average\n        \"weighted\"            \u2013 exponential\u2011weighted moving average\n        \"ema\"                 \u2013 classic EMA\n        \"enhanced\"            \u2013 hybrid variance\u2011weighted filter\n        any other value       \u2013 automatic selection of the best filter\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    # fallback: auto\u2011select the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4053), Performs well on output_length (91.0000), Performs well on overall_score (0.3662), Performs well on slope_changes (75.4000), Performs well on lag_error (0.9943), Performs well on avg_error (1.2930), Performs well on false_reversals (63.0000), Performs well on correlation (0.3105), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2096), Performs well on responsiveness_score (0.5014), Performs well on accuracy_score (0.3105), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0004), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.7995)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_input(x, window_size):\n    \"\"\"Validate window size and convert input to a 1\u2011D float array.\"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(f\"window_size must be a positive integer, got {window_size}\")\n    arr = np.asarray(x, dtype=float).ravel()\n    if arr.size < window_size:\n        # Return empty array to signal that processing cannot be performed\n        return np.empty(0, dtype=float), window_size\n    return arr, window_size\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    \"\"\"Cached right\u2011biased exponential weighting vector that sums to 1.\"\"\"\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef _moving_average(arr, ws):\n    \"\"\"Fast simple moving average using cumulative sum (valid output).\"\"\"\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[ws:] - csum[:-ws]) / ws\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Uniform moving\u2011average filter (valid output length).\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    return _moving_average(arr, ws)\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter (valid output length).\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    return np.convolve(arr, _exp_weights(ws), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (output aligned with other filters).\"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n    alpha = 2.0 / (ws + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[ws - 1 :]\n\ndef _residual_variance(original, filtered):\n    \"\"\"Variance of the residual (original \u2013 filtered) over overlapping region.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef combined_filter(x, window_size=20):\n    \"\"\"\n    Deterministic hybrid filter:\n    - Compute simple moving average and exponential\u2011weighted average.\n    - Blend them with weights inversely proportional to their residual variance.\n    \"\"\"\n    arr, ws = _prepare_input(x, window_size)\n    if arr.size == 0:\n        return arr\n\n    simple = _moving_average(arr, ws)\n    weighted = np.convolve(arr, _exp_weights(ws), mode=\"valid\")\n\n    # Align original signal to filtered length for variance calculation\n    orig_trunc = arr[ws - 1 :]\n\n    var_simple = _residual_variance(orig_trunc, simple)\n    var_weighted = _residual_variance(orig_trunc, weighted)\n\n    # Handle perfect or undefined cases\n    if var_simple == 0 and var_weighted == 0:\n        return simple\n    inv_simple = 1.0 / var_simple if var_simple > 0 else 0.0\n    inv_weighted = 1.0 / var_weighted if var_weighted > 0 else 0.0\n    total = inv_simple + inv_weighted\n    if total == 0.0:\n        return (simple + weighted) / 2.0\n\n    return (inv_simple * simple + inv_weighted * weighted) / total\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\" : simple moving average\n        - \"weighted\"           : exponential weighted moving average\n        - \"ema\"                : classic EMA (aligned output)\n        - \"enhanced\" / \"combined\" : hybrid filter with variance weighting\n        - any other value     : defaults to the combined filter\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    # \"enhanced\", \"combined\", or unknown \u2192 use hybrid approach\n    return combined_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4053), Performs well on output_length (91.0000), Performs well on overall_score (0.3662), Performs well on slope_changes (75.4000), Performs well on lag_error (0.9943), Performs well on avg_error (1.2930), Performs well on false_reversals (63.0000), Performs well on correlation (0.3105), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2096), Performs well on responsiveness_score (0.5014), Performs well on accuracy_score (0.3105), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0004), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"hybrid\": _hybrid_filter(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        # use hybrid for stronger noise reduction while preserving length\n        return _hybrid_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.7995)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\nfrom typing import Callable\n\ndef _to_array(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\ndef _valid(arr, w):\n    \"\"\"Validate window size.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Cached right\u2011biased exponential weights that sum to 1.\"\"\"\n    w = int(w)\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (valid output).\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving\u2011average filter (valid output).\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic EMA aligned to the simple\u2011MA length.\"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Variance of (original \u2013 filtered) over the overlapping region.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef _combined_filter(x, window_size=20):\n    \"\"\"\n    Deterministic hybrid filter:\n    - Compute simple moving average and exponential\u2011weighted average.\n    - Blend them with weights inversely proportional to their residual variance\n      against the original (truncated) signal.\n    \"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n\n    simple = adaptive_filter(a, w)\n    weighted = weighted_filter(a, w)\n\n    # Align original to filtered length for variance calculation\n    orig_trunc = a[w - 1 :]\n\n    var_simple = _residual_variance(orig_trunc, simple)\n    var_weighted = _residual_variance(orig_trunc, weighted)\n\n    # Perfect match shortcut\n    if var_simple == 0 and var_weighted == 0:\n        return simple\n\n    inv_simple = 1.0 / var_simple if var_simple > 0 else 0.0\n    inv_weighted = 1.0 / var_weighted if var_weighted > 0 else 0.0\n    total = inv_simple + inv_weighted\n\n    if total == 0.0:\n        # Fallback to plain average\n        return (simple + weighted) / 2.0\n\n    return (inv_simple * simple + inv_weighted * weighted) / total\n\ndef _select_best_filter(x, window_size=20):\n    \"\"\"\n    Evaluate a small set of filters and return the one with the lowest\n    residual variance against the original signal.\n    \"\"\"\n    a = _to_array(x)\n    w = int(window_size)\n    if not _valid(a, w):\n        return np.empty(0, dtype=float)\n\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"combined\": _combined_filter(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\ndef _apply_filter(name: str, x, w) -> np.ndarray:\n    \"\"\"Map filter name to its implementation.\"\"\"\n    mapping: dict[str, Callable[[np.ndarray, int], np.ndarray]] = {\n        \"adaptive\": adaptive_filter,\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"combined\": _combined_filter,\n    }\n    func = mapping.get(name)\n    return func(x, w) if func else np.empty(0, dtype=float)\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the requested filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"adaptive\" / \"basic\" : simple moving average\n        - \"weighted\"           : exponential\u2011weighted moving average\n        - \"ema\"                : classic EMA (aligned output)\n        - \"combined\" / \"enhanced\" / \"hybrid\"\n                               : variance\u2011weighted hybrid of adaptive & weighted\n        - \"auto\"               : automatically select the best filter\n        - any other value     : fallback to the hybrid filter\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"adaptive\", \"basic\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg in (\"combined\", \"enhanced\", \"hybrid\"):\n        return _combined_filter(input_signal, window_size)\n    if alg == \"auto\":\n        return _select_best_filter(input_signal, window_size)\n    # Unknown algorithm \u2013 default to the hybrid filter\n    return _combined_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.7277, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _as_arr(x, w):\n    \"\"\"Convert input to a 1\u2011D float array.\n\n    Returns an empty array when the input is shorter than the window\n    (or when the window is non\u2011positive) so callers can handle the case\n    gracefully instead of raising an exception.\n    \"\"\"\n    try:\n        w = int(w)\n    except Exception:\n        w = 0\n    a = np.asarray(x, dtype=float).ravel()\n    if w <= 0 or a.size < w:\n        return np.empty(0, dtype=float)\n    return a\n\n@lru_cache(maxsize=None)\ndef _exp_weights(w):\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef adaptive(x, w=20):\n    a = _as_arr(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\ndef enhanced(x, w=20):\n    a = _as_arr(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w)[::-1], mode=\"valid\")\n\ndef enhanced_blend(x, w=20):\n    \"\"\"Blend simple and exponential weighted averages.\"\"\"\n    a = _as_arr(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    e = np.convolve(a, _exp_weights(w)[::-1], \"valid\")\n    m = np.convolve(a, np.ones(w) / w, \"valid\")\n    return (e + m) / 2.0\n\ndef median(x, w=21):\n    if w % 2 == 0:\n        w += 1\n    a = _as_arr(x, w)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    s = a.strides[0]\n    win = np.lib.stride_tricks.as_strided(a, (a.size - w + 1, w), (s, s))\n    return np.median(win, axis=1)\n\ndef hybrid(x, w=20):\n    \"\"\"Median filter followed by blended enhanced smoothing.\"\"\"\n    med = median(x, w if w % 2 else w + 1)\n    return enhanced_blend(med, w)\n\n@lru_cache(maxsize=None)\ndef _sg_coeffs(w, order):\n    \"\"\"Compute Savitzky\u2011Golay convolution coefficients.\"\"\"\n    # ensure odd window\n    if w % 2 == 0:\n        w += 1\n    half = w // 2\n    # Vandermonde matrix\n    A = np.arange(-half, half + 1)[:, None] ** np.arange(order + 1)\n    # Pseudo\u2011inverse\n    coeffs = np.linalg.pinv(A)[0]\n    return coeffs\n\ndef savgol(x, w=21, order=2):\n    \"\"\"Savitzky\u2011Golay filter (polynomial order 2 by default).\"\"\"\n    if w % 2 == 0:\n        w += 1\n    a = _as_arr(x, w)\n    if a.size == 0:\n        return a\n    coeffs = _sg_coeffs(w, order)\n    return np.convolve(a, coeffs[::-1], mode=\"valid\")\n\ndef _residual_variance(original, filtered):\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return np.inf\n    return float(np.var(original[:n] - filtered[:n]))\n\ndef process_signal(sig, w=20, alg=\"auto\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best one.\n\n    Supported ``alg`` values (case\u2011insensitive):\n        - \"adaptive\"\n        - \"enhanced\"\n        - \"blend\"   (enhanced_blend)\n        - \"median\"\n        - \"hybrid\"\n        - \"sg\" or \"savgol\" (Savitzky\u2011Golay)\n        - any other value \u2192 automatic selection based on residual variance.\n    \"\"\"\n    a = (alg or \"\").lower()\n    if a == \"adaptive\":\n        return adaptive(sig, w)\n    if a == \"enhanced\":\n        return enhanced(sig, w)\n    if a in (\"blend\", \"enhanced_blend\"):\n        return enhanced_blend(sig, w)\n    if a == \"median\":\n        return median(sig, w)\n    if a == \"hybrid\":\n        return hybrid(sig, w)\n    if a in (\"sg\", \"savgol\", \"savitzky_golay\"):\n        return savgol(sig, w if w % 2 else w + 1)\n\n    # ----- Auto\u2011selection -------------------------------------------------\n    candidates = {\n        \"adaptive\": adaptive(sig, w),\n        \"enhanced\": enhanced(sig, w),\n        \"blend\": enhanced_blend(sig, w),\n        \"median\": median(sig, w),\n        \"hybrid\": hybrid(sig, w),\n        \"sg\": savgol(sig, w if w % 2 else w + 1),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best = min(candidates, key=lambda k: _residual_variance(sig, candidates[k]))\n    return candidates[best]\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 14.0152, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\n\ndef _to_arr(x):\n    \"\"\"Convert input to a 1\u2011D float ndarray.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _valid(arr, w):\n    \"\"\"Return (arr, w) if usable, else an empty array.\"\"\"\n    if w <= 0 or arr.size < w:\n        return np.empty(0, dtype=float), w\n    return arr, w\n\n\n@lru_cache(maxsize=64)\ndef _gauss_weights(w):\n    \"\"\"Gaussian kernel for weighted smoothing.\"\"\"\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return wts / wts.sum()\n\n\ndef _ma(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\") if a.size else a\n\n\ndef _ema(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, a.size):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef _gauss(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    return np.convolve(a, _gauss_weights(w), mode=\"valid\") if a.size else a\n\n\ndef _median(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _hybrid(x, w):\n    ma = _ma(x, w)\n    ga = _gauss(x, w)\n    if ma.size and ga.size:\n        return (ma + ga) / 2.0\n    return ma if ma.size else ga\n\n\ndef _resvar(orig, filt):\n    \"\"\"Residual variance on the overlapping segment.\"\"\"\n    if not filt.size:\n        return float(\"inf\")\n    n = min(orig.size, filt.size)\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best(x, w):\n    a, _ = _valid(_to_arr(x), w)\n    if not a.size:\n        return a\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gauss(a, w),\n        \"median\": _median(a, w),\n        \"hybrid\": _hybrid(a, w),\n    }\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, dtype=float)\n    best_key = min(candidates, key=lambda k: _resvar(a, candidates[k]))\n    return candidates[best_key]\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm or automatically select the best one.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        - \"basic\", \"ma\"      : simple moving average\n        - \"ewma\", \"ema\"      : exponential moving average\n        - \"weighted\", \"gauss\", \"gaussian\"\n                              : Gaussian\u2011weighted moving average\n        - \"median\"           : median filter\n        - \"hybrid\"           : average of MA and Gaussian\n        - \"combined\", \"auto\", \"enhanced\"\n                              : automatic selection of the best filter\n        - any other value   : falls back to automatic selection\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg in (\"weighted\", \"gauss\", \"gaussian\"):\n        return _gauss(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg == \"hybrid\":\n        return _hybrid(sig, w)\n    # default / auto\u2011selection\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _as_array(x, ws):\n    \"\"\"\n    Convert input to a 1\u2011D float array and validate ``ws``.\n    Returns a tuple ``(array, window)``; if the array is shorter than the\n    window an empty array is returned (no exception is raised).\n    \"\"\"\n    w = int(ws)\n    if w <= 0:\n        raise ValueError(\"window_size must be positive\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\ndef _moving_average(arr: np.ndarray, w: int) -> np.ndarray:\n    \"\"\"Fast simple moving average using cumulative sum (valid convolution).\"\"\"\n    if w <= 1:\n        return arr.copy()\n    csum = np.cumsum(np.insert(arr, 0, 0.0))\n    return (csum[w:] - csum[:-w]) / w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w: int) -> np.ndarray:\n    \"\"\"Cached exponential weighting vector (right\u2011biased).\"\"\"\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter (valid output length).\"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    return _moving_average(arr, w)\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average (valid output length).\"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    return np.convolve(arr, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (output aligns with other filters).\"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(arr)\n    ema[0] = arr[0]\n    for i in range(1, arr.size):\n        ema[i] = alpha * arr[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"\n    Detrend using a simple moving average, then apply an exponential\u2011weighted\n    moving average to the detrended series.\n    \"\"\"\n    arr, w = _as_array(x, window_size)\n    if arr.size == 0:\n        return np.empty(0, dtype=float)\n    trend = _moving_average(arr, w)\n    # Pad trend to original length (repeat edge values)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = arr - trend_full\n    return np.convolve(detrended, _exp_weights(w), mode=\"valid\")\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Variance of the residual (original - filtered) over the overlapping region.\"\"\"\n    m = min(len(original), len(filtered))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(original[:m] - filtered[:m]))\n\ndef _select_best_filter(x, window_size):\n    \"\"\"\n    Evaluate all available filters and return the one with the lowest residual\n    variance (i.e., the best noise\u2011reduction estimate).\n    \"\"\"\n    candidates = {\n        \"adaptive\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n    }\n    best_name = min(candidates, key=lambda k: _residual_variance(\n        np.asarray(x, dtype=float).ravel(), candidates[k]\n    ))\n    return candidates[best_name]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"\n    Dispatch to the selected filtering algorithm.\n\n    Supported ``algorithm_type`` values (case\u2011insensitive):\n        \"basic\" / \"adaptive\" \u2013 simple moving average\n        \"weighted\"            \u2013 exponential\u2011weighted moving average\n        \"ema\"                 \u2013 classic EMA\n        \"enhanced\"            \u2013 detrended weighted filter\n        any other value       \u2013 automatic selection of the best filter\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    # fallback: auto\u2011select the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare_signal(x, window_size):\n    \"\"\"Validate input and convert to 1\u2011D float array.\n\n    Returns (arr, w) where ``arr`` may be empty if the input length is\n    smaller than the window size.\n    \"\"\"\n    if not isinstance(window_size, int) or window_size <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n    arr = np.asarray(x, dtype=float).ravel()\n    w = int(window_size)\n    if arr.size < w:\n        return np.empty(0, dtype=float), w\n    return arr, w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    \"\"\"Right\u2011biased exponential weights that sum to 1.\"\"\"\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average using cumulative sums.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential weighted moving average (valid convolution).\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic EMA; output length matches other filters.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, a.size):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\ndef median_filter(x, window_size=20):\n    \"\"\"Median filter using stride tricks; forces odd window size.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w % 2 == 0:\n        w += 1\n    if w == 1:\n        return a.copy()\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    \"\"\"Adaptive filter followed by weighted filter (preserves length).\"\"\"\n    first = adaptive_filter(x, window_size)\n    return weighted_filter(first, window_size)\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Detrend with simple moving average then apply weighted filter.\"\"\"\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _residual_variance(orig, filt):\n    \"\"\"Variance of the residual (orig - filt) over overlapping region.\"\"\"\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:m] - filt[:m]))\n\ndef _select_best_filter(x, window_size):\n    \"\"\"Choose the filter with minimal residual variance.\"\"\"\n    candidates = {\n        \"adaptive\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"median\": median_filter(x, window_size),\n        \"hybrid\": hybrid_filter(x, window_size),\n        \"enhanced\": enhanced_filter(x, window_size),\n    }\n    best_key = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best_key]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to the requested algorithm or auto\u2011select the best one.\"\"\"\n    algo = (algorithm_type or \"\").lower()\n    if algo in (\"basic\", \"adaptive\"):\n        return adaptive_filter(sig, window_size)\n    if algo == \"weighted\":\n        return weighted_filter(sig, window_size)\n    if algo == \"ema\":\n        return ema_filter(sig, window_size)\n    if algo == \"median\":\n        return median_filter(sig, window_size)\n    if algo == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    if algo == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    # Fallback: automatic selection based on residual variance\n    a, w = _prepare_signal(sig, window_size)\n    if a.size == 0:\n        return a\n    return _select_best_filter(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\nfrom functools import lru_cache\n\ndef _prepare_signal(x, w):\n    if not isinstance(w, int) or w <= 0:\n        raise ValueError(\"window_size must be a positive integer\")\n    a = np.asarray(x, dtype=float).ravel()\n    w = int(w)\n    if a.size < w:\n        return np.empty(0, dtype=float), w\n    return a, w\n\n@lru_cache(maxsize=64)\ndef _exp_weights(w):\n    e = np.exp(np.linspace(-2, 0, w))\n    return e / e.sum()\n\ndef _moving_average(a, w):\n    if w == 1:\n        return a.copy()\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    return _moving_average(a, w)\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w == 1:\n        return a.copy()\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, a.size):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\ndef median_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    if w % 2 == 0:\n        w += 1\n    if w == 1:\n        return a.copy()\n    shape = (a.size - w + 1, w)\n    strides = (a.strides[0], a.strides[0])\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n    return np.median(windows, axis=1)\n\ndef hybrid_filter(x, window_size=20):\n    first = adaptive_filter(x, window_size)\n    return weighted_filter(first, window_size)\n\ndef combined_filter(x, window_size=20):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    adv = _moving_average(a, w)\n    wgt = np.convolve(a, _exp_weights(w), mode=\"valid\")\n    orig = a[w - 1 :]\n    var_adv = _residual_variance(orig, adv)\n    var_wgt = _residual_variance(orig, wgt)\n    if var_adv == 0 and var_wgt == 0:\n        return adv\n    inv_adv = 1.0 / var_adv if var_adv > 0 else 0.0\n    inv_wgt = 1.0 / var_wgt if var_wgt > 0 else 0.0\n    total = inv_adv + inv_wgt\n    if total == 0.0:\n        return (adv + wgt) / 2.0\n    return (inv_adv * adv + inv_wgt * wgt) / total\n\ndef enhanced_filter(x, window_size=20):\n    return combined_filter(x, window_size)\n\ndef _select_best_filter(x, window_size):\n    a, w = _prepare_signal(x, window_size)\n    if a.size == 0:\n        return a\n    cand = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"median\": median_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    best = min(cand, key=lambda k: _residual_variance(a, cand[k]))\n    return cand[best]\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"adaptive\"):\n        return adaptive_filter(sig, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(sig, window_size)\n    if alg == \"ema\":\n        return ema_filter(sig, window_size)\n    if alg == \"median\":\n        return median_filter(sig, window_size)\n    if alg == \"hybrid\":\n        return hybrid_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter(sig, window_size)\n    a, w = _prepare_signal(sig, window_size)\n    return _select_best_filter(a, w)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}