{"id": "077db35c-5fb2-484b-b76a-dbce6422623b", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _valid_window(arr, w):\n    \"\"\"Return True if window size is a positive integer that fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n\ndef _ma(arr, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(arr, w):\n    \"\"\"Classic exponential moving average.\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(arr)\n    e[0] = arr[0]\n    for i in range(1, arr.size):\n        e[i] = alpha * arr[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\n\ndef _gaussian(arr, w):\n    \"\"\"Gaussian\u2011kernel weighted moving average.\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    kernel = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    kernel /= kernel.sum()\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\ndef _median(arr, w):\n    \"\"\"Sliding\u2011window median filter.\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _combined_ma_ema(arr, w):\n    \"\"\"Average of MA and EMA (both must be present).\"\"\"\n    ma = _ma(arr, w)\n    ema = _ema(arr, w)\n    if ma.size and ema.size:\n        return (ma + ema) / 2.0\n    return ma if ma.size else ema\n\n\ndef _detrended_weighted(arr, w):\n    \"\"\"\n    Detrend with a simple moving average, smooth the detrended series\n    using a Gaussian kernel, then re\u2011add the trend.\n    \"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n\n    # Trend estimation (MA)\n    trend = _ma(arr, w)                       # length = n\u2011w+1\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n\n    # Detrend, smooth, and restore trend\n    detrended = arr - trend_full\n    smooth = _gaussian(detrended, w)          # length = n\u2011w+1\n    return smooth + trend                     # both have same length\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean\u2011squared deviation over the overlapping portion.\"\"\"\n    n = min(orig.size, filt.size)\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(arr, w):\n    \"\"\"Select the filter with the lowest residual variance (deterministic tie\u2011break).\"\"\"\n    candidates = {\n        \"ma\": _ma(arr, w),\n        \"ema\": _ema(arr, w),\n        \"gaussian\": _gaussian(arr, w),\n        \"median\": _median(arr, w),\n        \"combined\": _combined_ma_ema(arr, w),\n        \"detrended\": _detrended_weighted(arr, w),\n    }\n    # Discard empty results\n    filtered = [(name, data) for name, data in candidates.items() if data.size]\n    if not filtered:\n        return np.empty(0, float)\n\n    # Compute residual variances\n    metrics = [( _residual_variance(arr, data), name, data) for name, data in filtered]\n    # Sort by variance then name for deterministic choice\n    metrics.sort(key=lambda t: (t[0], t[1]))\n    return metrics[0][2]\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Alias for simple moving\u2011average (MA).\"\"\"\n    arr = _to_numpy(x)\n    w = int(window_size)\n    return _ma(arr, w)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported `algorithm_type` values (case\u2011insensitive):\n        basic / ma          \u2013 simple moving average\n        ema / ewma          \u2013 exponential moving average\n        weighted / gaussian \u2013 Gaussian\u2011weighted moving average\n        median              \u2013 median filter\n        combined / average  \u2013 average of MA and EMA\n        detrended / trend   \u2013 detrended\u2011weighted filter\n        auto / default      \u2013 automatic selection of the best filter\n        any other value     \u2013 falls back to automatic selection\n    \"\"\"\n    arr = _to_numpy(input_signal)\n    w = int(window_size)\n\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n\n    key = (algorithm_type or \"\").lower()\n\n    if key in (\"basic\", \"ma\"):\n        return _ma(arr, w)\n    if key in (\"ema\", \"ewma\"):\n        return _ema(arr, w)\n    if key in (\"weighted\", \"gaussian\"):\n        return _gaussian(arr, w)\n    if key == \"median\":\n        return _median(arr, w)\n    if key in (\"combined\", \"average\"):\n        return _combined_ma_ema(arr, w)\n    if key in (\"detrended\", \"trend\"):\n        return _detrended_weighted(arr, w)\n    if key in (\"auto\", \"default\"):\n        return _best_filter(arr, w)\n\n    # Fallback to automatic best\u2011filter selection\n    return _best_filter(arr, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "cf184b97-80db-4129-9d44-d05e6a16fd98", "generation": 9, "timestamp": 1764942587.904105, "iteration_found": 750, "metrics": {"runs_successfully": 1.0, "composite_score": 0.4205150578718804, "output_length": 91.0, "overall_score": 0.38035985150742135, "slope_changes": 68.0, "lag_error": 0.9780360456577568, "avg_error": 1.2635358917076887, "false_reversals": 57.8, "correlation": 0.3334964145206185, "noise_reduction": 0.0, "smoothness_score": 0.22727272727272727, "responsiveness_score": 0.5055519600844633, "accuracy_score": 0.3334964145206185, "efficiency_score": 1.0, "execution_time": 0.00113983154296875, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4205150578718804, "output_length": 91.0, "overall_score": 0.38035985150742135, "slope_changes": 68.0, "lag_error": 0.9780360456577568, "avg_error": 1.2635358917076887, "false_reversals": 57.8, "correlation": 0.3334964145206185, "noise_reduction": 0.0, "smoothness_score": 0.22727272727272727, "responsiveness_score": 0.5055519600844633, "accuracy_score": 0.3334964145206185, "efficiency_score": 1.0, "execution_time": 0.0005160331726074219, "success_rate": 1.0}, "island": 2}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are a code\u2011evolution assistant whose sole job is to rewrite and improve the **`construct_packing()`** function (and any helper functions defined inside the evolve block).  \n\n**What you must do**\n\n1. **Output ONLY the source code that belongs between the markers**  \n   ```text\n   # EVOLVE-BLOCK-START\n   ...your code here...\n   # EVOLVE-BLOCK-END\n   ```  \n   Do **not** include the marker lines themselves, and do **not** add any imports, top\u2011level statements, or surrounding boiler\u2011plate (e.g., `run_packing()`). The system will automatically splice your output into the existing file.\n\n2. **Preserve the existing function signatures** exactly as they appear in the original block. You may add new helper functions **inside** the block, but they must be defined before they are used and must not change the public API.\n\n3. **Focus your changes on algorithmic correctness and efficiency** for packing items into containers:\n   - Use clear variable names and docstrings.\n   - Validate inputs (e.g., non\u2011negative dimensions, positive capacities).\n   - Implement a deterministic, greedy heuristic (e.g., sort items by volume descending, then place each into the first container that fits).\n   - Return a well\u2011structured result (list of containers, each with the items it holds and remaining space).\n\n4. **Do NOT**:\n   - Produce code that exceeds the original block\u2019s indentation level.\n   - Introduce new global variables or modify code outside the block.\n   - Write code that relies on external libraries not already imported in the surrounding file.\n   - Leave placeholder `pass` statements or `TODO` comments; the function must be fully operational.\n   - Use overly aggressive meta\u2011programming tricks (e.g., `exec`, dynamic import) \u2013 these caused repeated invalid programs in earlier attempts.\n\n**Suggested concrete strategy**\n\n- **Step\u202f1 \u2013 Input sanitisation**: Verify that each item is a tuple `(id, width, height, depth)` and each container is a tuple `(id, max_volume)`. Raise a clear `ValueError` if the data is malformed.\n- **Step\u202f2 \u2013 Pre\u2011processing**: Compute the volume of every item once, store it alongside the item, and sort the items by volume descending.\n- **Step\u202f3 \u2013 Greedy packing loop**: Iterate over the sorted items; for each item, scan the containers in order of remaining capacity (largest first) and place the item in the first container where `item_volume <= remaining_capacity`. Update the container\u2019s remaining capacity and record the assignment.\n- **Step\u202f4 \u2013 Result construction**: Return a list of dictionaries, each representing a container:\n  ```python\n  {\n      \"container_id\": <id>,\n      \"items\": [<item_id>, ...],\n      \"remaining_volume\": <float>\n  }\n  ```\n  This format matches the expectations of the surrounding test harness.\n\n- **Step\u202f5 \u2013 Edge cases**: If an item cannot fit any container, collect it in a separate list `unpacked_items` and return it as part of the final structure (e.g., under the key `\"unpacked\"`). This prevents silent failures.\n\n**Remember**: The only output you may produce is the revised Python code for the evolve block. No explanations, no markers, no extra text. Follow the format exactly.", "user": "# Current Program Information\n- Fitness: 14.0152\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.0152 \u2192 14.0152. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0015, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4107, output_length: 91.0000, overall_score: 0.3693, slope_changes: 70.2000, lag_error: 1.0018, avg_error: 1.3037, false_reversals: 59.0000, correlation: 0.3035, noise_reduction: 0.0000, smoothness_score: 0.2217, responsiveness_score: 0.4996, accuracy_score: 0.3035, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4001, output_length: 91.0000, overall_score: 0.3625, slope_changes: 80.4000, lag_error: 0.9635, avg_error: 1.2731, false_reversals: 65.8000, correlation: 0.3131, noise_reduction: 0.0000, smoothness_score: 0.1992, responsiveness_score: 0.5093, accuracy_score: 0.3131, efficiency_score: 1.0000, execution_time: 0.0006, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 15.2834)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prep(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(f\"window_size must be positive, got {ws}\")\n    a = np.asarray(x, dtype=float).ravel()\n    if a.size < ws:\n        return np.empty(0, dtype=float), ws\n    return a, ws\n\n@lru_cache(maxsize=64)\ndef _expw(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[ws:] - c[:-ws]) / ws\n\ndef weighted_filter(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _expw(ws), mode=\"valid\")\n\ndef exponential_smoothing(x, window_size=20):\n    a, ws = _prep(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (ws + 1.0)\n    y = np.empty_like(a)\n    y[0] = a[0]\n    for i in range(1, a.size):\n        y[i] = alpha * a[i] + (1 - alpha) * y[i - 1]\n    return y[ws - 1 :]\n\ndef _resvar(orig, filt):\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    d = orig[:m] - filt[:m]\n    return float(np.mean(d * d))\n\ndef _combined(x, ws):\n    a, _ = _prep(x, ws)\n    if a.size == 0:\n        return a\n    # generate candidate filters\n    cand = {\n        \"basic\": adaptive_filter(a, ws),\n        \"weighted\": weighted_filter(a, ws),\n        \"exp\": exponential_smoothing(a, ws),\n    }\n    # compute residual variances, keep only non\u2011empty results\n    vars = {k: _resvar(a, v) for k, v in cand.items() if v.size}\n    if not vars:\n        return np.empty(0, dtype=float)\n    # perfect match shortcut\n    for k, v in vars.items():\n        if v == 0.0:\n            return cand[k][:len(a) - ws + 1]\n    # inverse\u2011variance weighting (deterministic order)\n    keys = sorted(vars)\n    inv = np.array([1.0 / vars[k] for k in keys])\n    wts = inv / inv.sum()\n    min_len = min(len(cand[k]) for k in keys)\n    combined = sum(wts[i] * cand[k][:min_len] for i, k in enumerate(keys))\n    return combined\n\ndef _best(x, ws):\n    return _combined(x, ws)\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    alg = str(algorithm_type).lower()\n    if alg == \"basic\":\n        return adaptive_filter(sig, window_size)\n    if alg == \"enhanced\":\n        return _combined(sig, window_size)\n    if alg == \"exp\":\n        return exponential_smoothing(sig, window_size)\n    return _best(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4001), Performs well on output_length (91.0000), Performs well on overall_score (0.3625), Performs well on slope_changes (80.4000), Performs well on lag_error (0.9635), Performs well on avg_error (1.2731), Performs well on false_reversals (65.8000), Performs well on correlation (0.3131), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.1992), Performs well on responsiveness_score (0.5093), Performs well on accuracy_score (0.3131), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.2259)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\nfrom functools import lru_cache\n\ndef _prepare(x, ws):\n    ws = int(ws)\n    if ws <= 0:\n        raise ValueError(\"window_size must be positive\")\n    arr = np.asarray(x, dtype=float)\n    if arr.ndim != 1:\n        raise ValueError(\"input must be 1\u2011D\")\n    if arr.size < ws:\n        return np.empty(0, dtype=float), ws\n    return arr, ws\n\n@lru_cache(maxsize=64)\ndef _exp_weights(ws):\n    w = np.exp(np.linspace(-2, 0, ws))\n    return w / w.sum()\n\ndef adaptive_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    c = np.cumsum(np.insert(a, 0, 0.0))\n    return (c[w:] - c[:-w]) / w\n\ndef weighted_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    return np.convolve(a, _exp_weights(w), mode=\"valid\")\n\ndef ema_filter(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, a.size):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    trend = adaptive_filter(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_filter(detrended, w)\n\ndef _hybrid_filter(x, window_size=20):\n    \"\"\"Average of weighted and adaptive filters \u2013 keeps length unchanged.\"\"\"\n    a, w = _prepare(x, window_size)\n    if a.size == 0:\n        return a\n    weighted = weighted_filter(a, w)\n    adaptive = adaptive_filter(a, w)\n    # both have identical length (n\u2011w+1)\n    return (weighted + adaptive) / 2.0\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\ndef _select_best_filter(x, window_size):\n    cand = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"hybrid\": _hybrid_filter(x, window_size),\n    }\n    return cand[min(cand, key=lambda k: _residual_variance(x, cand[k]))]\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"enhanced\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        # use hybrid for stronger noise reduction while preserving length\n        return _hybrid_filter(input_signal, window_size)\n    # fallback to auto\u2011selection of the best filter\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4107), Performs well on output_length (91.0000), Performs well on overall_score (0.3693), Performs well on slope_changes (70.2000), Performs well on lag_error (1.0018), Performs well on avg_error (1.3037), Performs well on false_reversals (59.0000), Performs well on correlation (0.3035), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2217), Performs well on responsiveness_score (0.4996), Performs well on accuracy_score (0.3035), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0006), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Classic exponential moving average aligned with MA output length.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011kernel moving average.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    kernel = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    kernel /= kernel.sum()\n    return np.convolve(a, kernel, mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _combined_ma_ema(x, w):\n    \"\"\"Average of MA and EMA (both have the same length).\"\"\"\n    ma = _ma(x, w)\n    ema = _ema(x, w)\n    if ma.size and ema.size:\n        return (ma + ema) / 2.0\n    return ma if ma.size else ema\n\n\ndef _detrended_weighted(x, w):\n    \"\"\"\n    Detrend using a simple moving average, smooth the detrended series\n    with a Gaussian weighted filter, then re\u2011add the trend.\n    \"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n\n    # Estimate local trend (MA)\n    trend = _ma(a, w)                     # length = n - w + 1\n    # Pad trend to original length by repeating edge values\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n\n    # Detrend\n    detrended = a - trend_full\n\n    # Smooth detrended series with Gaussian kernel\n    smooth = _gaussian(detrended, w)\n\n    # Re\u2011add trend (already aligned with smooth output)\n    return smooth + trend\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean\u2011squared deviation between aligned signals.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"\n    Select the filter with the lowest residual variance.\n    Considers MA, EMA, Gaussian, Median, combined MA/EMA, and detrended weighted.\n    \"\"\"\n    a = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gaussian\": _gaussian(a, w),\n        \"median\": _median(a, w),\n        \"combined\": _combined_ma_ema(a, w),\n        \"detrended\": _detrended_weighted(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.array([], dtype=float)\n    # Return the filter with minimal residual variance\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to requested filter.\n    Supported types (case\u2011insensitive):\n        basic/ma, ewma/ema, weighted, median, combined/enhanced.\n    Any other value falls back to automatic best\u2011filter selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg == \"weighted\":\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg in (\"combined\", \"enhanced\"):\n        return _best_filter(sig, w)\n    # Fallback to automatic selection\n    return _best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0015), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _arr(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(arr, w):\n    \"\"\"Return True if window size is positive and fits the array.\"\"\"\n    return w > 0 and arr.size >= w\n\n\ndef _ma(x, w, wt=None):\n    \"\"\"Simple moving average (uniform or custom weights).\"\"\"\n    a = _arr(x)\n    if not _validate_window(a, w):\n        return np.empty(0, float)\n    if wt is None:\n        wt = np.ones(w) / w\n    else:\n        wt = np.asarray(wt, dtype=float)\n        if wt.size != w:\n            raise ValueError(\"Weight vector length must equal window size\")\n        wt = wt / wt.sum()\n    return np.convolve(a, wt, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    a = _arr(x)\n    if not _validate_window(a, w):\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, len(a)):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011kernel moving average.\"\"\"\n    a = _arr(x)\n    if not _validate_window(a, w):\n        return np.empty(0, float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    kernel = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    kernel /= kernel.sum()\n    return np.convolve(a, kernel, mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter.\"\"\"\n    a = _arr(x)\n    if not _validate_window(a, w):\n        return np.empty(0, float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _combined_ma_ema(x, w):\n    \"\"\"Average of MA and EMA where both are available.\"\"\"\n    ma = _ma(x, w)\n    ema = _ema(x, w)\n    if ma.size and ema.size:\n        return (ma + ema) / 2.0\n    return ma if ma.size else ema\n\n\ndef _detrended_weighted(x, w):\n    \"\"\"\n    Detrend with a simple moving average, smooth the detrended series\n    using a Gaussian kernel, then re\u2011add the trend.\n    \"\"\"\n    a = _arr(x)\n    if not _validate_window(a, w):\n        return np.empty(0, float)\n\n    # Estimate trend via MA\n    trend = _ma(a, w)                     # length = n - w + 1\n    # Pad trend to original length (repeat first value for missing prefix)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n\n    # Detrend\n    detrended = a - trend_full\n\n    # Smooth detrended part\n    smooth = _gaussian(detrended, w)\n\n    # Align and add back trend (trend already aligned with smooth output)\n    return smooth + trend\n\n\ndef _resvar(orig, filt):\n    \"\"\"Mean\u2011squared deviation between overlapping portions of two signals.\"\"\"\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n])) if n else float(\"inf\")\n\n\ndef _best(x, w):\n    \"\"\"Select the single filter with the lowest residual variance (deterministic tie\u2011break).\"\"\"\n    a = _arr(x)\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gaussian(a, w),\n        \"median\": _median(a, w),\n        \"combined\": _combined_ma_ema(a, w),\n        \"detrended\": _detrended_weighted(a, w),\n    }\n    filtered = [(name, arr) for name, arr in candidates.items() if arr.size]\n    if not filtered:\n        return np.empty(0, float)\n\n    metrics = [( _resvar(a, arr), name, arr) for name, arr in filtered]\n    _, _, best_arr = min(metrics, key=lambda t: (t[0], t[1]))\n    return best_arr\n\n\ndef _ensemble_best(x, w, top_n=2):\n    \"\"\"\n    Combine the top `top_n` filters (by residual variance) using\n    inverse\u2011variance weighting. Guarantees deterministic output.\n    \"\"\"\n    a = _arr(x)\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gaussian(a, w),\n        \"median\": _median(a, w),\n        \"combined\": _combined_ma_ema(a, w),\n        \"detrended\": _detrended_weighted(a, w),\n    }\n    filtered = [(name, arr) for name, arr in candidates.items() if arr.size]\n    if not filtered:\n        return np.empty(0, float)\n\n    metrics = [( _resvar(a, arr), name, arr) for name, arr in filtered]\n    # Sort by variance then name for deterministic tie\u2011break\n    metrics.sort(key=lambda t: (t[0], t[1]))\n    top = metrics[:min(top_n, len(metrics))]\n\n    variances = np.array([m[0] for m in top], dtype=float)\n    # Guard against zero variance (perfect fit) \u2013 give it full weight\n    if np.any(variances == 0):\n        weights = np.where(variances == 0, 1.0, 0.0)\n    else:\n        inv = 1.0 / variances\n        weights = inv / inv.sum()\n\n    result = sum(wt * arr for wt, (_, _, arr) in zip(weights, top))\n    return result\n\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Parameters\n    ----------\n    sig : array\u2011like\n        Input signal.\n    w : int, optional\n        Window size (default 20).\n    alg : str, optional\n        Algorithm identifier. Supported (case\u2011insensitive):\n        - \"basic\" / \"ma\"          : simple moving average\n        - \"ema\" / \"ewma\"          : exponential moving average\n        - \"gaussian\" / \"weighted\": Gaussian\u2011kernel smoothing\n        - \"median\"                : median filter\n        - \"combined\" / \"average\"  : average of MA and EMA\n        - \"detrended\" / \"trend\"  : detrended\u2011weighted filter\n        - \"ensemble\"              : weighted ensemble of top filters\n        - any other value         : automatic best\u2011filter selection\n\n    Returns\n    -------\n    ndarray\n        Filtered signal (may be empty if window is invalid).\n    \"\"\"\n    a = _arr(sig)\n    if not _validate_window(a, w):\n        return np.empty(0, float)\n\n    key = (alg or \"\").lower()\n    if key in (\"basic\", \"ma\"):\n        return _ma(a, w)\n    if key in (\"ema\", \"ewma\"):\n        return _ema(a, w)\n    if key in (\"gaussian\", \"weighted\"):\n        return _gaussian(a, w)\n    if key == \"median\":\n        return _median(a, w)\n    if key in (\"combined\", \"average\"):\n        return _combined_ma_ema(a, w)\n    if key in (\"detrended\", \"trend\"):\n        return _detrended_weighted(a, w)\n    if key == \"ensemble\":\n        return _ensemble_best(a, w)\n\n    # Fallback: automatic selection of the single best filter\n    return _best(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_np(x):\n    return np.asarray(x, float).ravel()\n\ndef _valid(a, w):\n    return w > 0 and a.size >= w\n\ndef _conv(a, w, weights=None):\n    if not _valid(a, w):\n        return np.empty(0, float)\n    if weights is None:\n        weights = np.ones(w) / w\n    else:\n        weights = np.asarray(weights, float)\n        if weights.size != w:\n            raise ValueError(\"Weight length must equal window size\")\n        weights = weights / weights.sum()\n    return np.convolve(a, weights, mode=\"valid\")\n\ndef _ma(a, w):\n    return _conv(a, w)\n\ndef _gaussian(a, w):\n    if not _valid(a, w):\n        return np.empty(0, float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    kernel = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    return _conv(a, w, kernel)\n\ndef _ema(a, w):\n    if not _valid(a, w):\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, a.size):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\ndef _median(a, w):\n    if not _valid(a, w):\n        return np.empty(0, float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\ndef _combined_ma_ema(a, w):\n    ma = _ma(a, w)\n    ema = _ema(a, w)\n    return (ma + ema) / 2.0 if ma.size and ema.size else (ma if ma.size else ema)\n\ndef _detrended_weighted(a, w):\n    if not _valid(a, w):\n        return np.empty(0, float)\n    trend = _ma(a, w)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    smooth = _gaussian(detrended, w)\n    return smooth + trend\n\ndef _resvar(orig, filt):\n    n = min(orig.size, filt.size)\n    return float(np.var(orig[:n] - filt[:n])) if n else float(\"inf\")\n\ndef _best(a, w):\n    cand = {\n        \"combined\": _combined_ma_ema(a, w),\n        \"detrended\": _detrended_weighted(a, w),\n        \"gauss\": _gaussian(a, w),\n        \"ma\": _ma(a, w),\n        \"median\": _median(a, w),\n        \"ema\": _ema(a, w),\n    }\n    filtered = [(k, v) for k, v in cand.items() if v.size]\n    if not filtered:\n        return np.empty(0, float)\n    best = min((( _resvar(a, v), k, v) for k, v in filtered), key=lambda t: (t[0], t[1]))\n    return best[2]\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    a = _to_np(sig)\n    if not _valid(a, w):\n        return np.empty(0, float)\n    key = (alg or \"\").lower()\n    if key in (\"basic\", \"ma\"):\n        return _ma(a, w)\n    if key in (\"ema\", \"ewma\"):\n        return _ema(a, w)\n    if key in (\"gaussian\", \"weighted\"):\n        return _gaussian(a, w)\n    if key == \"median\":\n        return _median(a, w)\n    if key in (\"combined\", \"average\"):\n        return _combined_ma_ema(a, w)\n    if key in (\"detrended\", \"trend\"):\n        return _detrended_weighted(a, w)\n    return _best(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 13.5068, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _linear_detrend(arr: np.ndarray) -> np.ndarray:\n    n = arr.size\n    if n == 0:\n        return arr\n    t = np.arange(n, dtype=float)\n    A = np.vstack([t, np.ones_like(t)]).T\n    slope, intercept = np.linalg.lstsq(A, arr, rcond=None)[0]\n    return arr - (slope * t + intercept)\n\n\ndef adaptive_filter(x: np.ndarray, window_size: int = 20) -> np.ndarray:\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x: np.ndarray, window_size: int = 20) -> np.ndarray:\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(x, w, mode=\"valid\")\n\n\ndef ema_filter(x: np.ndarray, window_size: int = 20) -> np.ndarray:\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x: np.ndarray, window_size: int = 20) -> np.ndarray:\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    detrended = _linear_detrend(x)\n    return weighted_filter(detrended, window_size)\n\n\ndef median_filter(x: np.ndarray, window_size: int = 20) -> np.ndarray:\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    stride = x.strides[0]\n    shape = (x.size - window_size + 1, window_size)\n    windows = np.lib.stride_tricks.as_strided(x, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef combined_filter(x: np.ndarray, window_size: int = 20) -> np.ndarray:\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.array([], dtype=float)\n    detrended = _linear_detrend(x)\n    weighted = weighted_filter(detrended, window_size)\n    if weighted.size == 0:\n        return np.array([], dtype=float)\n    return median_filter(weighted, window_size)\n\n\ndef _residual_variance(orig: np.ndarray, filt: np.ndarray) -> float:\n    m = min(len(orig), len(filt))\n    if m == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:m] - filt[:m]))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"combined\": combined_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n    }\n    best_key = min(\n        candidates,\n        key=lambda k: (_residual_variance(x, candidates[k]), k)\n    )\n    return candidates[best_key]\n\n\ndef process_signal(input_signal, window_size: int = 20, algorithm_type: str = \"combined\") -> np.ndarray:\n    alg = (algorithm_type or \"\").lower()\n    if alg == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if alg == \"combined\":\n        return combined_filter(input_signal, window_size)\n    if alg == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if alg == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if alg == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0 or x.size < window_size:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponential\u2011weighted moving average.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0 or x.size < window_size:\n        return np.array([], dtype=float)\n    w = np.exp(np.linspace(-2, 0, window_size))\n    w /= w.sum()\n    return np.convolve(x, w, mode=\"valid\")\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0 or x.size < window_size:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter_with_trend_preservation(x, window_size=20):\n    \"\"\"Detrend with simple moving average then apply weighted filter.\"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.array([], dtype=float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = np.asarray(x, dtype=float) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef median_filter(x, window_size=20):\n    \"\"\"Sliding\u2011window median filter.\"\"\"\n    x = np.asarray(x, dtype=float)\n    if window_size <= 0 or x.size < window_size:\n        return np.array([], dtype=float)\n    stride = x.strides[0]\n    shape = (x.size - window_size + 1, window_size)\n    windows = np.lib.stride_tricks.as_strided(\n        x, shape=shape, strides=(stride, stride)\n    )\n    return np.median(windows, axis=1)\n\n\ndef combined_filter(x, window_size=20):\n    \"\"\"Detrend \u2192 weighted moving average \u2192 median smoothing.\"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.array([], dtype=float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = np.asarray(x, dtype=float) - trend_full\n    weighted = weighted_filter(detrended, window_size)\n    if weighted.size == 0:\n        return np.array([], dtype=float)\n    return median_filter(weighted, window_size)\n\n\ndef _residual_variance(original: np.ndarray, filtered: np.ndarray) -> float:\n    \"\"\"Variance of (original\u2011filtered); empty filtered yields infinite variance.\"\"\"\n    if filtered.size == 0:\n        return float(\"inf\")\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(original[:n] - filtered[:n]))\n\n\ndef _select_best_filter(x: np.ndarray, window_size: int) -> np.ndarray:\n    \"\"\"Pick the filter with the lowest residual variance.\"\"\"\n    candidates = {\n        \"basic\": adaptive_filter(x, window_size),\n        \"weighted\": weighted_filter(x, window_size),\n        \"ema\": ema_filter(x, window_size),\n        \"enhanced\": enhanced_filter_with_trend_preservation(x, window_size),\n        \"combined\": combined_filter(x, window_size),\n    }\n    best = min(candidates, key=lambda k: _residual_variance(x, candidates[k]))\n    return candidates[best]\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"Dispatch to the requested filter or auto\u2011select the best one.\"\"\"\n    if algorithm_type == \"basic\":\n        return adaptive_filter(input_signal, window_size)\n    if algorithm_type == \"weighted\":\n        return weighted_filter(input_signal, window_size)\n    if algorithm_type == \"ema\":\n        return ema_filter(input_signal, window_size)\n    if algorithm_type == \"enhanced\":\n        return enhanced_filter_with_trend_preservation(input_signal, window_size)\n    if algorithm_type == \"combined\":\n        return combined_filter(input_signal, window_size)\n\n    # Fallback: automatic selection based on residual variance\n    return _select_best_filter(np.asarray(input_signal, dtype=float), window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 14.0152, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(arr, w):\n    \"\"\"Return True if window size is a positive integer and fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n\ndef _simple_moving_average(arr, w):\n    \"\"\"Basic moving\u2011average filter (valid convolution).\"\"\"\n    if not _validate_window(arr, w):\n        return np.empty(0, float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Return a simple moving\u2011average filtered signal regardless of ``algorithm_type``.\n    Guarantees deterministic output, proper handling of edge cases, and minimal overhead.\n    \"\"\"\n    arr = _to_numpy(input_signal)\n    w = int(window_size)\n    return _simple_moving_average(arr, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    x = np.asarray(x, dtype=float).ravel()\n    if window_size <= 0 or x.size < window_size:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"Return a basic moving\u2011average filter regardless of the requested algorithm.\"\"\"\n    return adaptive_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _valid_window(arr, w):\n    \"\"\"Return True if window size is a positive integer that fits the array.\"\"\"\n    return isinstance(w, int) and w > 0 and arr.size >= w\n\n\ndef _ma(arr, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    return np.convolve(arr, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(arr, w):\n    \"\"\"Classic exponential moving average.\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(arr)\n    e[0] = arr[0]\n    for i in range(1, arr.size):\n        e[i] = alpha * arr[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\n\ndef _gaussian(arr, w):\n    \"\"\"Gaussian\u2011kernel weighted moving average.\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    kernel = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    kernel /= kernel.sum()\n    return np.convolve(arr, kernel, mode=\"valid\")\n\n\ndef _median(arr, w):\n    \"\"\"Sliding\u2011window median filter.\"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n    stride = arr.strides[0]\n    shape = (arr.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(arr, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _combined_ma_ema(arr, w):\n    \"\"\"Average of MA and EMA (both must be present).\"\"\"\n    ma = _ma(arr, w)\n    ema = _ema(arr, w)\n    if ma.size and ema.size:\n        return (ma + ema) / 2.0\n    return ma if ma.size else ema\n\n\ndef _detrended_weighted(arr, w):\n    \"\"\"\n    Detrend with a simple moving average, smooth the detrended series\n    using a Gaussian kernel, then re\u2011add the trend.\n    \"\"\"\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n\n    # Trend estimation (MA)\n    trend = _ma(arr, w)                       # length = n\u2011w+1\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n\n    # Detrend, smooth, and restore trend\n    detrended = arr - trend_full\n    smooth = _gaussian(detrended, w)          # length = n\u2011w+1\n    return smooth + trend                     # both have same length\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean\u2011squared deviation over the overlapping portion.\"\"\"\n    n = min(orig.size, filt.size)\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(arr, w):\n    \"\"\"Select the filter with the lowest residual variance (deterministic tie\u2011break).\"\"\"\n    candidates = {\n        \"ma\": _ma(arr, w),\n        \"ema\": _ema(arr, w),\n        \"gaussian\": _gaussian(arr, w),\n        \"median\": _median(arr, w),\n        \"combined\": _combined_ma_ema(arr, w),\n        \"detrended\": _detrended_weighted(arr, w),\n    }\n    # Discard empty results\n    filtered = [(name, data) for name, data in candidates.items() if data.size]\n    if not filtered:\n        return np.empty(0, float)\n\n    # Compute residual variances\n    metrics = [( _residual_variance(arr, data), name, data) for name, data in filtered]\n    # Sort by variance then name for deterministic choice\n    metrics.sort(key=lambda t: (t[0], t[1]))\n    return metrics[0][2]\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Alias for simple moving\u2011average (MA).\"\"\"\n    arr = _to_numpy(x)\n    w = int(window_size)\n    return _ma(arr, w)\n\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n\n    Supported `algorithm_type` values (case\u2011insensitive):\n        basic / ma          \u2013 simple moving average\n        ema / ewma          \u2013 exponential moving average\n        weighted / gaussian \u2013 Gaussian\u2011weighted moving average\n        median              \u2013 median filter\n        combined / average  \u2013 average of MA and EMA\n        detrended / trend   \u2013 detrended\u2011weighted filter\n        auto / default      \u2013 automatic selection of the best filter\n        any other value     \u2013 falls back to automatic selection\n    \"\"\"\n    arr = _to_numpy(input_signal)\n    w = int(window_size)\n\n    if not _valid_window(arr, w):\n        return np.empty(0, float)\n\n    key = (algorithm_type or \"\").lower()\n\n    if key in (\"basic\", \"ma\"):\n        return _ma(arr, w)\n    if key in (\"ema\", \"ewma\"):\n        return _ema(arr, w)\n    if key in (\"weighted\", \"gaussian\"):\n        return _gaussian(arr, w)\n    if key == \"median\":\n        return _median(arr, w)\n    if key in (\"combined\", \"average\"):\n        return _combined_ma_ema(arr, w)\n    if key in (\"detrended\", \"trend\"):\n        return _detrended_weighted(arr, w)\n    if key in (\"auto\", \"default\"):\n        return _best_filter(arr, w)\n\n    # Fallback to automatic best\u2011filter selection\n    return _best_filter(arr, w)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}