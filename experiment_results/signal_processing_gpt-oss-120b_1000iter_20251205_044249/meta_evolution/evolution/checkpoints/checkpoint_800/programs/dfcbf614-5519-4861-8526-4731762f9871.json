{"id": "dfcbf614-5519-4861-8526-4731762f9871", "code": "# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w, wt=None):\n    \"\"\"Simple moving\u2011average convolution.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        return np.empty(0, float)\n    if wt is None:\n        wt = np.ones(w) / w\n    return np.convolve(a, wt, mode=\"valid\")\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Uniform moving average.\"\"\"\n    return _ma(x, window_size)\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    a = _to_numpy(x)\n    if window_size <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Detrend with a simple MA then apply a weighted MA.\"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    # Pad trend to original length\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = _to_numpy(x) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean\u2011squared deviation between aligned signals.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"Select the filter with the lowest residual variance (deterministic).\"\"\"\n    a = _to_numpy(x)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    # Remove empty results\n    filtered = {k: v for k, v in candidates.items() if v.size}\n    if not filtered:\n        return np.empty(0, float)\n\n    # Compute variance and pick smallest; tie\u2011break by name\n    best_name = min(\n        filtered,\n        key=lambda k: ( _residual_variance(a, filtered[k]), k )\n    )\n    return filtered[best_name]\n\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm; unknown types default to best filter.\"\"\"\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter,\n        \"combined\": _best_filter,\n        \"auto\": _best_filter,\n        \"best\": _best_filter,\n    }\n    key = (algorithm_type or \"\").lower()\n    func = dispatch.get(key, _best_filter)\n    return func(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n", "language": "python", "parent_id": "1219e0d9-6db3-4ec3-aece-3d3992f78f6e", "generation": 7, "timestamp": 1764941500.819777, "iteration_found": 627, "metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.3624766970140094, "slope_changes": 68.6, "lag_error": 0.4099887786960041, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.20076318890739916, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.709225502436145, "accuracy_score": 0.20076318890739916, "efficiency_score": 1.0, "execution_time": 0.00039501190185546874, "success_rate": 1.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Evolve block rewrite (preserved external code)", "parent_metrics": {"runs_successfully": 1.0, "composite_score": 0.4429433309255678, "output_length": 91.0, "overall_score": 0.3624766970140094, "slope_changes": 68.6, "lag_error": 0.4099887786960041, "avg_error": 1.1150775516528053, "false_reversals": 56.6, "correlation": 0.20076318890739916, "noise_reduction": 0.0, "smoothness_score": 0.22573363431151244, "responsiveness_score": 0.709225502436145, "accuracy_score": 0.20076318890739916, "efficiency_score": 1.0, "execution_time": 0.000524139404296875, "success_rate": 1.0}, "island": 3}, "prompts": {"full_rewrite_user": {"system": "## CRITICAL FORMAT REQUIREMENTS (DO NOT VIOLATE)\n## OUTPUT FORMAT (CRITICAL)\n- Output ONLY the code that goes BETWEEN the `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers\n- Do NOT include the markers themselves in your output\n- Do NOT include any code outside the markers (imports, run_packing, etc.)\n- The system will automatically merge your output with the preserved code sections\n\n## FOCUS\n- Improve ONLY the `construct_packing()` function and helper functions within the evolve block\n- Functions like `run_packing()` are preserved automatically - do not include them\n\nViolating these requirements will cause the program to fail evaluation.\n\n---\n\nYou are tasked with improving the **`construct_packing()`** function and any helper functions that reside inside the **evolve block** delimited by `# EVOLVE\u2011BLOCK\u2011START` and `# EVOLVE\u2011BLOCK\u2011END`.  \n\n### Mandatory Output Rules  \n- **Output ONLY the code that belongs BETWEEN the `# EVOLVE\u2011BLOCK\u2011START` and `# EVOLVE\u2011BLOCK\u2011END` markers.**  \n- **Do NOT include the markers themselves** in your response.  \n- **Do NOT output any code outside those markers** (e.g., imports, the `run_packing()` wrapper, or other unrelated sections).  \nThe surrounding scaffold will be merged automatically; focus exclusively on the evolve block.\n\n### What You Must NOT Do (previous failures)  \n- Do **not** produce code that references undefined variables, missing imports, or external symbols that are not already available in the surrounding file.  \n- Do **not** change the overall file structure or add new top\u2011level statements outside the evolve block.  \n- Do **not** generate overly aggressive or speculative code that is likely to raise syntax or runtime errors.  \n- Do **not** repeat the same faulty pattern that caused consecutive invalid programs (e.g., returning the wrong type, missing required return statements, or using mutable default arguments).  \n\n### Goal of `construct_packing()`  \nCreate a deterministic packing plan that:  \n\n1. **Validates Input** \u2013 Ensure the `items` list and `max_weight`/`max_volume` parameters are present and of the correct type.  \n2. **Sorts Items** \u2013 Apply a clear, reproducible ordering (e.g., descending by weight\u202f+\u202fvolume density) to improve packing efficiency.  \n3. **Greedy Bin\u2011Packing** \u2013 Iterate through the sorted items, placing each into the first bin that can accommodate both its weight and volume. If no existing bin fits, open a new bin.  \n4. **Respect Constraints** \u2013 Never exceed `max_weight` **or** `max_volume` for any bin.  \n5. **Return Structure** \u2013 Produce a list of bins, where each bin is a dictionary:  \n   ```python\n   {\n       \"items\": [item_id, ...],\n       \"total_weight\": float,\n       \"total_volume\": float\n   }\n   ```  \n   The outer function must return this list directly.  \n\n### Specific Strategies to Try  \n\n- **Helper Function `fits(bin, item)`** \u2013 Returns `True` only if adding `item` keeps both weight and volume \u2264 limits.  \n- **Helper Function `add_to_bin(bin, item)`** \u2013 Updates the bin\u2019s `items`, `total_weight`, and `total_volume` in\u2011place.  \n- **Avoid Mutable Defaults** \u2013 Define helper functions with explicit parameters; never use `def helper(..., bins=[])`.  \n- **Early Exit** \u2013 If an item alone exceeds the limits, skip it and optionally record it in a separate \u201cunpacked\u201d list (but do **not** return this list unless the original signature expects it).  \n- **Deterministic Tie\u2011Breaking** \u2013 When multiple bins can accept an item, choose the bin with the smallest remaining capacity (e.g., minimal leftover weight + volume). This reduces variability and helps the evaluator.  \n\n### Formatting Guidelines  \n\n- Keep the code **PEP\u20118 compliant** (indentation, line length \u2264\u202f88 characters).  \n- Use clear variable names (`bins`, `item`, `weight`, `volume`).  \n- Include concise docstrings for any new helper functions you add.  \n- Do **not** add any `print` statements or debugging output; the evaluator expects only the packing data structure.  \n\n### Reminder  \n\nYour response must be **exactly** the code that belongs inside the evolve block\u2014no surrounding text, no markers, no extra sections. Follow the rules above to produce a valid, functional implementation of `construct_packing()` and its helpers.", "user": "# Current Program Information\n- Fitness: 13.9292\n- Feature coordinates: No feature coordinates\n- Focus areas: - Fitness declined: 14.0152 \u2192 13.9292. Consider revising recent changes.\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0009, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0011, success_rate: 1.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Evolve block rewrite (preserved external code)\n- Metrics: runs_successfully: 1.0000, composite_score: 0.4205, output_length: 91.0000, overall_score: 0.3804, slope_changes: 68.0000, lag_error: 0.9780, avg_error: 1.2635, false_reversals: 57.8000, correlation: 0.3335, noise_reduction: 0.0000, smoothness_score: 0.2273, responsiveness_score: 0.5056, accuracy_score: 0.3335, efficiency_score: 1.0000, execution_time: 0.0015, success_rate: 1.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Classic exponential moving average aligned with MA output length.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011kernel moving average.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    kernel = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    kernel /= kernel.sum()\n    return np.convolve(a, kernel, mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _combined_ma_ema(x, w):\n    \"\"\"Average of MA and EMA (both have the same length).\"\"\"\n    ma = _ma(x, w)\n    ema = _ema(x, w)\n    if ma.size and ema.size:\n        return (ma + ema) / 2.0\n    return ma if ma.size else ema\n\n\ndef _detrended_weighted(x, w):\n    \"\"\"\n    Detrend using a simple moving average, smooth the detrended series\n    with a Gaussian weighted filter, then re\u2011add the trend.\n    \"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n\n    # Estimate local trend (MA)\n    trend = _ma(a, w)                     # length = n - w + 1\n    # Pad trend to original length by repeating edge values\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n\n    # Detrend\n    detrended = a - trend_full\n\n    # Smooth detrended series with Gaussian kernel\n    smooth = _gaussian(detrended, w)\n\n    # Re\u2011add trend (already aligned with smooth output)\n    return smooth + trend\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean\u2011squared deviation between aligned signals.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"\n    Select the filter with the lowest residual variance.\n    Considers MA, EMA, Gaussian, Median, combined MA/EMA, and detrended weighted.\n    \"\"\"\n    a = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gaussian\": _gaussian(a, w),\n        \"median\": _median(a, w),\n        \"combined\": _combined_ma_ema(a, w),\n        \"detrended\": _detrended_weighted(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.array([], dtype=float)\n    # Return the filter with minimal residual variance\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to requested filter.\n    Supported types (case\u2011insensitive):\n        basic/ma, ewma/ema, weighted, median, combined/enhanced.\n    Any other value falls back to automatic best\u2011filter selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg == \"weighted\":\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg in (\"combined\", \"enhanced\"):\n        return _best_filter(sig, w)\n    # Fallback to automatic selection\n    return _best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0015), Performs well on success_rate (1.0000)\n\n### Program 2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    \"\"\"Simple moving average (valid convolution).\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0, float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(x, w):\n    \"\"\"Exponential moving average aligned with MA output length.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _gaussian(x, w):\n    \"\"\"Gaussian\u2011kernel moving average.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0, float)\n    sigma = 0.3 * w if w > 0 else 1.0\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    kernel = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    kernel /= kernel.sum()\n    return np.convolve(a, kernel, mode=\"valid\")\n\n\ndef _median(x, w):\n    \"\"\"Median filter.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0, float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(original, filtered):\n    \"\"\"Mean\u2011squared deviation between aligned signals.\"\"\"\n    n = min(len(original), len(filtered))\n    if n == 0:\n        return float(\"inf\")\n    return float(np.var(original[:n] - filtered[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"Select the filter (MA, EMA, Gaussian, Median) with lowest residual variance.\"\"\"\n    a = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gaussian\": _gaussian(a, w),\n        \"median\": _median(a, w),\n    }\n    # Discard empty results\n    candidates = {k: v for k, v in candidates.items() if v.size}\n    if not candidates:\n        return np.empty(0, float)\n    best_key = min(candidates, key=lambda k: _residual_variance(a, candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, alg=None):\n    \"\"\"\n    Dispatch to a filtering algorithm.\n    Supported ``alg`` values (case\u2011insensitive):\n        - \"ma\" / \"basic\": simple moving average\n        - \"ema\" / \"ewma\": exponential moving average\n        - \"gaussian\": Gaussian\u2011kernel moving average\n        - \"median\": median filter\n        - any other value or None: automatically select the best filter\n    \"\"\"\n    a = _to_numpy(sig)\n    if w <= 0 or a.size < w:\n        return np.empty(0, float)\n\n    if alg is None:\n        return _best_filter(a, w)\n\n    key = (alg or \"\").lower()\n    if key in (\"ma\", \"basic\"):\n        return _ma(a, w)\n    if key in (\"ema\", \"ewma\"):\n        return _ema(a, w)\n    if key == \"gaussian\":\n        return _gaussian(a, w)\n    if key == \"median\":\n        return _median(a, w)\n\n    # Fallback to automatic selection\n    return _best_filter(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0011), Performs well on success_rate (1.0000)\n\n### Program 3 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _gaussian(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    wts /= wts.sum()\n    return np.convolve(a, wts, mode=\"valid\")\n\n\ndef _median(x, w):\n    a = _to_numpy(x)\n    if w <= 0 or a.size < w:\n        return np.array([], dtype=float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(orig, filt):\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    candidates = {\n        \"ma\": _ma(x, w),\n        \"ema\": _ema(x, w),\n        \"gauss\": _gaussian(x, w),\n        \"median\": _median(x, w),\n    }\n    orig = _to_numpy(x)\n    return min(candidates.values(), key=lambda f: _residual_variance(orig, f))\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to requested filter.\n    Supported types (case\u2011insensitive):\n        basic/ma, ewma/ema, weighted, median, enhanced/combined.\n    Any other value falls back to automatic best\u2011filter selection.\n    \"\"\"\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg == \"weighted\":\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg in (\"enhanced\", \"combined\"):\n        return _best_filter(sig, w)\n    return _best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on composite_score (0.4205), Performs well on output_length (91.0000), Performs well on overall_score (0.3804), Performs well on slope_changes (68.0000), Performs well on lag_error (0.9780), Performs well on avg_error (1.2635), Performs well on false_reversals (57.8000), Performs well on correlation (0.3335), Performs well on noise_reduction (0.0000), Performs well on smoothness_score (0.2273), Performs well on responsiveness_score (0.5056), Performs well on accuracy_score (0.3335), Performs well on efficiency_score (1.0000), Performs well on execution_time (0.0009), Performs well on success_rate (1.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _validate_window(x, w):\n    \"\"\"Validate window size; raise ValueError for non\u2011positive or too large.\"\"\"\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if x.size < w:\n        # Return an empty array rather than raising \u2013 callers handle empties\n        return np.empty(0, dtype=float)\n    return None\n\n\ndef _ma(x, w):\n    a = _to_numpy(x)\n    if _validate_window(a, w) is not None:\n        return np.empty(0, dtype=float)\n    return np.convolve(a, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ema(x, w):\n    a = _to_numpy(x)\n    if _validate_window(a, w) is not None:\n        return np.empty(0, dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _gaussian(x, w):\n    a = _to_numpy(x)\n    if _validate_window(a, w) is not None:\n        return np.empty(0, dtype=float)\n    sigma = 0.3 * w\n    centre = (w - 1) / 2.0\n    idx = np.arange(w, dtype=float)\n    wts = np.exp(-0.5 * ((idx - centre) / sigma) ** 2)\n    wts /= wts.sum()\n    return np.convolve(a, wts, mode=\"valid\")\n\n\ndef _median(x, w):\n    a = _to_numpy(x)\n    if _validate_window(a, w) is not None:\n        return np.empty(0, dtype=float)\n    stride = a.strides[0]\n    shape = (a.size - w + 1, w)\n    windows = np.lib.stride_tricks.as_strided(a, shape=shape, strides=(stride, stride))\n    return np.median(windows, axis=1)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean\u2011squared deviation between aligned signals.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"Select the filter with the lowest residual variance (deterministic tie\u2011break).\"\"\"\n    a = _to_numpy(x)\n    candidates = {\n        \"ma\": _ma(a, w),\n        \"ema\": _ema(a, w),\n        \"gauss\": _gaussian(a, w),\n        \"median\": _median(a, w),\n    }\n    # Discard empty results\n    filtered = [(name, arr) for name, arr in candidates.items() if arr.size]\n    if not filtered:\n        return np.empty(0, dtype=float)\n\n    # Compute variance for each candidate\n    metrics = [\n        ( _residual_variance(a, arr), name, arr )\n        for name, arr in filtered\n    ]\n    # Deterministic: smallest variance, then alphabetical name\n    _, _, best_arr = min(metrics, key=lambda t: (t[0], t[1]))\n    return best_arr\n\n\ndef adaptive_filter(x, w=20):\n    \"\"\"Alias for simple moving\u2011average.\"\"\"\n    return _ma(x, w)\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    \"\"\"\n    Dispatch to requested filter.\n    Supported types (case\u2011insensitive):\n        basic/ma, ewma/ema, weighted, median, enhanced/combined.\n    Any other value falls back to automatic best\u2011filter selection.\n    \"\"\"\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ema(sig, w)\n    if alg == \"weighted\":\n        return _gaussian(sig, w)\n    if alg == \"median\":\n        return _median(sig, w)\n    if alg in (\"enhanced\", \"combined\"):\n        return _best_filter(sig, w)\n    # Fallback to automatic selection\n    return _best_filter(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n### Program D2 (Score: 14.0152)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_numpy(arr):\n    return np.asarray(arr, dtype=float).ravel()\n\n\ndef _ma(x, w):\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(w) / w, mode=\"valid\")\n\n\ndef _ewma(x, w):\n    x = _to_numpy(x)\n    if w <= 0 or x.size < w:\n        return np.array([], dtype=float)\n    alpha = 2.0 / (w + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[w - 1 :]\n\n\ndef _combined(x, w):\n    \"\"\"Average of MA and EMA where both are available.\"\"\"\n    ma = _ma(x, w)\n    ew = _ewma(x, w)\n    if ma.size and ew.size:\n        return (ma + ew) / 2.0\n    return ma if ma.size else ew\n\n\ndef _best(x, w):\n    x = _to_numpy(x)\n    candidates = [_ma(x, w), _ewma(x, w), _combined(x, w)]\n\n    def _residual_var(orig, filt):\n        if filt.size == 0:\n            return float(\"inf\")\n        n = min(len(orig), len(filt))\n        return float(np.var(orig[:n] - filt[:n]))\n\n    best = min(candidates, key=lambda f: _residual_var(x, f))\n    return best\n\n\ndef process_signal(sig, w=20, algorithm_type=\"combined\"):\n    alg = (algorithm_type or \"\").lower()\n    if alg in (\"basic\", \"ma\"):\n        return _ma(sig, w)\n    if alg in (\"ewma\", \"ema\"):\n        return _ewma(sig, w)\n    return _best(sig, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to composite_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 14.0152, Type: Migrant)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Simple moving\u2011average filter.\"\"\"\n    x = np.asarray(x, dtype=float).ravel()\n    if window_size <= 0 or x.size < window_size:\n        return np.array([], dtype=float)\n    return np.convolve(x, np.ones(window_size) / window_size, mode=\"valid\")\n\ndef process_signal(input_signal, window_size=20, algorithm_type=\"combined\"):\n    \"\"\"Return a basic moving\u2011average filter regardless of the requested algorithm.\"\"\"\n    return adaptive_filter(input_signal, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 2 (Score: 14.0152, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _arr(x):\n    return np.asarray(x, float)\n\n\ndef _ma(x, w, wt=None):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0, float)\n    if wt is None:\n        wt = np.ones(w) / w\n    return np.convolve(a, wt, mode=\"valid\")\n\n\ndef _ema(x, w):\n    a = _arr(x)\n    if w <= 0 or a.size < w:\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    e = np.empty_like(a)\n    e[0] = a[0]\n    for i in range(1, len(a)):\n        e[i] = alpha * a[i] + (1 - alpha) * e[i - 1]\n    return e[w - 1 :]\n\n\ndef _resvar(orig, filt):\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n])) if n else float(\"inf\")\n\n\ndef _best(x, w):\n    cand = {\"ma\": _ma(x, w), \"ema\": _ema(x, w)}\n    return min(cand.values(), key=lambda f: _resvar(_arr(x), f))\n\n\ndef process_signal(sig, w=20, alg=\"enhanced\"):\n    a = _arr(sig)\n    if alg == \"basic\":\n        return _ma(a, w)\n    if alg == \"ema\":\n        return _ema(a, w)\n    return _best(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n### Inspiration 3 (Score: 13.9292, Type: High-Performer)\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _to_array(x):\n    \"\"\"Convert input to 1\u2011D float array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w, weights=None):\n    \"\"\"Convolution helper for moving\u2011average style filters.\"\"\"\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    if weights is None:\n        weights = np.ones(w) / w\n    return np.convolve(x, weights, mode=\"valid\")\n\n\ndef simple_ma(x, w):\n    return _ma(_to_array(x), w)\n\n\ndef weighted_ma(x, w):\n    wt = np.exp(np.linspace(-2, 0, w))\n    wt /= wt.sum()\n    return _ma(_to_array(x), w, wt)\n\n\ndef ema(x, w):\n    a = _to_array(x)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        return np.empty(0, float)\n    alpha = 2.0 / (w + 1)\n    out = np.empty_like(a)\n    out[0] = a[0]\n    for i in range(1, len(a)):\n        out[i] = alpha * a[i] + (1 - alpha) * out[i - 1]\n    return out[w - 1 :]\n\n\ndef detrended_weighted(x, w):\n    a = _to_array(x)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        return np.empty(0, float)\n    trend = simple_ma(a, w)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(w - 1, trend[0]), trend))\n    detrended = a - trend_full\n    return weighted_ma(detrended, w)\n\n\ndef gaussian_ma(x, w):\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    sigma = max(w / 6.0, 0.1)\n    centre = (w - 1) / 2.0\n    i = np.arange(w, dtype=float)\n    wt = np.exp(-0.5 * ((i - centre) / sigma) ** 2)\n    wt /= wt.sum()\n    return _ma(_to_array(x), w, wt)\n\n\ndef median_ma(x, w):\n    a = _to_array(x)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        return np.empty(0, float)\n    out_len = a.size - w + 1\n    out = np.empty(out_len, float)\n    for i in range(out_len):\n        out[i] = np.median(a[i : i + w])\n    return out\n\n\ndef double_enhanced(x, w):\n    first = detrended_weighted(x, w)\n    if first.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(first, half)\n\n\ndef advanced_filter(x, w):\n    first = detrended_weighted(x, w)\n    if first.size == 0:\n        return np.empty(0, float)\n    med = median_ma(first, w)\n    if med.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(med, half)\n\n\ndef hybrid_filter(x, w):\n    weighted = weighted_ma(x, w)\n    if weighted.size == 0:\n        return np.empty(0, float)\n    half = max(1, w // 2)\n    return simple_ma(weighted, half)\n\n\ndef _residual_variance(orig, filt):\n    m = min(len(orig), len(filt))\n    return float(np.var(orig[:m] - filt[:m])) if m else float(\"inf\")\n\n\ndef _select_best(x, w):\n    a = _to_array(x)\n    candidates = {\n        \"basic\": simple_ma(a, w),\n        \"weighted\": weighted_ma(a, w),\n        \"ema\": ema(a, w),\n        \"enhanced\": detrended_weighted(a, w),\n        \"gaussian\": gaussian_ma(a, w),\n        \"median\": median_ma(a, w),\n        \"double\": double_enhanced(a, w),\n        \"advanced\": advanced_filter(a, w),\n        \"hybrid\": hybrid_filter(a, w),\n    }\n    variances = {k: _residual_variance(a, v) for k, v in candidates.items()}\n    min_var = min(variances.values())\n    best_keys = [k for k, v in variances.items() if v == min_var]\n    best_key = max(best_keys, key=lambda k: len(candidates[k]))\n    return candidates[best_key]\n\n\ndef process_signal(sig, w=20, algo=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm or auto\u2011select the best one.\"\"\"\n    a = _to_array(sig)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    dispatch = {\n        \"basic\": simple_ma,\n        \"weighted\": weighted_ma,\n        \"ema\": ema,\n        \"enhanced\": detrended_weighted,\n        \"gaussian\": gaussian_ma,\n        \"median\": median_ma,\n        \"double\": double_enhanced,\n        \"advanced\": advanced_filter,\n        \"hybrid\": hybrid_filter,\n    }\n    key = (algo or \"\").lower()\n    if key in dispatch:\n        return dispatch[key](a, w)\n    return _select_best(a, w)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\nUnique approach: Modification: Evolve block rewrite (preserved external code), Excellent runs_successfully (1.000), Excellent output_length (91.000)\n\n# Current Program\n```python\n# EVOLVE-BLOCK-START\nimport numpy as np\n\ndef _ma(x, w, wt):\n    \"\"\"Convolution helper for moving\u2011average\u2011style filters.\"\"\"\n    x = np.asarray(x, float)\n    if w <= 0:\n        raise ValueError(\"window must be positive\")\n    if x.size < w:\n        return np.empty(0, float)\n    return np.convolve(x, wt, mode=\"valid\")\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Uniform moving average.\"\"\"\n    return _ma(x, window_size, np.ones(window_size) / window_size)\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    x = np.asarray(x, float)\n    if window_size <= 0:\n        raise ValueError(\"window_size must be positive\")\n    if x.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(x)\n    ema[0] = x[0]\n    for i in range(1, len(x)):\n        ema[i] = alpha * x[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Detrend with a simple MA then apply a weighted MA.\"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = np.asarray(x, float) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm; unknown types default to enhanced.\"\"\"\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter,\n    }\n    func = dispatch.get(algorithm_type.lower(), enhanced_filter)\n    return func(sig, window_size)\n# EVOLVE-BLOCK-END\n\n\ndef generate_test_signal(length=1000, noise_level=0.3, seed=42):\n    \"\"\"\n    Generate synthetic test signal with known characteristics.\n\n    Args:\n        length: Length of the signal\n        noise_level: Standard deviation of noise to add\n        seed: Random seed for reproducibility\n\n    Returns:\n        Tuple of (noisy_signal, clean_signal)\n    \"\"\"\n    np.random.seed(seed)\n    t = np.linspace(0, 10, length)\n\n    # Create a complex signal with multiple components\n    clean_signal = (\n        2 * np.sin(2 * np.pi * 0.5 * t)  # Low frequency component\n        + 1.5 * np.sin(2 * np.pi * 2 * t)  # Medium frequency component\n        + 0.5 * np.sin(2 * np.pi * 5 * t)  # Higher frequency component\n        + 0.8 * np.exp(-t / 5) * np.sin(2 * np.pi * 1.5 * t)  # Decaying oscillation\n    )\n\n    # Add non-stationary behavior\n    trend = 0.1 * t * np.sin(0.2 * t)  # Slowly varying trend\n    clean_signal += trend\n\n    # Add random walk component for non-stationarity\n    random_walk = np.cumsum(np.random.randn(length) * 0.05)\n    clean_signal += random_walk\n\n    # Add noise\n    noise = np.random.normal(0, noise_level, length)\n    noisy_signal = clean_signal + noise\n\n    return noisy_signal, clean_signal\n\n\ndef run_signal_processing(signal_length=1000, noise_level=0.3, window_size=20):\n    \"\"\"\n    Run the signal processing algorithm on a test signal.\n\n    Returns:\n        Dictionary containing results and metrics\n    \"\"\"\n    # Generate test signal\n    noisy_signal, clean_signal = generate_test_signal(signal_length, noise_level)\n\n    # Process the signal\n    filtered_signal = process_signal(noisy_signal, window_size, \"enhanced\")\n\n    # Calculate basic metrics\n    if len(filtered_signal) > 0:\n        # Align signals for comparison (account for processing delay)\n        delay = window_size - 1\n        aligned_clean = clean_signal[delay:]\n        aligned_noisy = noisy_signal[delay:]\n\n        # Ensure same length\n        min_length = min(len(filtered_signal), len(aligned_clean))\n        filtered_signal = filtered_signal[:min_length]\n        aligned_clean = aligned_clean[:min_length]\n        aligned_noisy = aligned_noisy[:min_length]\n\n        # Calculate correlation with clean signal\n        correlation = np.corrcoef(filtered_signal, aligned_clean)[0, 1] if min_length > 1 else 0\n\n        # Calculate noise reduction\n        noise_before = np.var(aligned_noisy - aligned_clean)\n        noise_after = np.var(filtered_signal - aligned_clean)\n        noise_reduction = (noise_before - noise_after) / noise_before if noise_before > 0 else 0\n\n        return {\n            \"filtered_signal\": filtered_signal,\n            \"clean_signal\": aligned_clean,\n            \"noisy_signal\": aligned_noisy,\n            \"correlation\": correlation,\n            \"noise_reduction\": noise_reduction,\n            \"signal_length\": min_length,\n        }\n    else:\n        return {\n            \"filtered_signal\": [],\n            \"clean_signal\": [],\n            \"noisy_signal\": [],\n            \"correlation\": 0,\n            \"noise_reduction\": 0,\n            \"signal_length\": 0,\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the algorithm\n    results = run_signal_processing()\n    print(f\"Signal processing completed!\")\n    print(f\"Correlation with clean signal: {results['correlation']:.3f}\")\n    print(f\"Noise reduction: {results['noise_reduction']:.3f}\")\n    print(f\"Processed signal length: {results['signal_length']}\")\n\n```\n\n# Task\nImprove the code to maximize FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\n\n## CRITICAL INSTRUCTIONS\nIf the code contains `# EVOLVE-BLOCK-START` and `# EVOLVE-BLOCK-END` markers:\n- Output ONLY the code that goes BETWEEN these markers\n- Do NOT include the markers themselves\n- Do NOT include code outside the markers (like `run_packing()`)\n- The system will automatically merge your output with the preserved sections\n\nIf there are NO EVOLVE-BLOCK markers:\n- Output the complete rewritten program\n\n```python\n# Your improved code here (ONLY the evolve block content if markers exist)\n```", "responses": ["import numpy as np\n\ndef _to_numpy(x):\n    \"\"\"Convert input to a 1\u2011D float NumPy array.\"\"\"\n    return np.asarray(x, dtype=float).ravel()\n\n\ndef _ma(x, w, wt=None):\n    \"\"\"Simple moving\u2011average convolution.\"\"\"\n    a = _to_numpy(x)\n    if w <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < w:\n        return np.empty(0, float)\n    if wt is None:\n        wt = np.ones(w) / w\n    return np.convolve(a, wt, mode=\"valid\")\n\n\ndef adaptive_filter(x, window_size=20):\n    \"\"\"Uniform moving average.\"\"\"\n    return _ma(x, window_size)\n\n\ndef weighted_filter(x, window_size=20):\n    \"\"\"Exponentially weighted moving average.\"\"\"\n    wt = np.exp(np.linspace(-2, 0, window_size))\n    wt /= wt.sum()\n    return _ma(x, window_size, wt)\n\n\ndef ema_filter(x, window_size=20):\n    \"\"\"Classic exponential moving average (recursive).\"\"\"\n    a = _to_numpy(x)\n    if window_size <= 0:\n        raise ValueError(\"window size must be positive\")\n    if a.size < window_size:\n        return np.empty(0, float)\n    alpha = 2.0 / (window_size + 1)\n    ema = np.empty_like(a)\n    ema[0] = a[0]\n    for i in range(1, len(a)):\n        ema[i] = alpha * a[i] + (1 - alpha) * ema[i - 1]\n    return ema[window_size - 1 :]\n\n\ndef enhanced_filter(x, window_size=20):\n    \"\"\"Detrend with a simple MA then apply a weighted MA.\"\"\"\n    trend = adaptive_filter(x, window_size)\n    if trend.size == 0:\n        return np.empty(0, float)\n    # Pad trend to original length\n    trend_full = np.concatenate((np.full(window_size - 1, trend[0]), trend))\n    detrended = _to_numpy(x) - trend_full\n    return weighted_filter(detrended, window_size)\n\n\ndef _residual_variance(orig, filt):\n    \"\"\"Mean\u2011squared deviation between aligned signals.\"\"\"\n    if filt.size == 0:\n        return float(\"inf\")\n    n = min(len(orig), len(filt))\n    return float(np.var(orig[:n] - filt[:n]))\n\n\ndef _best_filter(x, w):\n    \"\"\"Select the filter with the lowest residual variance (deterministic).\"\"\"\n    a = _to_numpy(x)\n    candidates = {\n        \"adaptive\": adaptive_filter(a, w),\n        \"weighted\": weighted_filter(a, w),\n        \"ema\": ema_filter(a, w),\n        \"enhanced\": enhanced_filter(a, w),\n    }\n    # Remove empty results\n    filtered = {k: v for k, v in candidates.items() if v.size}\n    if not filtered:\n        return np.empty(0, float)\n\n    # Compute variance and pick smallest; tie\u2011break by name\n    best_name = min(\n        filtered,\n        key=lambda k: ( _residual_variance(a, filtered[k]), k )\n    )\n    return filtered[best_name]\n\n\ndef process_signal(sig, window_size=20, algorithm_type=\"enhanced\"):\n    \"\"\"Dispatch to a filtering algorithm; unknown types default to best filter.\"\"\"\n    dispatch = {\n        \"basic\": adaptive_filter,\n        \"weighted\": weighted_filter,\n        \"ema\": ema_filter,\n        \"enhanced\": enhanced_filter,\n        \"combined\": _best_filter,\n        \"auto\": _best_filter,\n        \"best\": _best_filter,\n    }\n    key = (algorithm_type or \"\").lower()\n    func = dispatch.get(key, _best_filter)\n    return func(sig, window_size)"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}